<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>论文阅读笔记：A Survey on Principles, Models and Methods for Learning from Irregularly Sampled Time Series - harry-zzh的博客</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="harry-zzh的博客"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="harry-zzh的博客"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="本篇综述介绍了不规则采样时间序列的表示形式、建模原语以及它们所承担的推理任务，并调研了与建模原语有关的最新文献。之后，讨论了基于时间离散化，内插，递归，注意力和结构不变性的方法和方法之间的异同，并强调了主要的优点和缺点。"><meta property="og:type" content="blog"><meta property="og:title" content="论文阅读笔记：A Survey on Principles, Models and Methods for Learning from Irregularly Sampled Time Series"><meta property="og:url" content="http://harry-zzh.github.io/2021/03/20/1/"><meta property="og:site_name" content="harry-zzh的博客"><meta property="og:description" content="本篇综述介绍了不规则采样时间序列的表示形式、建模原语以及它们所承担的推理任务，并调研了与建模原语有关的最新文献。之后，讨论了基于时间离散化，内插，递归，注意力和结构不变性的方法和方法之间的异同，并强调了主要的优点和缺点。"><meta property="og:locale" content="en_US"><meta property="og:image" content="http://harry-zzh.github.io/2021/03/20/1/regularAndIr.png"><meta property="og:image" content="http://harry-zzh.github.io/2021/03/20/1/series-based.png"><meta property="og:image" content="http://harry-zzh.github.io/2021/03/20/1/vector-based.PNG"><meta property="og:image" content="http://harry-zzh.github.io/2021/03/20/1/set-based.PNG"><meta property="og:image" content="http://harry-zzh.github.io/2021/03/20/1/Inference_Tasks.PNG"><meta property="og:image" content="http://harry-zzh.github.io/2021/03/20/1/methodsbasedonmodelingprimitives.PNG"><meta property="og:image" content="http://harry-zzh.github.io/2021/03/20/1/discretation.PNG"><meta property="og:image" content="http://harry-zzh.github.io/2021/03/20/1/interpolation.PNG"><meta property="og:image" content="http://harry-zzh.github.io/2021/03/20/1/recurrence.PNG"><meta property="og:image" content="http://harry-zzh.github.io/2021/03/20/1/Attention.PNG"><meta property="og:image" content="http://harry-zzh.github.io/2021/03/20/1/structuralInvariance.PNG"><meta property="article:published_time" content="2021-03-20T08:39:22.558Z"><meta property="article:modified_time" content="2021-03-28T09:06:40.610Z"><meta property="article:author" content="harry-zzh"><meta property="article:tag" content="不规则采样"><meta property="article:tag" content="时间序列"><meta property="article:tag" content="srp周报"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/2021/03/20/1/regularAndIr.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://harry-zzh.github.io/2021/03/20/1/"},"headline":"论文阅读笔记：A Survey on Principles, Models and Methods for Learning from Irregularly Sampled Time Series","image":["http://harry-zzh.github.io/2021/03/20/1/regularAndIr.png","http://harry-zzh.github.io/2021/03/20/1/series-based.png"],"datePublished":"2021-03-20T08:39:22.558Z","dateModified":"2021-03-28T09:06:40.610Z","author":{"@type":"Person","name":"harry-zzh"},"description":"本篇综述介绍了不规则采样时间序列的表示形式、建模原语以及它们所承担的推理任务，并调研了与建模原语有关的最新文献。之后，讨论了基于时间离散化，内插，递归，注意力和结构不变性的方法和方法之间的异同，并强调了主要的优点和缺点。"}</script><link rel="canonical" href="http://harry-zzh.github.io/2021/03/20/1/"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.14.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/favicon.svg" alt="harry-zzh的博客" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">主页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/Harry-zzh"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-9-widescreen"><div class="card"><article class="card-content article" role="article"><h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal"><i class="fas fa-angle-double-right"></i>论文阅读笔记：A Survey on Principles, Models and Methods for Learning from Irregularly Sampled Time Series</h1><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><i class="far fa-calendar-alt"> </i><time dateTime="${date_xml(page.date)}" title="${date_xml(page.date)}">2021-03-20</time></span><span class="level-item is-hidden-mobile"><i class="far fa-calendar-check"> </i><time dateTime="${date_xml(page.updated)}" title="${date_xml(page.updated)}">2021-03-28</time></span><span class="level-item"><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">论文阅读笔记</a></span><span class="level-item">31 minutes read (About 4688 words)</span></div></div><div class="content"><p>本篇综述介绍了不规则采样时间序列的表示形式、建模原语以及它们所承担的推理任务，并调研了与建模原语有关的最新文献。之后，讨论了基于时间离散化，内插，递归，注意力和结构不变性的方法和方法之间的异同，并强调了主要的优点和缺点。</p>
<span id="more"></span>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><h3 id="1-1-定义"><a href="#1-1-定义" class="headerlink" title="1.1 定义"></a>1.1 定义</h3><p>不规则采样时间序列，是指采样时间（也称观测时间）间隔不固定的序列。<br><img src="/2021/03/20/1/regularAndIr.png" alt="Illustration of regularly and irregularly sampled univariate and two-dimensional multivariate time series"><br>如上图，进一步细分，可以分为单变量不规则采样和多变量不规则采样。多变量不规则采样中，不同维度的采样时间可能对齐(aligned)，也可能不对齐(unaligned)。</p>
<h3 id="1-2-挑战"><a href="#1-2-挑战" class="headerlink" title="1.2 挑战"></a>1.2 挑战</h3><p>不规则采样时间序列的特点，给传统的机器学习模型带来了挑战，主要有以下三点：</p>
<ul>
<li><p>Irregular sampling - 采样时间间隔不固定。</p>
</li>
<li><p>Variable number of observations - 一个数据集内，每个样本的采样总次数可能不同。</p>
</li>
<li><p>Lack of alignment - 单个多变量时间序列内，每个维度的采样时间可能不同；每个样本在不同维度的采样时间也可能不同。</p>
</li>
</ul>
<h2 id="2-Data-Representations-for-Irregularly-Sampled-Time-Series"><a href="#2-Data-Representations-for-Irregularly-Sampled-Time-Series" class="headerlink" title="2. Data Representations for Irregularly Sampled Time Series"></a>2. Data Representations for Irregularly Sampled Time Series</h2><p>不规则采样序列的数据表示，主要有三种方式：Series-Based（基于序列的表示），Vector-Based（基于向量的表示），Set-Based（基于集合的表示）。<strong>不同的表示方式会影响建模方法的选择。</strong></p>
<h3 id="2-1-Series-Based-Representation"><a href="#2-1-Series-Based-Representation" class="headerlink" title="2.1 Series-Based Representation"></a>2.1 Series-Based Representation</h3><p>如果样本有多个维度，则将每个维度以$s_d = (t_{d}, x_{d})$的形式表示，再结合在一起。</p>
<p>其中，d表示所在的维度，$t_{d}$是d维度下的采样时间点，$x_{d}$是在$t_{d}$时间点采样的值。如下图所示：<br><img src="/2021/03/20/1/series-based.png" alt="series-based"><br>特点：</p>
<ul>
<li>不表示缺失数据，即不表示未被采样的时间点。</li>
<li>允许样本的每个维度的采样时间点不同。</li>
<li>如果在所有时间点都有采样，这种表示方式浪费空间。因为完全可以将同一时间点下不同维度的值放在一起表示，而不是每个维度单独表示后，再结合在一起。Vector-Based方式改进了这一点。</li>
</ul>
<h3 id="2-2-Vector-Based-Representation"><a href="#2-2-Vector-Based-Representation" class="headerlink" title="2.2 Vector-Based Representation"></a>2.2 Vector-Based Representation</h3><p>每个样本所有维度的观测值，以$(t, x_{id}, r_{id})$的形式表示。</p>
<p>其中，t是采样时间点，不同于Series-Based表示中的t，这里包括了所有维度的采样时间点。$x_{id}$是表示在d维度下，在i时间点采样的值（可为空值）。$r_{id}$是用来指示在d维度下，在i时间点是否采样。如下图所示：<br><img src="/2021/03/20/1/vector-based.PNG" alt="vector-based"><br>特点：</p>
<ul>
<li>可以表示缺失数据。</li>
<li>与Series-Based表示相比，如果在所有时间点都有采样，则更节约空间。</li>
</ul>
<h3 id="2-3-Set-Based-Representation"><a href="#2-3-Set-Based-Representation" class="headerlink" title="2.3 Set-Based Representation"></a>2.3 Set-Based Representation</h3><p>每个样本所有维度的观测值，以$(t, d, x)$的形式表示。</p>
<p>其中，t是采样时间点，d是维度，x是具体值。如下图所示：<br><img src="/2021/03/20/1/set-based.PNG" alt="set-based"><br>特点：<br>1、不表示缺失数据，与Series-Based一样。<br>2、<strong>并不一定要按数据被观测的时间顺序表示</strong>。与前两种表示方法都不同。</p>
<h3 id="2-4-思考"><a href="#2-4-思考" class="headerlink" title="2.4 思考"></a>2.4 思考</h3><p>个人认为Set-Based表示方法会有更广泛的应用场景，因为不再限制数据表示的先后顺序。</p>
<h2 id="3-Inference-Tasks"><a href="#3-Inference-Tasks" class="headerlink" title="3. Inference Tasks"></a>3. Inference Tasks</h2><p>推理任务并不局限于不规则采样时间序列这一领域，但通过给推理任务分类，可以更好地判断用于不规则采样时间序列的模型适用于哪些任务。</p>
<p>这些任务包括：Detection(检测)、Prediction(预测)、Forecasting(预报)、 Filtering(滤波)、 Smoothing(平滑)、Interpolation(插值)。</p>
<p>下图清晰地表示了每种任务的内容：<br><img src="/2021/03/20/1/Inference_Tasks.PNG" alt="Inference Tasks"></p>
<ul>
<li>Detection和Prediction任务，是推断某一时刻时间序列对应的标签值的。</li>
<li>其他任务是填补或预报时间序列本身的值。</li>
</ul>
<h2 id="4-Modeling-Primitives-for-Irregularly-Sampled-Time-Series"><a href="#4-Modeling-Primitives-for-Irregularly-Sampled-Time-Series" class="headerlink" title="4. Modeling Primitives for Irregularly Sampled Time Series"></a>4. Modeling Primitives for Irregularly Sampled Time Series</h2><p>建模原语是现有的模型的基础，一个模型可能包括一种或多种建模原语。</p>
<p>建模原语主要包括：Discretization(离散化)、Interpolation<br>(插值)、Recurrence(递推)、Attention(注意力)、Structural Invariance(结构不变性)。</p>
<p>下图展示了使用建模原语的一些模型和相关论文：<br><img src="/2021/03/20/1/methodsbasedonmodelingprimitives.PNG" alt="methods based on modeling primitives"></p>
<h3 id="4-1-Discretization"><a href="#4-1-Discretization" class="headerlink" title="4.1 Discretization"></a>4.1 Discretization</h3><p>离散化方法，将不规则采样时间序列变为规则时间序列。</p>
<p>通过将原时间序列的时间跨度，<strong>平均</strong>分为不重叠的间隔，这个间隔就是离散窗口。对每个离散窗口，根据落在窗口里的采样值，用取平均值等方式计算该窗口的值，从而得到以窗口大小为间隔、窗口值为新值的规则时间序列。<br><img src="/2021/03/20/1/discretation.PNG" alt="discretization"><br>表示方式为Vector-Based，NA表示该窗口的值缺失，缺失值可采用现有的手段填补。</p>
<p>离散窗口的大小选择十分重要，大窗口，丢失的数据量通常较少，但可能因为更多值落在同一离散化窗口内，导致聚合(aggregation)现象，使原序列想要表达的信息丢失。小窗口，聚合现象较弱，但丢失的数据量多。</p>
<h3 id="4-2-Interpolation"><a href="#4-2-Interpolation" class="headerlink" title="4.2 Interpolation"></a>4.2 Interpolation</h3><p>插值方法也是将不规则采样时间序列变得规则，但比离散化方法更灵活。个人认为是因为插值函数有多种，如使用高斯过程回归（Gaussian process regression，GPR）模型。</p>
<p>下图展示的是线性插值：<br><img src="/2021/03/20/1/interpolation.PNG" alt="interpolation"><br>表示方式为Series-Based。插值方法通常用于在不规则采样的时间序列数据和固定维特征空间或可变长度序列的模型之间提供接口。</p>
<p>可以基于相似性(Similarity)建模，如将相似性应用于多变量不规则采样时间序列。</p>
<h3 id="4-3-Recurrence"><a href="#4-3-Recurrence" class="headerlink" title="4.3 Recurrence"></a>4.3 Recurrence</h3><p>不同的样本可能包含不同数量的采样结果，可以使用递归方法来解决，即使用一组固定的、有限的参数来模拟任意长度的序列。<br><img src="/2021/03/20/1/recurrence.PNG" alt="recurrence"><br>表示方式为Vector-Based。当输入的不规则采样时间序列是单变量的，或是多变量、无缺失值的，则可以直接使用传统RNN模型，并丢弃时间点。我对丢弃时间点的理解是：使不规则采样的时间间隔变得规则，即与上两种方式的目标是一样的。但这种丢弃时间点的方式会使模型效果变差。</p>
<p>有一种解决方案，是使用基于常微分方程的RNN，也被称为ODE-RNN，在适应不规则采样数据的能力方面比传统RNN具有更好的性能。常微分方程用于演化连续时间观测之间的隐藏状态。</p>
<p>然而，对于多变量、有缺失数据的时间序列，就不能直接使用RNN了，因为在同一时间点，并不是所有维度都有采样值。需要与处理缺失数据的方法结合在一起使用。</p>
<h3 id="4-4-Attention"><a href="#4-4-Attention" class="headerlink" title="4.4 Attention"></a>4.4 Attention</h3><p>注意力机制，应用于许多机器学习任务。它比RNN具有更大的计算优势，<br>因为处理序列时可以完全并行化，不依赖RNN的结构。<br><img src="/2021/03/20/1/Attention.PNG" alt="Attention"><br>表示方式为Vector-Based，注意模型通过位置编码或时间编码，了解在不同时间点计算输出时，输入的不规则采样时间序列的哪些区域需要关注。</p>
<p>与RNN模型一样，需要与处理缺失数据的方法结合在一起使用。</p>
<h3 id="4-5-Structural-Invariance"><a href="#4-5-Structural-Invariance" class="headerlink" title="4.5 Structural Invariance"></a>4.5 Structural Invariance</h3><p>结构不变性，并不严格要求观测值的时间顺序。使用编码函数(encoding function)，处理单个集合表示元组，并对输出进行池化(pool)操作。<br><img src="/2021/03/20/1/structuralInvariance.PNG" alt="structural invariance"><br>表示方式为Set-Based。</p>
<h2 id="5-Approaches"><a href="#5-Approaches" class="headerlink" title="5. Approaches"></a>5. Approaches</h2><p>这一章节会介绍使用建模原语在论文中的使用。</p>
<h3 id="5-1-Discretization-Based-Approaches"><a href="#5-1-Discretization-Based-Approaches" class="headerlink" title="5.1 Discretization-Based Approaches"></a>5.1 Discretization-Based Approaches</h3><p>离散化方法一般要与缺失数据的处理结合使用。结合插补和任何监督模型，为解决整个时间序列分类和回归问题提供了一个简单、易于实现和模块化的基线(baseline)。将应答指标(response indicators)作为额外输入有助于改善表现效果。这里的应答指标，我的理解是样本标签。</p>
<p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/abs/10.1145/2110363.2110408">Marlin et al. (2012)</a> 使用了概率混合模型，这样就无需显式地填补缺失值。利用这种特性，作者定义了一种通用的混合<strong>分类</strong>器，对整个时间序列进行分类。</p>
<p><a target="_blank" rel="noopener" href="http://proceedings.mlr.press/v56/Lipton16">Lipton et al. (2016)</a> 应用离散化和RNN建立了一个完整的时间序列<strong>分类</strong>模型，需要对缺失数据进行明确插补。他们考虑了正向填充法和零插补法。</p>
<p><a target="_blank" rel="noopener" href="https://www.nature.com/articles/s41597-019-0103-9">Harutyunyan et al. (2019)</a> 在离散化窗口中应用平均值或选择间隔中的最后一个时间点来处理多个观测值，并使用平均值插补或正向填充来处理缺失值。亮点是通过<strong>联合</strong>预测多个任务的输出，来扩充标准RNN模型。与标准RNN不同，该模型并不是只对最后一个位置的隐层输出进行解码(decoder)，而是在每个时间步都做了解码，以进行监督学习。该框架已应用于全时间序列<strong>分类</strong>任务、<strong>检测</strong>任务和<strong>预测</strong>任务。</p>
<p><a target="_blank" rel="noopener" href="https://ojs.aaai.org/index.php/AAAI/article/view/11635">Song et al. (2018)</a> 遵循Harutyunyan et al.(2019)，但采用了类似于Vaswani et al.(2017)的多头注意机制(multi-head attention mechanism)，而不是RNN作为主要模型结构。使用了位置编码，并使用密集插值技术（Trask et al., 2015）获得序列的统一表示。</p>
<p>一些方法还采用了encoder-decoder框架来学习时间序列中的缺失数据，这种框架也适用于离散化后的不规则采样时间序列。<a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/abs/pii/S0031320319302766">Bianchi et al. (2019)</a>提出了一种基于自动编码器(autoencoder)的方法，用于学习整个时间序列<strong>分类和平滑</strong>任务中缺失数据的多元时间序列表示，还介绍了一个内核对齐(kernel alignment)过程，以保持学习表示中输入的成对相似性。<a target="_blank" rel="noopener" href="http://proceedings.mlr.press/v108/fortuin20a.html">Fortuin et al.(2020)</a>提出了一种变分自动编码器（VAE）的方法，用于在潜在空间中使用高斯过程对多变量时间序列进行<strong>平滑</strong>，以捕获时间动态。</p>
<h3 id="5-2-Interpolation-Based-Approaches"><a href="#5-2-Interpolation-Based-Approaches" class="headerlink" title="5.2 Interpolation-Based Approaches"></a>5.2 Interpolation-Based Approaches</h3><p>插值比离散化方法更复杂。迄今为止，在这一领域探索的两个主要方法是确定性核平滑方法，如IP-Net，和基于概率高斯过程的方法。这两种方法都能自然地适应连续时间观测，并能为其他建模构件（如核机器和神经网络）提供接口。</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1909.07782">Shukla and Marlin (2019)</a> 提出了插值预测网络（IP-Net）框架，该框架使用多个半参数径向基函数（RBF）插值层，在给定不规则采样的多元时间序列作为输入的情况下生成多个插值。预测网络可以由任何标准的有监督神经网络结构组成（全连接前馈、卷积、递归等），已应用于整个时间序列的<strong>回归和分类</strong>。由于该框架使用固定的时间RBF核和可调参数，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2101.10318">Shukla和Marlin（2020）</a> 在此基础上用可学习的连续时间值嵌入和时间注意机制构建的可学习时间相似函数，来替换IP-Net框架中的固定RBF核。该框架已应用于<strong>插值、分类、检测</strong>任务。</p>
<p><a target="_blank" rel="noopener" href="http://proceedings.mlr.press/v119/li20k.html">Li and Marlin (2020)</a> 提出了一种用于不规则采样时间序列数据建模的encoder-decoder结构。该框架中的编码器基于分段线性函数，该函数学习一组固定参考时间点上的插值。</p>
<p>注：这一小节中还介绍了其他论文，但理论较为晦涩，我暂时看不懂。以后有机会再补充。</p>
<p>基于确定性核平滑的方法，在许多任务都优于基于RNN和ODE的方法。这些模型也被证明比高斯过程回归和基于ODE的模型训练得更快。</p>
<h3 id="5-3-Recurrence-Based-Approaches"><a href="#5-3-Recurrence-Based-Approaches" class="headerlink" title="5.3 Recurrence-Based Approaches"></a>5.3 Recurrence-Based Approaches</h3><p>递归方法，主要是用在了以RNN为基础的模型。再细分可以分为基于传统RNN的，和基于ODE模型的。</p>
<p>基本离散RNN比插值和平滑更适合于检测和预测任务。双向递归模型(Bi-directional recurrent models)在有监督任务的情况下的效果一般比单向模型好。使用时间戳或时间增量(time deltas)作为额外输入，似乎有助于克服表示规则序列和表示不规则采样时间序列之间的差距。RNN必须处理有缺失值的数据，更为复杂的模型组件（例如衰变机制）的端到端学习方法似乎有助于进行插补。</p>
<p>基于ODE的递归方法比离散RNN提供了处理不规则采样的更优雅的解决方案，在许多任务中，优于基于离散RNN的模型。然而，ODE-RNN模型仍然存在局限性，即不能适应不完全向量值观测。基于神经CDE的模型族在通过时间整合观测的能力方面，似乎比ODE-RNN模型具有许多优势。</p>
<h3 id="5-4-Attention-Based-Approaches"><a href="#5-4-Attention-Based-Approaches" class="headerlink" title="5.4 Attention-Based Approaches"></a>5.4 Attention-Based Approaches</h3><p>注意力机制在模型中的应用，一般是使用自注意机制(self-attention)和模型序列编码来代替常见的位置编码。</p>
<p><a target="_blank" rel="noopener" href="https://par.nsf.gov/biblio/10136491">Zhang et al. (2019)</a> 提出了基于注意的时间感知(time-aware)方法，使用了LSTM网络结构，并使用了注意力权重和时间间隔来组合前面所有细胞状态的值。</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1608.05745">Choi et al. (2016b)</a> 使用两级注意模型学习不规则采样事件的可解释表示。attention向量是通过反向运行RNN生成的。通过将相应的时间戳与事件嵌入连接起来，可以提高性能。</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1911.12864">Xu et al. (2019)</a> 提出了用于泛函时间表征学习(functional time representation learning)的时间嵌入和自注意机制结合的方法。关于functional time representation learning的定义，我还不是很了解。</p>
<p><a target="_blank" rel="noopener" href="http://proceedings.mlr.press/v119/horn20a.html">Horn et al. (2020)</a> 采用基于transformer的方法，对不规则采样时间序列进行建模。</p>
<h3 id="5-5-Structural-Invariance-Based-Approaches"><a href="#5-5-Structural-Invariance-Based-Approaches" class="headerlink" title="5.5 Structural Invariance-Based Approaches"></a>5.5 Structural Invariance-Based Approaches</h3><p>结构不变性方法，利用不规则采样时间序列的set-based表示来建立模型，其结构完全不依赖于输入时间序列的时间顺序。</p>
<p>在最近的工作中，<a target="_blank" rel="noopener" href="http://proceedings.mlr.press/v119/horn20a.html">Horn et al. (2020)</a> 提出了一种基于集合函数的方法，用于对具有不规则采样和未对齐观测值的时间序列进行分类和检测。</p>
<p>基于注意和结构不变性的方法都打破了递归方法的连续性，因此相对于递归模型有可能减少训练时间。然而，到目前为止，这些模型已经能够满足但不超过ODE和基于插值的方法的性能。</p>
<h2 id="6-Discussion-and-Directions-for-Future-Work"><a href="#6-Discussion-and-Directions-for-Future-Work" class="headerlink" title="6. Discussion and Directions for Future Work"></a>6. Discussion and Directions for Future Work</h2><p>结合文章所说和我自己的思考，基于<strong>Attention和Structural Invariance</strong>有较好的应用前景，可以利用增强的并行计算来提高精度和速度。这是对比其他建模原语的优势。</p>
<p>在递归模型领域内，与基于ODE的模型相比，<strong>神经cde</strong>更有优势，也是值得探索的领域。</p>
<p>机器学习领域的最新研究显然主要集中在监督问题上，其次是插值和平滑。对预测任务的关注明显减少。所以，开发从不规则采样输入中学习<strong>准确预测</strong>的方法，也是一个方向。</p>
<p>在性能评估方面，这里考虑的大多数方法都集中在精度和均方误差等标准上，并不考虑模型<strong>输出的不确定性</strong>。除了基于高斯过程的模型外，应用于插值和平滑任务的大多数模型根本不产生作为输出的概率分布。在追求更快的训练时间和提高多元环境下的灵活性的过程中，基于确定性插值的模型牺牲了基于高斯过程的模型将输入稀疏性转化为预测输出的不确定性的能力。恢复敏感地传播不确定性(sensibly propagate uncertainty)的能力是当前基于确定性深度学习模型的一个关键挑战。</p>
<h2 id="7-Datasets"><a href="#7-Datasets" class="headerlink" title="7. Datasets"></a>7. Datasets</h2><p>文章整理了一些在真实世界中不规则采样的数据集，前四个与医疗信息相关，最后一个和人的活动信息相关：<a target="_blank" rel="noopener" href="https://mimic.physionet.org/">MIMIC-III</a>, <a target="_blank" rel="noopener" href="https://physionet.org/content/challenge-2012/">PhysioNet 2012</a>, <a target="_blank" rel="noopener" href="https://eicu-crd.mit.edu/">eICU Collaborative Research Data Set</a>, <a target="_blank" rel="noopener" href="https://physionet.org/content/challenge-2019/1.0.0/">PhysioNet 2019</a>, <a target="_blank" rel="noopener" href="https://archive.ics.uci.edu/ml/datasets/Localization+Data+for+Person+Activity">Human Activity</a>。</p>
</div><div class="article-licensing box"><div class="licensing-title"><p>论文阅读笔记：A Survey on Principles, Models and Methods for Learning from Irregularly Sampled Time Series</p><p><a href="http://harry-zzh.github.io/2021/03/20/1/">http://harry-zzh.github.io/2021/03/20/1/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>harry-zzh</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2021-03-20</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2021-03-28</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icon" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a><a class="icon" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a><a class="icon" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><hr style="height:1px;margin:1rem 0"><div class="level is-mobile is-flex"><div class="article-tags is-size-7 is-uppercase"><i class="fas fa-tags has-text-grey"></i> <a class="link-muted" rel="tag" href="/tags/%E4%B8%8D%E8%A7%84%E5%88%99%E9%87%87%E6%A0%B7/">不规则采样, </a><a class="link-muted" rel="tag" href="/tags/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97/">时间序列, </a><a class="link-muted" rel="tag" href="/tags/srp%E5%91%A8%E6%8A%A5/">srp周报 </a></div></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2021/03/22/survey-prediction%EF%BC%881%EF%BC%89/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">论文阅读笔记：不规则采样时间序列的预测任务（1）</span></a></div></nav><!--!--></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/avatar.png" alt="harry-zzh"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">harry-zzh</p><p class="is-size-6 is-block">华南理工大学 计科</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>广东 广州</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">31</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">3</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">7</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/Harry-zzh" target="_blank" rel="noopener">Follow</a></div></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="level is-mobile" href="#1-Introduction"><span class="level-left"><span class="level-item">1. Introduction</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#1-1-定义"><span class="level-left"><span class="level-item">1.1 定义</span></span></a></li><li><a class="level is-mobile" href="#1-2-挑战"><span class="level-left"><span class="level-item">1.2 挑战</span></span></a></li></ul></li><li><a class="level is-mobile" href="#2-Data-Representations-for-Irregularly-Sampled-Time-Series"><span class="level-left"><span class="level-item">2. Data Representations for Irregularly Sampled Time Series</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#2-1-Series-Based-Representation"><span class="level-left"><span class="level-item">2.1 Series-Based Representation</span></span></a></li><li><a class="level is-mobile" href="#2-2-Vector-Based-Representation"><span class="level-left"><span class="level-item">2.2 Vector-Based Representation</span></span></a></li><li><a class="level is-mobile" href="#2-3-Set-Based-Representation"><span class="level-left"><span class="level-item">2.3 Set-Based Representation</span></span></a></li><li><a class="level is-mobile" href="#2-4-思考"><span class="level-left"><span class="level-item">2.4 思考</span></span></a></li></ul></li><li><a class="level is-mobile" href="#3-Inference-Tasks"><span class="level-left"><span class="level-item">3. Inference Tasks</span></span></a></li><li><a class="level is-mobile" href="#4-Modeling-Primitives-for-Irregularly-Sampled-Time-Series"><span class="level-left"><span class="level-item">4. Modeling Primitives for Irregularly Sampled Time Series</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#4-1-Discretization"><span class="level-left"><span class="level-item">4.1 Discretization</span></span></a></li><li><a class="level is-mobile" href="#4-2-Interpolation"><span class="level-left"><span class="level-item">4.2 Interpolation</span></span></a></li><li><a class="level is-mobile" href="#4-3-Recurrence"><span class="level-left"><span class="level-item">4.3 Recurrence</span></span></a></li><li><a class="level is-mobile" href="#4-4-Attention"><span class="level-left"><span class="level-item">4.4 Attention</span></span></a></li><li><a class="level is-mobile" href="#4-5-Structural-Invariance"><span class="level-left"><span class="level-item">4.5 Structural Invariance</span></span></a></li></ul></li><li><a class="level is-mobile" href="#5-Approaches"><span class="level-left"><span class="level-item">5. Approaches</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#5-1-Discretization-Based-Approaches"><span class="level-left"><span class="level-item">5.1 Discretization-Based Approaches</span></span></a></li><li><a class="level is-mobile" href="#5-2-Interpolation-Based-Approaches"><span class="level-left"><span class="level-item">5.2 Interpolation-Based Approaches</span></span></a></li><li><a class="level is-mobile" href="#5-3-Recurrence-Based-Approaches"><span class="level-left"><span class="level-item">5.3 Recurrence-Based Approaches</span></span></a></li><li><a class="level is-mobile" href="#5-4-Attention-Based-Approaches"><span class="level-left"><span class="level-item">5.4 Attention-Based Approaches</span></span></a></li><li><a class="level is-mobile" href="#5-5-Structural-Invariance-Based-Approaches"><span class="level-left"><span class="level-item">5.5 Structural Invariance-Based Approaches</span></span></a></li></ul></li><li><a class="level is-mobile" href="#6-Discussion-and-Directions-for-Future-Work"><span class="level-left"><span class="level-item">6. Discussion and Directions for Future Work</span></span></a></li><li><a class="level is-mobile" href="#7-Datasets"><span class="level-left"><span class="level-item">7. Datasets</span></span></a></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/favicon.svg" alt="harry-zzh的博客" height="28"></a><p class="is-size-7"><span>&copy; 2021 harry-zzh</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/Harry-zzh"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>