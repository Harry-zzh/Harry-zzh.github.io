{"pages":[],"posts":[{"title":"论文阅读笔记：A Survey on Principles, Models and Methods for Learning from Irregularly Sampled Time Series","text":"本篇综述介绍了不规则采样时间序列的表示形式、建模原语以及它们所承担的推理任务，并调研了与建模原语有关的最新文献。之后，讨论了基于时间离散化，内插，递归，注意力和结构不变性的方法和方法之间的异同，并强调了主要的优点和缺点。 1. Introduction1.1 定义不规则采样时间序列，是指采样时间（也称观测时间）间隔不固定的序列。如上图，进一步细分，可以分为单变量不规则采样和多变量不规则采样。多变量不规则采样中，不同维度的采样时间可能对齐(aligned)，也可能不对齐(unaligned)。 1.2 挑战不规则采样时间序列的特点，给传统的机器学习模型带来了挑战，主要有以下三点： Irregular sampling - 采样时间间隔不固定。 Variable number of observations - 一个数据集内，每个样本的采样总次数可能不同。 Lack of alignment - 单个多变量时间序列内，每个维度的采样时间可能不同；每个样本在不同维度的采样时间也可能不同。 2. Data Representations for Irregularly Sampled Time Series不规则采样序列的数据表示，主要有三种方式：Series-Based（基于序列的表示），Vector-Based（基于向量的表示），Set-Based（基于集合的表示）。不同的表示方式会影响建模方法的选择。 2.1 Series-Based Representation如果样本有多个维度，则将每个维度以$s_d = (t_{d}, x_{d})$的形式表示，再结合在一起。 其中，d表示所在的维度，$t_{d}$是d维度下的采样时间点，$x_{d}$是在$t_{d}$时间点采样的值。如下图所示：特点： 不表示缺失数据，即不表示未被采样的时间点。 允许样本的每个维度的采样时间点不同。 如果在所有时间点都有采样，这种表示方式浪费空间。因为完全可以将同一时间点下不同维度的值放在一起表示，而不是每个维度单独表示后，再结合在一起。Vector-Based方式改进了这一点。 2.2 Vector-Based Representation每个样本所有维度的观测值，以$(t, x_{id}, r_{id})$的形式表示。 其中，t是采样时间点，不同于Series-Based表示中的t，这里包括了所有维度的采样时间点。$x_{id}$是表示在d维度下，在i时间点采样的值（可为空值）。$r_{id}$是用来指示在d维度下，在i时间点是否采样。如下图所示：特点： 可以表示缺失数据。 与Series-Based表示相比，如果在所有时间点都有采样，则更节约空间。 2.3 Set-Based Representation每个样本所有维度的观测值，以$(t, d, x)$的形式表示。 其中，t是采样时间点，d是维度，x是具体值。如下图所示：特点：1、不表示缺失数据，与Series-Based一样。2、并不一定要按数据被观测的时间顺序表示。与前两种表示方法都不同。 2.4 思考个人认为Set-Based表示方法会有更广泛的应用场景，因为不再限制数据表示的先后顺序。 3. Inference Tasks推理任务并不局限于不规则采样时间序列这一领域，但通过给推理任务分类，可以更好地判断用于不规则采样时间序列的模型适用于哪些任务。 这些任务包括：Detection(检测)、Prediction(预测)、Forecasting(预报)、 Filtering(滤波)、 Smoothing(平滑)、Interpolation(插值)。 下图清晰地表示了每种任务的内容： Detection和Prediction任务，是推断某一时刻时间序列对应的标签值的。 其他任务是填补或预报时间序列本身的值。 4. Modeling Primitives for Irregularly Sampled Time Series建模原语是现有的模型的基础，一个模型可能包括一种或多种建模原语。 建模原语主要包括：Discretization(离散化)、Interpolation(插值)、Recurrence(递推)、Attention(注意力)、Structural Invariance(结构不变性)。 下图展示了使用建模原语的一些模型和相关论文： 4.1 Discretization离散化方法，将不规则采样时间序列变为规则时间序列。 通过将原时间序列的时间跨度，平均分为不重叠的间隔，这个间隔就是离散窗口。对每个离散窗口，根据落在窗口里的采样值，用取平均值等方式计算该窗口的值，从而得到以窗口大小为间隔、窗口值为新值的规则时间序列。表示方式为Vector-Based，NA表示该窗口的值缺失，缺失值可采用现有的手段填补。 离散窗口的大小选择十分重要，大窗口，丢失的数据量通常较少，但可能因为更多值落在同一离散化窗口内，导致聚合(aggregation)现象，使原序列想要表达的信息丢失。小窗口，聚合现象较弱，但丢失的数据量多。 4.2 Interpolation插值方法也是将不规则采样时间序列变得规则，但比离散化方法更灵活。个人认为是因为插值函数有多种，如使用高斯过程回归（Gaussian process regression，GPR）模型。 下图展示的是线性插值：表示方式为Series-Based。插值方法通常用于在不规则采样的时间序列数据和固定维特征空间或可变长度序列的模型之间提供接口。 可以基于相似性(Similarity)建模，如将相似性应用于多变量不规则采样时间序列。 4.3 Recurrence不同的样本可能包含不同数量的采样结果，可以使用递归方法来解决，即使用一组固定的、有限的参数来模拟任意长度的序列。表示方式为Vector-Based。当输入的不规则采样时间序列是单变量的，或是多变量、无缺失值的，则可以直接使用传统RNN模型，并丢弃时间点。我对丢弃时间点的理解是：使不规则采样的时间间隔变得规则，即与上两种方式的目标是一样的。但这种丢弃时间点的方式会使模型效果变差。 有一种解决方案，是使用基于常微分方程的RNN，也被称为ODE-RNN，在适应不规则采样数据的能力方面比传统RNN具有更好的性能。常微分方程用于演化连续时间观测之间的隐藏状态。 然而，对于多变量、有缺失数据的时间序列，就不能直接使用RNN了，因为在同一时间点，并不是所有维度都有采样值。需要与处理缺失数据的方法结合在一起使用。 4.4 Attention注意力机制，应用于许多机器学习任务。它比RNN具有更大的计算优势，因为处理序列时可以完全并行化，不依赖RNN的结构。表示方式为Vector-Based，注意模型通过位置编码或时间编码，了解在不同时间点计算输出时，输入的不规则采样时间序列的哪些区域需要关注。 与RNN模型一样，需要与处理缺失数据的方法结合在一起使用。 4.5 Structural Invariance结构不变性，并不严格要求观测值的时间顺序。使用编码函数(encoding function)，处理单个集合表示元组，并对输出进行池化(pool)操作。表示方式为Set-Based。 5. Approaches这一章节会介绍使用建模原语在论文中的使用。 5.1 Discretization-Based Approaches离散化方法一般要与缺失数据的处理结合使用。结合插补和任何监督模型，为解决整个时间序列分类和回归问题提供了一个简单、易于实现和模块化的基线(baseline)。将应答指标(response indicators)作为额外输入有助于改善表现效果。这里的应答指标，我的理解是样本标签。 Marlin et al. (2012) 使用了概率混合模型，这样就无需显式地填补缺失值。利用这种特性，作者定义了一种通用的混合分类器，对整个时间序列进行分类。 Lipton et al. (2016) 应用离散化和RNN建立了一个完整的时间序列分类模型，需要对缺失数据进行明确插补。他们考虑了正向填充法和零插补法。 Harutyunyan et al. (2019) 在离散化窗口中应用平均值或选择间隔中的最后一个时间点来处理多个观测值，并使用平均值插补或正向填充来处理缺失值。亮点是通过联合预测多个任务的输出，来扩充标准RNN模型。与标准RNN不同，该模型并不是只对最后一个位置的隐层输出进行解码(decoder)，而是在每个时间步都做了解码，以进行监督学习。该框架已应用于全时间序列分类任务、检测任务和预测任务。 Song et al. (2018) 遵循Harutyunyan et al.(2019)，但采用了类似于Vaswani et al.(2017)的多头注意机制(multi-head attention mechanism)，而不是RNN作为主要模型结构。使用了位置编码，并使用密集插值技术（Trask et al., 2015）获得序列的统一表示。 一些方法还采用了encoder-decoder框架来学习时间序列中的缺失数据，这种框架也适用于离散化后的不规则采样时间序列。Bianchi et al. (2019)提出了一种基于自动编码器(autoencoder)的方法，用于学习整个时间序列分类和平滑任务中缺失数据的多元时间序列表示，还介绍了一个内核对齐(kernel alignment)过程，以保持学习表示中输入的成对相似性。Fortuin et al.(2020)提出了一种变分自动编码器（VAE）的方法，用于在潜在空间中使用高斯过程对多变量时间序列进行平滑，以捕获时间动态。 5.2 Interpolation-Based Approaches插值比离散化方法更复杂。迄今为止，在这一领域探索的两个主要方法是确定性核平滑方法，如IP-Net，和基于概率高斯过程的方法。这两种方法都能自然地适应连续时间观测，并能为其他建模构件（如核机器和神经网络）提供接口。 Shukla and Marlin (2019) 提出了插值预测网络（IP-Net）框架，该框架使用多个半参数径向基函数（RBF）插值层，在给定不规则采样的多元时间序列作为输入的情况下生成多个插值。预测网络可以由任何标准的有监督神经网络结构组成（全连接前馈、卷积、递归等），已应用于整个时间序列的回归和分类。由于该框架使用固定的时间RBF核和可调参数，Shukla和Marlin（2020） 在此基础上用可学习的连续时间值嵌入和时间注意机制构建的可学习时间相似函数，来替换IP-Net框架中的固定RBF核。该框架已应用于插值、分类、检测任务。 Li and Marlin (2020) 提出了一种用于不规则采样时间序列数据建模的encoder-decoder结构。该框架中的编码器基于分段线性函数，该函数学习一组固定参考时间点上的插值。 注：这一小节中还介绍了其他论文，但理论较为晦涩，我暂时看不懂。以后有机会再补充。 基于确定性核平滑的方法，在许多任务都优于基于RNN和ODE的方法。这些模型也被证明比高斯过程回归和基于ODE的模型训练得更快。 5.3 Recurrence-Based Approaches递归方法，主要是用在了以RNN为基础的模型。再细分可以分为基于传统RNN的，和基于ODE模型的。 基本离散RNN比插值和平滑更适合于检测和预测任务。双向递归模型(Bi-directional recurrent models)在有监督任务的情况下的效果一般比单向模型好。使用时间戳或时间增量(time deltas)作为额外输入，似乎有助于克服表示规则序列和表示不规则采样时间序列之间的差距。RNN必须处理有缺失值的数据，更为复杂的模型组件（例如衰变机制）的端到端学习方法似乎有助于进行插补。 基于ODE的递归方法比离散RNN提供了处理不规则采样的更优雅的解决方案，在许多任务中，优于基于离散RNN的模型。然而，ODE-RNN模型仍然存在局限性，即不能适应不完全向量值观测。基于神经CDE的模型族在通过时间整合观测的能力方面，似乎比ODE-RNN模型具有许多优势。 5.4 Attention-Based Approaches注意力机制在模型中的应用，一般是使用自注意机制(self-attention)和模型序列编码来代替常见的位置编码。 Zhang et al. (2019) 提出了基于注意的时间感知(time-aware)方法，使用了LSTM网络结构，并使用了注意力权重和时间间隔来组合前面所有细胞状态的值。 Choi et al. (2016b) 使用两级注意模型学习不规则采样事件的可解释表示。attention向量是通过反向运行RNN生成的。通过将相应的时间戳与事件嵌入连接起来，可以提高性能。 Xu et al. (2019) 提出了用于泛函时间表征学习(functional time representation learning)的时间嵌入和自注意机制结合的方法。关于functional time representation learning的定义，我还不是很了解。 Horn et al. (2020) 采用基于transformer的方法，对不规则采样时间序列进行建模。 5.5 Structural Invariance-Based Approaches结构不变性方法，利用不规则采样时间序列的set-based表示来建立模型，其结构完全不依赖于输入时间序列的时间顺序。 在最近的工作中，Horn et al. (2020) 提出了一种基于集合函数的方法，用于对具有不规则采样和未对齐观测值的时间序列进行分类和检测。 基于注意和结构不变性的方法都打破了递归方法的连续性，因此相对于递归模型有可能减少训练时间。然而，到目前为止，这些模型已经能够满足但不超过ODE和基于插值的方法的性能。 6. Discussion and Directions for Future Work结合文章所说和我自己的思考，基于Attention和Structural Invariance有较好的应用前景，可以利用增强的并行计算来提高精度和速度。这是对比其他建模原语的优势。 在递归模型领域内，与基于ODE的模型相比，神经cde更有优势，也是值得探索的领域。 机器学习领域的最新研究显然主要集中在监督问题上，其次是插值和平滑。对预测任务的关注明显减少。所以，开发从不规则采样输入中学习准确预测的方法，也是一个方向。 在性能评估方面，这里考虑的大多数方法都集中在精度和均方误差等标准上，并不考虑模型输出的不确定性。除了基于高斯过程的模型外，应用于插值和平滑任务的大多数模型根本不产生作为输出的概率分布。在追求更快的训练时间和提高多元环境下的灵活性的过程中，基于确定性插值的模型牺牲了基于高斯过程的模型将输入稀疏性转化为预测输出的不确定性的能力。恢复敏感地传播不确定性(sensibly propagate uncertainty)的能力是当前基于确定性深度学习模型的一个关键挑战。 7. Datasets文章整理了一些在真实世界中不规则采样的数据集，前四个与医疗信息相关，最后一个和人的活动信息相关：MIMIC-III, PhysioNet 2012, eICU Collaborative Research Data Set, PhysioNet 2019, Human Activity。","link":"/2021/03/20/1/"},{"title":"论文阅读笔记：不规则采样时间序列的预测任务（1）","text":"上周阅读了关于不规则采样时间序列的综述，提到了未来几个可能的发展方向，其中有一个方向与预测任务相关。因此，这周任务是调研有关不规则采样时间序列预测的论文，并进行总结。共调研了三篇论文： Learning Adaptive Forecasting Models from Irregularly Sampled Multivariate Clinical Data (AAAI, 2016) Attend and Diagnose: Clinical Time Series Analysis Using Attention Models (AAAI, 2018) Scalable Joint Models for Reliable Uncertainty-Aware Event Prediction(TPAMI, 2018) 1. Learning Adaptive Forecasting Models from Irregularly Sampled Multivariate Clinical Data1.1 摘要建立一个对多元不规则采样时间序列作预测的模型，主要针对医疗方面。医疗方面的时间序列数据，是比较经典的不规则采样时间序列。 1.2 模型我认为本文最大的创新点有两点： 一般和特殊的结合。患者之间有个体差异，然而在时间序列中能够体现这种个体差异的数据稀疏，且跨度短。因此，本文先训练适应性广的一般模型，再训练能够捕获不同患者特有变化的模型，即two-stage approach。将两个模型在某时间点的预测值结合，得到该患者在该时间点的状态的预测结果。 动态适应新观测值。根据新的观测值，能够重新调整模型参数，以达到更好的预测效果。 具体算法如下所示： 训练population模型：使用离散化方法(Discretization)和直接插值技术(Direct Value Interpolation，即DVI)，将已观察到的数据离散化，获得规则的时间序列。使用EM算法(Expectation Maximization)，训练模型参数。模型目标是反映患者状态变化的一般趋势。 训练multivariate interaction模型：使用训练好的population模型，对已观察到的时间点的作预测，根据预测值和观察值，计算残差。利用残差和MGTP(Multi-task Gaussian Process)，训练模型参数。模型目标是捕获不同患者特有的短期变化。 动态预测：对于目标时刻t，使用训练好的population模型，预测接近t时刻的值，利用插值方法，获得t的预测值，设为y1。使用训练好的multivariate interaction模型，预测t时刻的值，设为y2。y1+y2，即为t时刻的最终预测值。 1.3 思考 根据一般规律和特殊规律建立预测模型，最后将两者结合的预测值结合。这种思路值得借鉴与参考。 当有新观测值时，模型要重新调整参数，这样的做法可能会导致较高的时间复杂度。 包括本文在内的不少论文都提到过高斯过程（GP），是一个常见的建模方法，所以我去学习了一下，具体内容会整理在另一篇文章。 2. Attend and Diagnose: Clinical Time Series Analysis Using Attention Models2.1 摘要SAnD (Simply Attend and Diagnose) 仅用基于带掩膜的自注意力机制(masked self-attention)来建立模型，使用了位置编码和密集插值技术。使用了多任务学习（也包括预测任务）的方式，来训练模型参数。 2.2 模型模型的基本架构如下所示： 该模型并未直接使用transformer模型，而是做出了两点修改： 只是使用了transformer模型的encoder部分，即在Dense Interpolation(密集插值)前的步骤。使用密集插值技术，是为了防止encoder输出向量维度过高。 使用了masked self-attention。transformer模型应用于翻译任务时，对于每个单词，要创建一个查询向量Q、一个键向量K和一个值向量V。本文也是如此，只不过添加了一个masked size，假设为r。个人理解是，确定r之后，表示t时刻的值要和t - r ~ t - 1时刻的观测值产生联系。所以在下面这一核心公式时，K和V只需将这一范围的观测值代入计算，而不是所有的观测值： Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d}})V这也是我认为本文的创新点之一。 本文还有一个创新点，就是使用了多任务学习的方式来训练参数。上图中最后的softmax层，可以根据具体任务换成其他层，比如sigmoid层用于多标签分类任务、ReLU层用于回归任务。用一个损失函数，将所有任务的损失函数以不同权重结合起来：这里的$\\lambda$是权重，$l$是对应任务的损失。本文使用了MIMIC-III医疗数据集，以此为基础进行了分类、预测等共计四个任务。结果表明多任务学习的效果还不错。 2.3 思考 RNN难以并行化，所以本文考虑使用注意力机制。 第一次接触transformer模型，这篇文章解释得很清楚：图解Transformer（完整版） 多任务学习来训练参数的方式，可能比单任务学习更好。 3. Scalable Joint Models for Reliable Uncertainty-Aware Event Prediction3.1 摘要通过建立联合模型，缓解缺失数据和噪声观测对不规则采样的多元时间序列（纵向）的影响。利用联合模型估计的事件发生率分布，推导出预测事件的最优策略。 3.2 模型该联合模型分为了两个子模型：longitudinal子模型和time-to-event子模型。前者使用了高斯过程进行训练，得到时间序列的分布；后者根据前者估计的分布，来计算事件概率。这两个子模型进行联合学习，目标是最大化数据的联合可能性。 我认为本文的创新点有： 在longitudinal子模型使用了高斯过程，不用进行强参数假设，可以拓展到大数据集。 推导出预测事件的最优策略，权衡了延迟检测和错误评估的成本，并在估计的事件概率不满足导出的置信标准时避免做出决策。 3.3 思考 可以本文的方法，缓解不规则采样时间序列中，数据缺失的影响。 4. 小结这一周收获许多，学到了一些新的知识，如GP，transformer模型等。但由于打比赛的缘故，我来不及对第三篇文章进行完全解读，之后阅读完后会更新这一部分。 下周计划继续进行该方向的文章阅读。","link":"/2021/03/22/survey-prediction%EF%BC%881%EF%BC%89/"},{"title":"线性代数：第1章 行列式","text":"在学习深度学习知识时，掌握线性代数的基本知识是必要的。而且这学期开设的数值分析课程，也与线性代数知识密切相关。因此，我计划复习一下线性代数，使用的教材是同济大学的《线性代数第六版》，并将复习笔记记录在此。 第1章介绍了行列式的基本知识，重点掌握行列式的性质和展开方法。 1. 全排列和对换1.1 排列及其逆序数 (全)排列的逆序数：该排列所有逆序的总数 当某对元素的先后次序与标准次序（从小到大排列）不同时，则构成1个逆序。 逆序数为奇数的是奇排列，逆序数为偶数的是偶排列 标准排列、自然排列：1，2，3，… ，n，逆序数为0，是个偶排列。 1.2 对换 定理1：一个排列中任意两个元素对换，排列改变奇偶性 证明：相邻对换（改变1次奇偶性）=&gt;一般对换（奇数次相邻对换） 推论：奇排列对换成标准排列的对换次数是奇数，偶排列对换成标准排列的对换次数是偶数。 此定理在行列式的性质里很重要。 2. n阶行列式的定义 n阶行列式，记作：D = \\left| \\begin{matrix} a_{11} & a_{12} & ... & a_{1n} \\\\ a_{21} & a_{22} & ... & a_{2n} \\\\ \\vdots & \\vdots & \\vdots & \\vdots \\\\ a_{n1} & a_{n2} & ... & a_{nn} \\end{matrix} \\right| 简记为$det(a_{ij})$，值为$\\sum{(-1)^ta_{1p_1}a_{2p_2}…a_{np_n}}$，t是列标排列的逆序数。可以想一下三阶行列式的计算。 三角形行列式和对角行列式的值，就是对角线元素的乘积（由 n阶行列式的定义可知）。 3. 行列式的性质 性质1：行列式与它的转置行列式相等。 性质2：某一行所有元素有公因子k，则可以把k提到外面。 性质3：对换行列式的两行/列，行列式变号。 由n阶行列式的定义和定理1可知。 推论： 如果行列式有两行/列完全相同，则此行列式等于0 如果有两行/列成比例，则此行列式等于0（结合性质2） 性质4：某行所有元素都是两个数的和，则可把行列式写成两个行列式之和。 如果n阶行列式，每一行都是两个数的和，则可写成2^n个行列式之和。（根据n阶行列式的定义，拆开来看） 性质5：把某行的k倍加到另一行，行列式的值不变。 综合性质2，性质3的推论，性质4。 4. 行列式按行/列展开4.1 代数余子式 代数余子式$A_{ij} = (-1)^{i+j} M_{ij}$。 $M_{ij}$即为余子式，是把矩阵元素$a_{ij}$所在的第i行、第j列去掉后，留下来的n - 1阶行列式。 4.2 行列式展开 定理2：行列式等于它的任一行/列与其对应的代数余子式乘积之和。 这是由引理推导得到的，引理说明：一个n阶行列式，如果其中第i行所有元素除$a_{ij}$外都为0，那么行列式等于$a_{ij}A_{ij}$。所以结合性质4，可以推导得到定理2，具体过程不再叙述。 可以将引理描述的情况，看作是定理2描述的特殊情况。因为引理描述的是某行/列只有一个元素不为0。 即 $D = a_{i1}A_{i1} + a_{i2}A_{i2} + … + a_{in}A_{in}$，或$D = a_{1j}A_{1j} + a_{2j}A_{2j} + … + a_{nj}A_{nj}$ 所以定理2可用来做降阶，选那些0的个数多的行或列，方便做行列式展开。然而，一般会运用行列式的性质，对行列式进行处理，希望能够满足引理描述的情况，即某行或列只有一个元素不为0，这样运算更方便。 推论：行列式某一行/列，与另一行/列的对应元素的代数余子式乘积之和等于0。 5. 小结这一章介绍的知识并不难，但是为学习矩阵运算奠定了基础。","link":"/2021/03/29/linear-algebra-ch1/"},{"title":"线性代数：第2章 矩阵及其运算","text":"第2章介绍了矩阵的基本知识和运算法则。引入了矩阵的转置、伴随矩阵、逆矩阵和矩阵的n次多项式，和矩阵分块法。 1. 线性方程组和矩阵 矩阵：可以看作映射。比如Ax = y，A是m × n维矩阵，x是n维列向量，则y是m × 1维列向量，A的作用就是将n维列向量，映射为m维列向量。 对角阵：从左上角到右下角的直线以外的元素都是0，记作$diag(\\lambda_{1},\\lambda_{1},…,\\lambda_{n})$。对角线上的元素全为1的对角阵被称为单位矩阵。 方阵：行数 = 列数的矩阵。对角阵都是方阵。n阶矩阵，就是n阶方阵的意思。 2. 矩阵的运算2.1 矩阵与矩阵相乘 矩阵满足结合律和分配律，但不满足交换律。可交换：对于两个n阶方阵，若AB=BA，则称方阵A与B是可交换的。只有方阵才可能达成可交换的条件。 $EA = AE = A$，其中，E是单位矩阵。 矩阵的幂：k是矩阵的幂，则$A^k$表示k个A相乘。显然只有方阵的幂才有意义。 2.2 矩阵的转置 定义：将矩阵A的行换成同序数的列，得到新矩阵，即为A的转置矩阵，记为$A^T$。 运算规律（假设运算都是可行的）： $(A^T)^T = A$ $(A + B)^T = A^T + B^T$ $(\\lambda A)^T = \\lambda A^T$ $(AB)^T = B^TA^T$（由定义证明） 对称矩阵：元素以对角线为对称轴，对应相等，即$A = A^T$ 例题：设列矩阵$X = (x_1, x_2, … , x_n)^T$满足$X^TX = 1$，$E$是n阶单位矩阵，$H = E - 2XX^T$，证明$H$是对称矩阵，且$HH^T = E$。 此题涉及矩阵乘法和矩阵转置知识点，比较典型。 证： $H^T = (E - 2XX^T)^T = E^T - (2XX^T)^T = E - 2XX^T = H$，所以$H$是对称矩阵。 因为： $\\begin{aligned} HH^T &amp;= H^2 \\\\&amp;= (E - 2XX^T)^2 \\\\&amp;= E - 4XX^T + 4(XX^T)(XX^T) \\\\&amp;= E - 4XX^T + 4X(X^TX)X^T\\\\ &amp;= E-4XX^T+4XX^T\\\\&amp;=E \\end{aligned}$ 得证。 2.3 方阵的行列式 定义：由n阶方阵A的元素所构成的行列式，称为方阵A的行列式，记作det A 或 |A|。 运算规律（设$A, B$均为n阶方阵）： $|A^T| = |A|$ $|\\lambda A| = \\lambda^n |A|$ $|AB| = |A||B| = |BA|$ 伴随矩阵：行列式$|A|$的各个元素的代数余子式$A_{ij}$所构成的矩阵。记作$A^*$，注意看清元素位置： A^* = \\left( \\begin{matrix} A_{11} & A_{21} & ... & A_{n1} \\\\ A_{12} & A_{22} & ... & A_{n2} \\\\ \\vdots & \\vdots & \\vdots & \\vdots \\\\ A_{1n} & A_{2n} & ... & A_{nn} \\end{matrix} \\right) 伴随矩阵的性质（利用代数余子式的性质证明）： AA^* = A^*A = |A|E 3. 逆矩阵3.1 定义、性质、求法 定义：对于n阶方阵$A$，若有一个n阶方阵$B$，使得$AB = BA = E$，则说矩阵$A$是可逆的，$B$为$A$的逆矩阵，记作$A^{-1}$。A的逆矩阵是唯一的。 这一小节里，默认$A$是方阵。 定理1：若矩阵$A$可逆，则$|A| \\neq 0$（由方阵的行列式的运算规律3可证） 定理2：若$|A| \\neq 0$，则矩阵$A$可逆，且$A^{-1} = \\frac{1}{|A|}A^*$（由伴随矩阵的性质可证）这是求逆矩阵的方法之一。 推论：若$AB = E$（或$BA = E$），则$B = A^{-1}$ 当$|A| = 0$时，$A$被称为奇异矩阵，否则为非奇异矩阵。由上两个定理可知，$A$是可逆矩阵的充分必要条件是：$|A| \\neq 0$，是非奇异矩阵。 运算规律（假设运算都是可行的）： $(A^{-1})^{-1} = A$ $\\lambda \\neq 0, (\\lambda A)^{-1} = \\frac{1}{\\lambda}A^{-1}$ $(AB)^{-1} = B^{-1}A^{-1}$（由结合律和定理2的推论可证） $(A^T)^{-1} = (A^{-1})^T$ 3.2 方阵$A$的m次多项式 定义：设$\\phi(x) = a_0 + a_1x+ … + a_mx^m$，记$\\phi(A) = a_0E + a_1A + … + a_mA^m$，$\\phi(A)$称为方阵$A$的m次多项式。 运算规律： 如果$A=P\\Lambda P^{-1}$，则$A^k = P\\Lambda^k P^{-1}$，从而： \\begin{aligned} \\phi(A) &= a_0E + a_1A + ... + a_mA^m \\\\ &= Pa_0EP^{-1}+Pa_1\\Lambda P^{-1} + ...+Pa_m\\Lambda^mP^{-1} \\\\ &= P\\phi(\\Lambda)P^{-1} \\end{aligned} 如果$\\Lambda=diag(\\lambda_1, \\lambda_2,…,\\lambda_n)$为对角矩阵，则$\\Lambda^k = diag(\\lambda_1^k, \\lambda_2^k,…,\\lambda_n^k)$，从而： \\begin{aligned} \\phi(\\Lambda) &= a_0\\left( \\begin{matrix} 1 & & & \\\\ & 1 & & \\\\ & & \\ddots & \\\\ & & & 1 \\end{matrix} \\right) +a_1\\left( \\begin{matrix} \\lambda_1 & & & \\\\ & \\lambda_2 & & \\\\ & & \\ddots & \\\\ & & & \\lambda_n \\end{matrix} \\right) + ... +a_m\\left( \\begin{matrix} \\lambda_1^m & & & \\\\ & \\lambda_2^m & & \\\\ & & \\ddots & \\\\ & & & \\lambda_n^m \\end{matrix} \\right) \\\\ &= \\left(\\begin{matrix} \\phi(\\lambda_1) & & & \\\\ & \\phi(\\lambda_2) & & \\\\ & & \\ddots & \\\\ & & & \\phi(\\lambda_n) \\end{matrix}\\right) \\end{aligned} 由此可知，若$\\Lambda$是对角矩阵，且$A=P\\Lambda P^{-1}$，并给出了$\\phi(A)$的表达式。就能很容易求出$\\phi(\\Lambda)$，继而求出$\\phi(A)$的值了。 4. 克拉默法则 用于求解n个n元线性方程组。 定义：如果线性方程组的系数矩阵$A$的行列式不等于0，则方程组有唯一解$x_1 = \\frac{|A_1|}{|A|}, x_2 = \\frac{|A_2|}{|A|}, …, x_n = \\frac{|A_n|}{|A|}$，其中$A_j$是把系数矩阵$A$中第j列的元素用方程组右端的常数项代替后所得到的n阶矩阵。 5. 矩阵分块法 定义：将矩阵$A$分为若干个子矩阵，则$A$是分块矩阵，比如分成4块，可记作$A = \\left(\\begin{matrix}A_{11} &amp; A_{12} \\\\A_{21} &amp; A_{22}\\end{matrix}\\right)$。 运算规律： 设矩阵$A$与矩阵$B$的行数相同、列数相同，采用相同的分块法。则这两个分块矩阵可以对应位置相加。 设$A$为m × l矩阵，$B$为l × n矩阵，分块成$A = \\left(\\begin{matrix}A_{11} &amp; \\cdots &amp;A_{1t} \\\\\\vdots &amp; &amp; \\vdots \\\\A_{s1} &amp; \\cdots &amp;A_{st}\\end{matrix}\\right)$，$B = \\left(\\begin{matrix}B_{11} &amp; \\cdots &amp;B_{1r} \\\\\\vdots &amp; &amp; \\vdots \\\\B_{t1} &amp; \\cdots &amp;B_{tr}\\end{matrix}\\right)$，其中$A_{i1}, A_{i2}, … ,A_{it}$的列数，分别等于$B_{1j}, B_{2j}, … ,B_{tj}$的行数，那么这两个分块矩阵可以进行矩阵乘法。 分块矩阵的转置，就是令每个分块分别转置：$\\left(\\begin{matrix}A \\\\ B\\end{matrix}\\right)^T$ =$(A^T,B^T)$ $A$是方阵，且$A = \\left(\\begin{matrix}A_1&amp; &amp; &amp; O \\\\ &amp; A_2 &amp; &amp; \\\\ &amp; &amp; \\ddots &amp; \\\\ O&amp; &amp; &amp;A_s\\end{matrix}\\right)$，其中在对角线上的每个分块都是方阵，则$A$是分块对角矩阵。 当矩阵进行分块后，可以变成分块对角矩阵，且行列式的值不为0，那么可以直接写出逆矩阵：A^{-1} = \\left(\\begin{matrix} A_1^{-1}& & & O \\\\ & A_2^{-1} & & \\\\ & & \\ddots & \\\\ O& & &A_s^{-1} \\end{matrix}\\right) 有两种重要的分块方法，按列分块：$A=(a_1,a_2,…,a_n)$；按行分块：$A = \\left(\\begin{matrix}\\alpha_1^T \\\\ \\alpha_2^T \\\\ \\vdots \\\\ \\alpha_m^T\\end{matrix}\\right)$，其中$\\alpha_i^T=(a_{i1}, a_{i2},…,a_{in})$ 矩阵$A=O$的充分必要条件是方阵$A^TA=O$。 6. 小结总结的所有运算规律很重要，是必须掌握的。","link":"/2021/03/30/linear-algebra-ch2/"},{"title":"线性代数：逆矩阵","text":"由于逆矩阵的应用广泛，所以专门写一篇文章，来记录逆矩阵的运算规律和性质。 1. 逆矩阵的判断方阵$A$是可逆矩阵的充分必要条件有（满足一个即可）： $|A| \\neq 0$ 存在有限个初等矩阵$P_1,P_2,…,P_l$，使$A=P_1P_2…P_l$。 $A$和$E$行等价。 $A$的所有特征值全不为零。 2. 逆矩阵的求法2.1 利用伴随矩阵$A^{-1} = \\frac{1}{|A|}A^{*}$（由伴随矩阵的性质可证） 2.2 利用增广矩阵和初等行变换设有增广矩阵$(A, E)$，进行初等行变化，得到$(PA, PE)$，也就是$(PA,P)$。因为，方阵$A$可逆的充要条件是$A$与$E$行等价。所以$PA=E$，那么$P$就是$A^{-1}$。 这就意味着，对于矩阵$(A, E)$，当$A$经过初等行变换变为了$E$时，$E$就变成了$A^{-1}$，即为所求。","link":"/2021/03/31/inverse-matrix/"},{"title":"线性代数：第3章 矩阵的初等变换与线性方程组","text":"本章介绍了矩阵的初等变换，建立矩阵的秩的概念，并讨论了线性方程组无解、有唯一解、无限多解的充要条件，并介绍用初等变换解线性方程组的方法。 1. 矩阵的初等变换 平常在解方程组$AX=b$ 时，只是对方程组的系数$A$和常数$b$进行运算，比如消元运算。未知数并未参与运算。所以可以将系数和常数放在同一个矩阵里，即为增广矩阵，记作$B=(A,b)$。解方程组的过程，可以转换为对矩阵$B$进行变换。 矩阵的三种初等行变换： 对换两行。 以数$k\\neq 0$乘某一行的所有元。 把某一行所有元的k倍，加到另一行的对应元上。 注：把行换成列就是初等列变换。矩阵的行列式的值虽然经过变换，可能会变，但是并没有什么影响，因为目的不在于此。 矩阵的等价： 如果矩阵$A$经过有限次初等行变换变成矩阵$B$，则$A$与$B$行等价，记作$A \\sim^{r} B$。 如果列等价，记作$A \\sim^{c} B$。 如果经过有限次的初等变换，则等价，记作$A \\sim B$。 等价关系具有反身性、对称性、传递性。 对于矩阵A： 满足1. 非零行在零行的上面。2. 非零行的首非零元所在列在上一行（如果存在）的首非零元所在列的右边。则$A$是行阶梯形矩阵。 进一步，如果还满足1. 非零行的首非零元为1。2. 首非零元所在的列的其他元为0。则$A$是行最简形矩阵。 要解线性方程组，只需把增广矩阵，化为行最简形矩阵（后面会介绍更简单的方法）。行最简形矩阵是唯一确定的。 对行最简形矩阵施以初等列变换，可得到标准形： F = \\left( \\begin{matrix} E_{r} & O \\\\ O & O \\end{matrix}\\right)_{m \\times n} r是行阶梯形矩阵中非零行的行数。 为了更好地阐明矩阵初等变换的性质，引入初等矩阵的概念：由单位矩阵$E$，经过一次初等变换得到的矩阵。三种初等变换对应有三种初等矩阵： 单位矩阵中的两行/列对换。 以数$k\\neq 0$乘单位矩阵某一行/列的所有元。 把单位矩阵某一行/列所有元的k倍，加到另一行/列的对应元上。 可得初等矩阵的两个性质： 设$A$是一个m × n矩阵，对$A$施行一次初等行变换，相当于在$A$的左边乘相应的m阶初等矩阵，即$PA$；施行一次初等列变换，相当于在$A$的右边乘相应的n阶初等矩阵,即$AQ$。 初等矩阵都是可逆的，它们的乘积也可逆。 方阵$A$可逆的充要条件是，存在有限个初等矩阵$P_1,P_2,…,P_l$，使$A=P_1P_2…P_l$。 矩阵初等变换的性质： 设$A$与$B$为m × n矩阵，那么 $A \\sim^{r} B$的充要条件是存在m阶可逆矩阵$P$，使$PA=B$。 $A \\sim^{r} B$ &lt;=&gt; A经过有限次初等变换变为$B$ &lt;=&gt; 存在有限个m阶初等矩阵$P_1,P_2,…,P_l$，使$P_1P_2…P_lA=B$（由初等矩阵的性质1） &lt;=&gt; 存在m阶可逆矩阵$P$，使$PA=B$。 推论：方阵$A$可逆的充要条件是，$A \\sim^{r} E$。因为$A$可逆，就存在一个可逆矩阵$P$，使得$PA=E$，这恰恰也是$A \\sim^{r} E$的充要条件。 $A \\sim^{c} B$的充要条件是存在n阶可逆矩阵$Q$，使$AQ=B$。 $A \\sim B$的充要条件是存在m阶可逆矩阵$P$和n阶可逆矩阵$Q$，使$PAQ=B$。 至此，得到了求解逆矩阵和有唯一解的线性方程组的重要方法（利用增广矩阵）： 求方阵$A$的逆矩阵： 设有增广矩阵$(A, E)$，进行初等行变化，得到$(PA, PE)$，也就是$(PA,P)$。因为由推论可知，方阵$A$可逆的充要条件是$A \\sim^{r} E$。所以$PA=E$，那么$P$就是$A^{-1}$。 这就意味着，对于矩阵$(A, E)$，当$A$经过初等行变换变为了$E$时，$E$就变成了$A^{-1}$，即为所求。 求方程个数和未知数个数相同的线性方程组（只有这样$A$才是方阵）： 解方程组$AX=b$ ，有增广矩阵$(A,b)$，经过初等行变化变成$(PA,Pb)$。 当$PA=E$时，$Pb=A^{-1}b$，也就是$X$，得解。 总结一下，就是要建立关于方阵$A$的增广矩阵，将$A$化为单位矩阵$E$时，增广矩阵右半边即为所求。 2. 矩阵的秩 矩阵$A$的k阶子式：在$A$中任取k行与k列，位于这些行列交界处的$k^2$个元素，不改变它们在$A$中所处的位置次序而得到的$k$阶行列式。 矩阵的秩：设在$A$中有一个不等于0的r阶子式$D$，而它的r + 1阶子式（若存在）均为0，则矩阵的秩为r，记作$R(A)$。$D$是矩阵$A$的最高阶非零子式。行阶梯形矩阵的秩，就是非零行的行数。 矩阵的秩的性质： $0 \\leq R(A_{m\\times n})\\leq min(m, n)$ $R(A^T) = R(A)$（因为行列式相等） 若$A\\sim B$，则$R(A) = R(B)$ 若$P, Q$可逆，则$R(PAQ) = R(A)$（其实也就是等价的意思） 可逆矩阵的秩等于矩阵的阶数，因为可逆矩阵的行列式一定不为0，所以可逆矩阵又称满秩矩阵。不可逆矩阵的秩小于矩阵的阶数，所以又称为降秩矩阵。 $max(R(A),R(B)) \\leq R(A,B) \\leq R(A) + R(B)$，这里的$(A,B)$是增广矩阵。特别的，当$B$是非零列向量时，有$R(A) \\leq R(A,b) \\leq R(A) + 1$。 $R(A+B)\\leq R(A) + R(B)$ $R(AB)\\leq min(R(A),R(B))$ 若$A_{m\\times n}B_{n \\times l}=O$，则$R(A)+R(B)\\leq n$ 若$A_{m\\times n}B_{n \\times l}=C$，且$R(A) = n$，则$R(B)=R(C)$ 矩阵$A$的秩等于它的列数，所以被称为列满秩矩阵。 当$C=O$，则$R(B)=0$，则$B=O$，这一结论被称为矩阵乘法的消去律。 3. 线性方程组的解 n元线性方程组$Ax=b$ 无解的充要条件是$R(A) &lt; R(A,b)$ 有唯一解的充要条件是$R(A) = R(A,b) = n$（这说明了$A$是可逆矩阵） 有无限多解的充要条件是$R(A)=R(A,b) &lt; n$（未知数的个数大于有效方程数） 特殊情况：n元齐次线性方程组$Ax=0$有非零解的充要条件是$R(A)&lt; n$。不是$R(A)= n$的原因：若$R(A)= n$，则$x=A^{-1}b$只有零解($b=0$)。 例：求解非齐次线性方程组 \\begin{cases} x_1+x_2-3x_3-x_4&=1\\\\ 3x_1-x_2-3x_3+4x_4&=4\\\\ x_1+5x_2-9x_3-8x_4&=0 \\end{cases} 解：对增广矩阵$B$施行初等行变换，得到行最简形矩阵，其中r个非零行的首非零元对应的未知数取作非自由未知数，其余n-r个未知数取作自由未知数，设为$c_1,c_2,\\cdots,c_{n-r}$： B=\\left( \\begin{matrix} 1&1&-3&-1&1\\\\ 3&-1&-3&4&4\\\\ 1&5&-9&-8&0 \\end{matrix} \\right)\\sim\\left( \\begin{matrix} 1&0&-\\frac{3}{2}&\\frac{3}{4}&\\frac{5}{4}\\\\ 0&1&-\\frac{3}{2}&-\\frac{7}{4}&-\\frac{1}{4}\\\\ 0&0&0&0&0 \\end{matrix} \\right) 即得 \\begin{cases} x_1&=\\frac{3}{2}c_1-\\frac{3}{4}c_2+\\frac{5}{4}\\\\ x_2&=\\frac{3}{2}c_1+\\frac{7}{4}c_2-\\frac{1}{4}\\\\ x_3&=c_1\\\\ x_4&=c_2 \\end{cases} 也可表示成向量形式。 矩阵方程$AX=B$有解的充要条件是$R(A)=R(A,B)$。 设$AB=C$，则$R(C)\\leq min(R(A), R(B))$","link":"/2021/03/31/linear-algebra-ch3/"},{"title":"论文阅读笔记：不规则采样时间序列的预测任务（2）","text":"这周任务是调研有关不规则采样时间序列预测的论文，并进行总结。共调研了两篇论文： DATA-GRU: Dual-Attention Time-Aware Gated Recurrent Unit for Irregular Multivariate Time Series (AAAI, 2020) Deep State-Space Generative Model For Correlated Time-to-Event Predictions (KDD, 2020) 1. DATA-GRU: Dual-Attention Time-Aware Gated Recurrent Unit for Irregular Multivariate Time Series1.1 摘要处理不规则采样时间序列的传统方法，通常是使其时间间隔变得规则。但是时间序列之所以不规则，有两个原因：1、患者进行检查的次数不确定。2、患者状况变化，需要检查的生理指标不同。因此，不规则的时间间隔和缺失值，往往蕴含着许多重要信息。 本篇文章建立了DATA-GRU模型，引入time-aware（时间感知）机制和dual-attention（双注意）机制，发现并保存不规则时间间隔蕴含的信息，在处理数据时，还考虑了数据的重要性和医疗信息。 1.2 模型 高斯过程：处理缺失值 使用高斯过程，学习观测值的分布；计算条件概率，以填补某时刻的缺失值。 利用在高斯过程中得到的协方差矩阵，计算填补值的不可靠分数，填补的质量越好，不可靠分数越低，真实观测值的不可靠分数为0。这部分的计算结果用于dual-attention机制。 time-aware机制：处理不规则的时间间隔蕴含的信息 DATA-GRU模型，内核其实是标准GRU。 总体结构图(a)的紫色部分表示time-aware机制，在将上一时刻的隐层状态输入到标准GRU之前，还需对其进行处理： 利用decay（衰退）函数 $w_{\\Delta t}=1/log(e+\\Delta t)$，处理上一时刻的隐层状态$h^{t-1}$，得到$h_{t-1}^{d} = h_{t-1}\\odot w_{\\Delta t}$，再将$h_{t-1}^{d}$输入到标准GRU。 decay函数，是为了确保先前状态的影响随着时间间隔的增加而减弱。这里很好理解，因为患者的生理指标值和最近的检测结果的关系更大。 unreliability-aware attention机制：考虑数据的重要性 总体结构图(a)的蓝色部分表示unreliability-aware attention机制。为了方便，将不可靠分数$u_{t}$，转变成$c_{t}$。 由于$c_{t}$只能反映数据质量，不能反映重要性，所以利用sigmoid函数，计算$\\alpha_{t}^u$。该参数用来反映数据的重要性。 将$\\alpha_{t}^u$和序列值$x_{t}$相乘，得到的$x_{t}^u$就是时间序列中比较重要的数据信息。 \\begin{aligned} c_{t} &= 1-u_{t} \\\\ \\alpha_{t}^u &=sigmoid(W^uc_{t}+b^u) \\\\ x_{t}^u&=x_{t} \\odot \\alpha_{t}^u \\end{aligned} symptom-aware attention机制：考虑缺失值蕴含的医疗信息 总体结构图(a)的绿色部分表示symptom-aware attention机制，是对unreliability-aware attention机制的补充。因为unreliability-aware attention机制处理的是已进行填补的序列，没有考虑填补缺失数据可能会破坏一些医疗信息。缺失数据可能反映患者的状况发生改变，有些生理指标不用再进行检查。 symptom-aware attention机制，直接对真实数据进行处理。先使用过滤器，对可靠性分数$c_{t}$进行过滤，使真实数据的可靠性分数不变，也就是1；填补数据的可靠性分数变为0，得到$c_{t0/1}^s$。 与已填补缺失数据的序列相乘，得到只包含真实数据的序列$x_{t0/true}^s$。 将在time-aware中得到的$w_{\\Delta t}$，分别和$c_{t0/1}^s$与$x_{t0/true}^s$相乘，得到$\\alpha_{tdeep}^s$和$x_{tdeep}^s$。个人理解，这里考虑了时间间隔的影响，保证了数据质量。 最后将两者相乘。也就是在保证数据质量的基础上，获得比较重要的数据。 \\begin{aligned} c_{t0/1}^s &= F_{ARPF}(c_t) = \\lfloor c_t-0.5 \\rfloor\\\\ x_{t0/true}^s &= x_t \\odot c_{t0/1}^s\\\\ x_{tdeep}^s &= TGRU(x_{t0/true}^s,w_{\\Delta t})\\\\ \\alpha_{tdeep}^s &= TGRU(c_{t0/1}^s,w_{\\Delta t})\\\\ x_{t}^s &= x_{tdeep}^s \\odot \\alpha_{tdeep}^s \\\\ \\end{aligned} Interpretable embedding 为了保证特征的可解释性，将双注意力机制得到的结果用ReLU函数结合起来：$x_t^{adjust}=ReLU(W_emb[x_t^u; x_t^s] + b^{emb})$ 和time-aware中计算得到的$h_{t-1}^{d}$，一起送入标准GRU。$x_t^{adjust}$表示的是当前时刻的信息，$h_{t-1}^{d}$表示的是上一时刻的。 目标函数 使用softmax层进行预测，用交叉熵损失函数计算损失。 1.3 思考 创新点： time-aware机制中decay函数的使用，较好地考虑了时间间隔对数据的影响。 使用symptom-aware attention机制作为unreliability-aware attention机制的补充，考虑了填补数据可能会破坏医疗信息。这一点常常会被忽视。 疑惑： 个人觉得本文使用协方差来计算不可靠分数，不够合理。因为协方差大有两个可能：1、两个变量各自的方差不变的情况下，两个变量的正相关性越强烈，协方差越大。2、两个变量的相关性不变的情况下，任何一个变量的方差越大，协方差的绝对值越大。所以协方差大不一定表示该填补值不可靠，有可能是和真实值的相关性较强。 symptom-aware attention机制中，计算$c_{t0/1}^s$的公式不太合理，比如真实值的$c_{t0/1}^s$应该为1，但是按公式计算，就是0。 2. Deep State-Space Generative Model For Correlated Time-to-Event Predictions2.1 摘要捕获多种临床事件之间的相互依赖性不仅对准确的未来事件预测至关重要，而且对制定更好的治疗计划也至关重要。 本文提出了deep state-space generative model，通过明确地模拟患者状态的时间动态，来捕获临床干预和观测之间的相互作用。 进一步开发了一种新的hazard rate function(危险率函数)，来预测临床事件发生的概率，捕获多种临床事件之间的相互依赖性。 2.2 模型与患者状况有关的时间序列，包括生理指标观测值和临床干预的等级。临床干预包括给药的剂量、呼吸机的设置等，和临床事件（死亡等）区分开。 处理缺失值 对于生理指标观测值缺失，本文使用已观测值中，出现最多的值填补。重点放在了对临床干预等级缺失的填补。 对于临床干预，缺失数据的产生有两个原因：1、在该时间点无操作。2、可能延续了上一个时间点的干预。 观察到大多数干预是定期设置的。所以，推导出干预设置时间的分布，选取90%的时间作为截止时间阈值。如果两次连续的干预都在相应阈值的时间范围内，则数据缺失的原因为原因2，并使用最后一个设置来填补缺失值。否则视为无操作。 State Space Model 为了捕获观测和临床干预之间的关系，本文利用高斯状态空间模型，显式地为患者的隐层状态建模。根据上图，个人理解是，当前时刻的临床干预$u_{t}$，会影响当前时刻的隐层状态$z_t$；$z_t$会影响该时刻患者生理指标的值$x_t$、临床事件发生的危险率$\\lambda_{t}^e$，和下一时刻的干预$u_{t+1}$。 引入transition和emission概率计算公式: \\begin{aligned} &p(z_t|z_{t-1},u_t)\\sim N(A_t(z_{t-1})+B_t(u_t),Q), &Transition\\\\ &p(x_t|z_t)\\sim N(C(z_t),R), &Emission \\end{aligned}其中，函数$A$ 不考虑临床干预的影响，函数$B$ 则考虑。$C$ 用来捕获隐层状态和观测值之间的关系。 计算临床干预行为的概率分布： p(u_t|z_{t-1})\\sim N(D(z_{t-1}),U) $A,B,C,D$，都是可学习的参数。 State-based Discrete-time Hazard Rate 本文的预测任务：预测临床事件发生的危险率。 这里详细介绍如何计算危险率$\\lambda_{t}^e$。为方便描述，State Space Model简化成上图，只包含$z_t$和$\\lambda_{t}^e$。 $\\lambda_{t}^e$和隐层状态有关： \\lambda_{t}^e = L^e(z_{t}) $L^e$也是可学习的参数，对于不同event，该值是不同的。所以可以看到，上图有$\\lambda_{t}^e$和$\\lambda_{t}^{e’}$之分。 由危险率，可以计算患者生存率公式（这应该是用在后文计算损失函数）： S^e(t)=(1-\\lambda_{t}^e)S^e(t-1) 设$S^e(0)=1$，则上式写成： S^e(t)=\\prod_{s=1}^t(1-\\lambda_{s}^e) 个人理解，生存率公式表示在t时刻，临床事件尚未发生的概率。 同理，计算incidence density function： f(t^e)=\\lambda_{t}^e\\prod_{s=1}^{t-1}(1-\\lambda_{s}^e) 个人理解，该公式表示在t时刻，临床事件发生的概率。 损失函数 利用变分下界，推导出了损失函数： \\begin{aligned} &(1-c)\\cdot E_{q_\\theta(\\hat{z}|\\overline{x},\\overline{u})}[ logf_{\\theta}^e(t^e|\\hat{z}) ] + c\\cdot E_{q_\\theta(\\hat{z}|\\overline{x},\\overline{u})}[ logS_{\\theta}^e(t^e|\\hat{z}) ] \\\\ &-KL(q_\\theta(\\hat{z}|\\overline{x},\\overline{u})||p_\\theta(\\hat{z}|\\overline{x},\\overline{u})) \\end{aligned} 其中，$\\theta$是之前提到的可学习的参数。c为1时，表示在$t^e$时刻临床事件$e$已发生；c为0时，表示未发生。$\\hat{z}$表示1~$t^e$的隐层状态。 该函数是$t^e$时刻下、单个临床事件的损失函数。 总体结构如下： 2.3 思考 创新点： 计算每个临床事件在某时刻的危险率时，都是以患者的隐层状态为基础，个人理解这里是捕获多种临床事件之间的相互依赖性的体现，因为用了相同的隐层状态。但是用来计算的具体参数值因事件而异，即$L^e$不同，保证了事件的独特性。 处理临床干预缺失值的方式，是通过观察数据的分布，从而制定的新方案。 3. 小结这周精读了两篇文章，一开始，文章的关键模型我看了许多遍都未完全看懂。但后来自己试着写写模型公式，慢慢地去推敲参数的含义，继而逐步理解了模型，或许这是一种学习方法吧。","link":"/2021/03/29/survey-prediction%EF%BC%882%EF%BC%89/"},{"title":"线性代数：第4章 向量组的线性相关性","text":"本章将上一章的$Ax=b$，拓展到了$AX=B$。介绍了线性方程组的解的结构和向量空间。 1. 向量组及其线性组合 向量组：若干个同维度的列向量（或同维度的行向量）所组成的集合。m个n维列向量所组成的向量组A，构成一个n × m矩阵： A=(a_1,a_2,\\cdots,a_m)m个n维行向量所组成的向量组B，构成一个m × n矩阵：B=\\left( \\begin{matrix} \\beta_1^T \\\\ \\beta_2^T \\\\ \\vdots\\\\ \\beta_m^T \\end{matrix} \\right) 线性表示： 给定向量组$A:a_1,a_2,\\cdots,a_m$和向量$b$，若存在一组数$\\lambda_1,\\lambda_2,\\cdots,\\lambda_m$，使$b=\\lambda_1a_1+\\lambda_2a_2+\\cdots+\\lambda_ma_m$，则向量$b$能由向量组$A$线性表示，充要条件是$R(A)=R(A,b)$，即为上一章讲的线性方程组有解的条件。 若向量组$B$中的每个向量都能由向量组$A$线性表示，则向量组$B$能由向量组$A$线性表示，充要条件是$R(A)=R(A,B)$。且有推论$R(B)\\leq R(A,B) = R(A)$。 若向量组$A$与向量组$B$能相互线性表示，则称这两个向量组等价，充要条件是$R(A)=R(B)=R(A,B)$。 几个等价说法：向量组$B$能由向量组$A$线性表示 &lt;=&gt; 有矩阵$K$，使$B=AK$。&lt;=&gt; 方程$AX=B$有解。 n维单位坐标向量$E$能由向量组$A$线性表示的充要条件是$R(A)=n$。 2. 向量组的线性相关性 给定向量组$A:a_1,a_2,\\cdots,a_m$，如果存在不全为零的数$k_1,k_2,\\cdots,k_m$，使$k_1a_1+k_2a_2+\\cdots+k_ma_m=0$，则称向量组$A$是线性相关的，否则线性无关。 向量组$A$线性相关的充要条件是：向量组$A$中有某个向量能由其余m - 1个向量线性表示。 当方程组中有某个方程是其余方程的线性组合，该方程就是多余方程，该方程组就是线性相关的。 定理1：向量组$A:a_1,a_2,\\cdots,a_m$线性相关的充要条件是它所构成的矩阵$A=(a_1,a_2,\\cdots,a_m)$的秩小于向量个数m；向量组$A$线性无关的充要条件是$R(A)=m$。 证明：向量组$A$线性相关，也就是线性方程组$Ax=0$有非零解，在上一章线性方程组有无限多解的特殊情况中已经阐述过了。 定理2： 若向量组$A:a_1,a_2,\\cdots,a_m$线性相关，则向量组$B:a_1,a_2,\\cdots,a_m,a_{m+1}$也线性相关。反之，若向量组$B$线性无关，则向量组$A$也线性无关。 证：$R(B)\\leq R(A)+1$，这是因为$B$对于$A$，只是增加了一个列向量，$B$对应的矩阵化为行阶梯形矩阵，非零行的数目对比$A$的行阶梯形矩阵，最多多出1行。 $A$线性相关=&gt;$R(A)&lt;m$，所以$R(B)&lt;m+1$，也线性相关。 推广：一个向量组若有线性相关的部分组，则该向量组线性相关。特别地，含零向量的向量组必线性相关（因为如果向量组中，有1个零向量，那么只要这个零向量的系数不为0，其他向量的系数都为0，那么这就是一组不全为0的系数，必然线性相关）。一个向量组若线性无关，则它的任何部分组都线性无关。 m个n维向量组成的向量组，当维数n小于向量个数m时，一定线性相关（直观地理解就是未知数个数小于方程个数） 设向量组$A:a_1,a_2,\\cdots,a_m$ 线性无关，而向量组$B:a_1,a_2,\\cdots,a_m,b$ 线性相关，则向量$b$必能由向量组$A$线性表示，且表达式是唯一的（也就是有唯一解）。 证：$R(A)\\leq R(B)$。$A$线性无关=&gt;$R(A)=m$。$B$线性相关=&gt;$R(B) &lt; m+1$。因为$m\\leq R(B) &lt; m+1$，所以$R(B)=R(A)=m$。这就是方程组$Ax=b$ 有唯一解的条件。 3. 向量组的秩 之前的讨论中，向量组的向量个数是有限的，现在去掉这一限制，即向量组可以包含无限多个向量。 设有向量组$A$，如果在$A$中能选出向量组$A_0:a_1,a_2,\\cdots,a_r$，满足$A_0$线性无关，且向量组$A$中任意$r+1$个向量（如果有的话）都线性相关，则称$A_0$是$A$的一个最大线性无关向量组（最大无关组）。且$A$的秩$R_A=r$。 只含零向量的向量组没有最大无关组，规定它的秩为0。 最大无关组的等价定义：设有向量组$A$，如果在$A$中能选出向量组$A_0:a_1,a_2,\\cdots,a_r$，满足$A_0$线性无关，且$A$的任一向量都能由$A_0$线性表示。则称$A_0$是$A$的一个最大无关组。 矩阵的秩，等于它的列向量组的秩，也等于它的行向量组的秩。 所以，若$D_r$是矩阵$A$的一个最高阶非零子式，则$D_r$所在的$r$列即是$A$的列向量组的一个最大无关组，$D_r$所在的$r$行即是$A$的行向量组的一个最大无关组。（因为r+1列或r+1行是线性相关的） 4. 线性方程组的解的结构 假设有n元齐次线性方程组$Ax=0$，设该方程组全体解的集合记作$S$，如果能求得解集$S$的一个最大无关组$S_0: \\xi_1, \\xi_2, \\cdots, \\xi_t$，则方程的任一解都能由$S_0$线性表示（最大无关组的等价定义）。因此，齐次方程组$Ax=0$的通解是$x=k_1\\xi_1+k_2\\xi_2+\\cdots+k_t\\xi_t$，$S_0$被称为基础解系。 找基础解系时，将系数矩阵化成行最简形矩阵，那么每行首非零元所在的列对应的列向量线性无关。剩下的n-r个列向量对应的x，就是自由未知数。用自由未知数解得非自由未知数，可以写出基础解系。后面的例子会讲到。 设m × n矩阵$A$的秩$R(A)=r$，则n元齐次线性方程组$Ax=0$的解集$S$的秩$R_s=n-r$。该定理是线性方程组各种解法的理论基础。 非齐次方程的通解=对应的齐次方程的通解+非齐次方程的一个特解。 例子：求解方程组 \\begin{cases} x_1-x_2-x_3+x_4&=0\\\\ x_1-x_2+x_3-3x_4&=1\\\\ x_1-x_2-2x_3+3x_4&=-\\frac{1}{2} \\end{cases} 解：对增广矩阵$B$进行初等行变换，化为行最简形矩阵： $B=\\left( \\begin{matrix} 1 &amp; -1 &amp; -1 &amp; 1 &amp; 0 \\\\ 1 &amp; -1 &amp; 1 &amp; -3 &amp; 1 \\\\ 1 &amp; -1 &amp; -2 &amp; 3 &amp; -\\frac{1}{2} \\end{matrix} \\right)\\sim\\left(\\begin{matrix} 1 &amp; -1 &amp; 0 &amp; -1 &amp; \\frac{1}{2} \\\\ 0 &amp; 0 &amp; 1 &amp; -2 &amp; \\frac{1}{2} \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\end{matrix}\\right)$ 可见$R(A)=R(B)=2$，该方程组有解。有n-r个自由向量，$x_2, x_4$为自由向量。 \\begin{cases} x_1&=x_2+x_4+\\frac{1}{2}\\\\ x_3&=2x_4 + \\frac{1}{2} \\end{cases} 该方程组对应的齐次方程组（注意，是$\\begin{cases} x_1&amp;=x_2+x_4\\\\ x_3&amp;=2x_4 \\end{cases}$）取： $\\left(\\begin{matrix} x_2\\\\x_4\\end{matrix}\\right)$ = $\\left(\\begin{matrix} 1\\\\0\\end{matrix}\\right)$, $\\left(\\begin{matrix} 0\\\\1\\end{matrix}\\right)$。则基础解系为$\\left(\\begin{matrix} 1\\\\1\\\\0\\\\0\\end{matrix}\\right)$和$\\left(\\begin{matrix} 1\\\\0\\\\2\\\\1\\end{matrix}\\right)$。通解为$\\left(\\begin{matrix} x_1\\\\x_2\\\\x_3\\\\x_4\\end{matrix}\\right)= c_1\\left(\\begin{matrix} 1\\\\1\\\\0\\\\0\\end{matrix}\\right) + c_2\\left(\\begin{matrix} 1\\\\0\\\\2\\\\1\\end{matrix}\\right)$。 非齐次方程组，取$x_2, x_4=0$，则$x_1=x_3=\\frac{1}{2}$。则该方程组的一个特解是： $\\left(\\begin{matrix} \\frac{1}{2}\\\\0\\\\\\frac{1}{2}\\\\0\\end{matrix}\\right)$ 综上，该非齐次方程组的通解是 $\\left(\\begin{matrix} x_1\\\\x_2\\\\x_3\\\\x_4\\end{matrix}\\right)= c_1\\left(\\begin{matrix} 1\\\\1\\\\0\\\\0\\end{matrix}\\right) + c_2\\left(\\begin{matrix} 1\\\\0\\\\2\\\\1\\end{matrix}\\right)+\\left(\\begin{matrix} \\frac{1}{2}\\\\0\\\\\\frac{1}{2}\\\\0\\end{matrix}\\right)(c_1, c_2\\in R)$。 5. 向量空间 向量空间的定义：设$V$为n维向量的集合，如果集合$V$非空，且集合$V$对于向量的加法及数乘两种运算封闭（也就是做了运算后向量仍属于$V$），则称$V$为向量空间。所以齐次线性方程组的解集是一个向量空间，称为解空间，非齐次线性方程组的解集不是向量空间。 设有向量空间$V_1, V_2$，若$V_1 \\subseteq V_2$，则称$V_1$是$V_2$的子空间。 设$V$为向量空间，如果r个向量$a_1,a_2,\\cdots,a_r\\in V$，满足$a_1,a_2,\\cdots,a_r$线性无关，且$V$中的任一向量都能由$a_1,a_2,\\cdots,a_r$线性表示，则称$a_1,a_2,\\cdots,a_r$是向量空间$V$的一个基，r称为$V$的维数，并称$V$为r维向量空间。 类比最大无关组的等价定义，把$V$看作向量组，$V$的基就是向量组的最大无关组，$V$的维数就是向量组的秩。 $V$记作$V=\\{x=\\lambda_1 a_1 + \\cdots + \\lambda_r a_r | \\lambda_1,\\cdots,\\lambda_r \\in R\\}$ 如果在向量空间$V$中取定一个基$a_1,a_2,\\cdots,a_r$，那么$V$中任一向量$x$可唯一地表示为$x=\\lambda_1a_1 + \\lambda_2a_2+\\cdots+\\lambda_ra_r$。称$\\lambda_1,\\lambda_2,\\cdots,\\lambda_r$为向量x在基$a_1,a_2,\\cdots,a_r$上的坐标。特别地，在n维向量空间R中取单位坐标向量组$e_1,e_2,\\cdots,e_n$叫做自然基。 在向量空间中，取定一个基$a_1,\\cdots,a_n$，令$A=(a_1,\\cdots,a_n)$，再取一个新基$b_1,\\cdots,b_n$，令$B=(b_1,\\cdots,b_n)$，则从旧基变换到新基，有基变换公式：$B=AP$，其中$P=A^{-1}B$称为旧基变换到新基的过渡矩阵。 假设向量$x$在旧基和新基中的坐标为$y_1,\\cdots,y_n$和$z_1,\\cdots.z_n$，即$x=A\\left(\\begin{matrix} y_1\\\\\\vdots\\\\y_n \\end{matrix}\\right)$和$x=B\\left(\\begin{matrix} z_1\\\\\\vdots\\\\z_n \\end{matrix}\\right)$，则从旧坐标变换到新坐标，有坐标变换公式：$\\left(\\begin{matrix}z_1\\\\\\vdots\\\\z_n \\end{matrix}\\right) =B^{-1}A\\left(\\begin{matrix} y_1\\\\\\vdots\\\\y_n \\end{matrix}\\right)= P^{-1}\\left(\\begin{matrix} y_1\\\\\\vdots\\\\y_n \\end{matrix}\\right)$。","link":"/2021/04/05/linear-algebra-ch4/"},{"title":"数据结构：链表、栈、队列","text":"本章总结链表、栈、队列的基础知识，以及具体题型。参考了老师上课的课件和在leetcode刷的相关题目。 1. 链表1.1 基础知识 链表的实现：基于数组(Array-based)或基于链接(Linked-based)。 时间复杂度对比： 操作 Array-based Linked-based 插入 O(n) O(1) 删除 O(n) O(1) 寻值 O(1) O(n) 注：Linked-based的插入和删除操作的时间复杂度，没有算入寻值的时间复杂度。 freelist：对于Linked-based实现的链表，每次插入或删除某值时，都要调用new或delete函数来增添或删除节点，造成时间浪费。 所以，要删除某值时，将其对应的节点作为空闲节点，放在一个freelist里，当要插入新元素时，取freelist的头节点，来存放新元素，可以重复利用，节约时间。 双向链表：对于Linked-based实现的链表，找前一个节点更方便。 1.2 题目默认是Linked-based的链表。 获取倒数第k个元素 剑指 Offer 22 链表中倒数第k个节点 可以用双指针的做法：设有两个指针，初始时均指向头结点。先让快指针移动 k 次，此时，快慢指针的距离为 k 。然后，同时移动快慢指针，直到快指针指向空，此时慢指针即指向倒数第 k 个结点。 获取中间元素 876 链表的中间结点 可以用双指针的做法，不过和上一题有所不同：慢指针每次移动一次，快指针每次移动两次，当快指针无法移动两次（即快指针指向末尾节点或指向空），则慢指针指向了链表的中间节点（链表长度为偶数时指向靠后的那一个）。 原因：快指针走过的路程是慢指针的两倍。 判断链表是否有环 141 环形链表 可以用双指针的做法：慢指针每次移动一次，快指针每次移动两次，当一个链表有环时，快慢指针都会陷入环中进行无限次移动，变成追及问题。快指针速度比慢指针快，总会追上慢指针，相遇时，两次相遇间的移动次数即为环的长度。 参考题解：https://leetcode-cn.com/problems/linked-list-cycle/solution/yi-wen-gao-ding-chang-jian-de-lian-biao-wen-ti-h-2/ 反转链表 206 反转链表 可以用迭代或递归的做法。 2. 栈2.1 基础知识栈具有后进先出的特性，常用在有配对关系的场景，比如括号匹配。函数的递归调用，实则是把每一次调用的变量等保存在了一个栈里。 2.2 题目 有配对关系的场景 1047. 删除字符串中的所有相邻重复项 844. 比较含退格的字符串 这道题也可以用双指针法做。 单调栈 496. 下一个更大元素 I，503. 下一个更大元素 II 单调栈一般用于寻找在当前元素之后，且比它大或小的元素。 用栈实现队列 剑指 Offer 09. 用两个栈实现队列 用两个栈，一个专门负责入列，一个专门负责出列。 用队列实现栈 225. 用队列实现栈 可以用一个队列，在插入新元素后，弹出原来所有元素，并将它们按顺序重新插入到队列，这样可以使最后插入的元素保持在队列最前面。也可以用两个队列实现以上效果。 3. 队列3.1 基础知识队列具有先进先出的特性，如果是基于数组的队列，一般采用环形数组实现。 为什么基于数组的栈不需要环形数组？因为栈是后进先出，不会影响整体的空间结构。队列，先进先出，所以要用两个变量记录队列的头和尾，当弹出队列头对应的数后，队列头后移。如果不用环形数组，前面的空间会被浪费。 插入新元素：(rear + 1) % size，如果不对size取模，那么数组会超界，要循环利用数组的空间。 数组元素个数：（rear + 1 + size - front）% size。因为当rear &gt;= front 时，数组元素个数为rear + 1 - front；当rear &lt; front时，数组元素个数为rear + size + 1 - front。两种情况合并，则为上式。 队列满或空时，条件都是front == (rear + 1) % size（可画个图看看），不能区分。所以，如果数组大小为n，那么就只存储n - 1个元素。这样，当front == (rear + 1) % size 时，队列是空的；front == (rear + 2) % size 时，队列是满的。","link":"/2021/04/11/ds-link/"},{"title":"论文阅读笔记：不规则采样时间序列的预测任务（3）","text":"前两周看的5篇论文，主要和ISMTS的预测任务有关。本周任务是依据这些论文，和这周新调研的论文，初步总结ISMTS预测的核心问题。 注：ISMTS = Irregular Sampled Medical Time Series，ISTS = Irregular Sampled Time Series 1. 核心问题核心问题主要围绕数据特征和模型特征这两个方面提出。下面会分别展开叙述。 1.1 数据特征之前阅读的关于ISTS的综述[1]，归纳出了ISTS具有三个特点，这些特点同样适用于ISMTS，也是该场景下的相关论文要解决的问题： irregular sampling：患者进行检查的时间间隔不固定。 variable number of observations：每个患者的状况不同，进行检查的次数不同。 lack of alignment：患者的生理指标的检测次数不固定，导致某些变量值缺失。 还有一个未提到的特点，就是4. dependencies：患者不同生理指标有联系，同一指标在不同时刻的值也有联系。 这些特点往往包含着重要信息。 下面介绍相关论文对一个或多个问题的解决方案： 文献[2]使用离散化方法(Discretization)和直接插值技术(Direct Value Interpolation，即DVI)来处理缺失。利用残差和MGTP(Multi-task Gaussian Process)，训练模型参数，捕获不同患者特有的短期变化。 文献[3]更关注某时刻的邻域内的依赖关系，所以使用了自注意力机制(masked self-attention)来建立模型。 文献[4]使用高斯过程处理缺失值，利用缺失所导致的不确定性来提高预测未来事件的可靠性。 文献[5]使用高斯过程处理缺失值，time-aware机制处理不规则的时间间隔，双attention机制关注数据的重要性和缺失值蕴含的医疗信息。 文献[6]根据数据的分布，对生理指标观测值的缺失和临床干预等级缺失，使用了不同的处理方案。并进一步开发了一种新的hazard rate function(危险率函数)，来预测临床事件发生的概率，捕获多种临床事件之间的相互依赖性。 文献[7]是将非线性降维与表达性时间序列模型相结合，处理缺失值。 文献[8]指出将不规则采样时间序列问题，视作缺失数据问题，将ISTS数据，建模为从连续但未观察到的函数采样的索引值对序列。 小结：虽然以上解决方案的侧重点不完全一致，但是对缺失的处理是必要的。在做ISMTS预测任务时，设计合适的缺失处理方案是重要环节。 1.2 模型特征RNN模型、LSTM模型等缺乏可解释性。所以引入attention机制，可增强模型的可解释性，并保持预测性能。 文献[9]、文献[10]和前文提到的使用了attention机制的论文，在一定程度上增强了模型的可解释性。 小结：在做ISMTS预测任务时，保证模型预测性能的同时，也要关注可解释性。 2. 数据集下面总结已调研论文所使用的数据集。 常见的医疗场景数据集：MIMIC-III, PhysioNet 2012, eICU Collaborative Research Data Set, PhysioNet 2019。 与人类活动相关的数据集Human Activity。 脑电波数据集EEG。 手势模式数据集UWaveGestureLibraryAll，是一个单变量时间序列数据集，由简单的手势模式组成，分为八类。 MuJoCo (multi-joint dynamics with contact) 是一个物理引擎，用于实现对强化学习方法进行基准测试的环境。利用此数据集，验证模型捕捉动态隐层状态的能力。 Krishnan等人[11]基于手写数字mnist数据集，生成了一个名为Healing MNIST的数据集，该数据集旨在反映人们在真实医疗数据中发现的许多属性，长期结构、噪声、动作被运用在数字序列上。 还有一些常见的图片数据集，如mnist数据集、CelebA数据集、SPRITES数据集。有论文是将它们转变成了时间序列，再进行实验，或是验证自己模型的插补性能。 3. 总结总结一下，ISMTS预测的核心问题，与数据特征和模型特征两个方面有关。这也不仅限于医疗场景，其他场景下进行ISTS预测的核心问题可能也是这些。可以针对这些问题提出通用的解决方案，再在不同场景下的数据集验证。 学长之前提到时序数据的低质量问题，包括缺失、异常、样本极少三个方面。现有的ISTS预测论文，有关注缺失处理这一方面，但暂时未看到其他两个方面的，或许是个方向。 在调研过程中，还发现了两篇ISTS在交通[12]、消费[13]场景下的应用。下周打算看看。 查找资料时，发现用于ISTS的模型，很多都来自NLP领域。或许可以通过了解阅读NLP领域的新模型，找到一些灵感。 4. 参考文献 [1] A Survey on Principles, Models and Methods for Learning from Irregularly Sampled Time Series (arxiv, 2020) [2] Learning Adaptive Forecasting Models from Irregularly Sampled Multivariate Clinical Data (AAAI, 2016) [3] Attend and Diagnose: Clinical Time Series Analysis Using Attention Models (AAAI, 2018) [4] Scalable Joint Models for Reliable Uncertainty-Aware Event Prediction(TPAMI, 2018) [5] DATA-GRU: Dual-Attention Time-Aware Gated Recurrent Unit for Irregular Multivariate Time Series (AAAI, 2020) [6] Deep State-Space Generative Model For Correlated Time-to-Event Predictions (KDD, 2020) [7] GP-VAE: Deep Probabilistic Time Series Imputation (PMLR，2020) [8] Learning from Irregularly-Sampled Time Series: A Missing Data Perspective (ICML，2020) [9] RETAIN: An Interpretable Predictive Model for Healthcare using Reverse Time Attention Mechanism (NIPS，2016) [10] ATTAIN: Attention-based Time-Aware LSTM Networks for Disease Progression Modeling (IJCAI，2019) [11] Deep Kalman Filters (arxiv, 2015) [12] Revisiting Spatial-Temporal Similarity: A Deep Learning Framework for Traffic Prediction (AAAI, 2019) [13] RESTFul: Resolution-Aware Forecasting of Behavioral Time Series Data (CIKM, 2018)","link":"/2021/04/08/survey-prediction%EF%BC%883%EF%BC%89/"},{"title":"线性代数：第5章 相似矩阵及二次型","text":"本章讨论 1. 向量的内积、长度及正交性 内积：记为$[x, y]=x^Ty$。结果是一个实数。有如下性质： $[x, y] = [y, x]$ $[\\lambda x, y]= \\lambda[x, y]$ $[x + y, z]= [x, z] + [y, z]$ 当$x=0$时，$[x, x]=0$；当$x \\neq 0$时，$[x, x] &gt; 0$ 施瓦茨不等式：$[x, y]^2 \\leq [x, x] [y, y]$ 长度：$||x|| = \\sqrt{[x, x]} = \\sqrt{x_1^2+x_2^2+\\cdots+x_n^2}$，长度也被称为范数。 当$||x||=1$时，称向量$x$为单位向量。注意，单位向量是长度为1的向量，不代表向量的每一项为1！ 若向量$a\\neq 0$，则$\\frac{a}{||a||}$是单位向量，这一过程称为把向量$a$单位化。 夹角$\\theta = arccos\\frac{[x, y]}{||x|| ||y||}, x, y \\neq 0$ 正交：当$[x, y]=0$时，称向量$x$与$y$正交。显然零向量和任何向量都正交。 定理1：正交向量组，是指一组两两正交的非零向量。若有n维正交向量组，则这些向量线性无关。 标准正交基：若n维向量$e_1,e_2,\\cdots,e_r$是向量空间$V$的一个基，若它们两两正交，且都是单位向量，则称它们是$V$的一个标准正交基。 $V$中的任一向量$a=\\lambda_1e_1+\\lambda_2e_2+\\cdots+\\lambda_re_r$。为求$a$的坐标，也就是$\\lambda$，上式左乘$e_i^T$，则$e_i^Ta=\\lambda_ie_i^Te_i=\\lambda_i$。故$\\lambda_i=e_i^Ta=[a, e_i]$。用这个公式可以很方便的求得向量坐标，故在给向量空间取积时常常取标准正交基。 标准正交化：把向量空间的一个基，化为标准正交基的过程。常用施密特正交化，把$a_1,a_2,\\cdots,a_r$标准正交化： 第一步： \\begin{aligned} b_1&=a_1,\\\\ b_2&=a_2-\\frac{[b_1, a_2]}{[b_1, b_1]}b_1, \\\\ &\\cdots\\cdots\\\\ b_r&=a_r-\\frac{[b_1, a_r]}{[b_1, b_1]}b_1-\\frac{[b_2, a_r]}{[b_2, b_2]}b_2-\\cdots-\\frac{[b_{r-1}, a_r]}{[b_{r-1}, b_{r-1}]}b_{r-1} \\end{aligned} 第二步，把它们单位化，得： e_1=\\frac{1}{||b_1||}b_1, \\cdots, e_r=\\frac{1}{||b_r||}b_r 例： 已知$a_1=\\left(\\begin{matrix} 1\\\\1\\\\1 \\end{matrix}\\right)$，求一组非零向量$a_2,a_3$，使$a_1,a_2,a_3$两两正交。 解：$a_2,a_3$应是$a_1^Tx=0$的解（正交条件），即$x_1+x_2+x_3=0$。 则基础解系$\\xi_1=\\left(\\begin{matrix} -1\\\\1\\\\0 \\end{matrix}\\right), \\xi_2=\\left(\\begin{matrix} -1\\\\0\\\\1 \\end{matrix}\\right)$。 把基础解系进行施密特正交化，则得$a_1=\\left(\\begin{matrix} -1\\\\1\\\\0 \\end{matrix}\\right), a_2=\\left(\\begin{matrix} -1\\\\{-1}\\\\2 \\end{matrix}\\right)$（$a_2,a_3$已两两正交） 因为$a_2,a_3$是$\\xi_1,\\xi_2$的线性组合，则它们仍与$a_1$正交，所以$a_2,a_3$即为所求。（基础解系的线性组合，也是原方程的解） 正交矩阵：如果n阶矩阵$A$满足$A^TA=E$，即$A^{-1}=A^T$，则称$A$为正交矩阵，也称正交阵。正交阵一定是方阵。 用$A$的列向量表示，则$a^T_ia_j=\\begin{cases}1, i=j\\\\0, i\\neq j\\end{cases}$ 由上式，可知方阵$A$为正交矩阵的充要条件是$A$的列向量都是单位向量，且两两正交（可以和标准正交基的定义作类比）。 因为$a^T_ia_j$是内积的表达式，当$i=j$，说明是两个相同向量作内积，且得到1，则该向量是单位向量；当$i\\neq j$，说明是两个不同向量作内积，且得到0，说明这两个向量正交。故能够得到方阵$A$为正交矩阵的充要条件。 正交矩阵的性质： 若$A$为正交矩阵，则$A^{-1}=A^T$也是正交矩阵，且$|A|=1$或$-1$。 若$A,B$为正交矩阵，则$AB$也为正交矩阵。 正交变换：若$P$为正交矩阵，则线性变换$y=Px$称为正交变换。且$||y||=\\sqrt{y^Ty}=\\sqrt{x^TP^TPx}=||x||$，变换后向量长度不变。 2. 方阵的特征值与特征向量 设$A$是n阶方阵，如果数$\\lambda$和n维非零列向量$x$使 Ax = \\lambda x 成立，那么，数$\\lambda$被称为矩阵$A$的特征值，非零向量$x$称为$A$对应于特征值$\\lambda$的特征向量。 上式也可写成： (A-\\lambda E)x=0 这是含有n个方程n个未知数的齐次线性方程组。它有非零解的充要条件是： |A-\\lambda E|=0 （因为齐次线性方程组一定有零解，所以它要么有唯一解，要么有无数解，不可能无解。 根据第3章提到的【线性方程组的解】，只有在无数解这一情况下方程组才可能有非零解。所以，$R(A-\\lambda E) &lt; n$，系数矩阵是奇异矩阵，行列式为0。） $|A-\\lambda E|=0$是以$\\lambda$为未知数的一元n次方程，称为矩阵$A$的特征方程。左端是$\\lambda$的n次多项式，记作$f(\\lambda)$，称为矩阵$A$的特征多项式。所以，n阶方阵$A$在复数范围内有n个特征值，为$\\lambda_1\\sim\\lambda_n$（包括重根）。 有两个重要性质： $\\lambda_1+\\lambda_2+\\cdots+\\lambda_n=a_{11}+a_{22}+\\cdots+a_{nn}$ $\\lambda_1\\lambda_2\\cdots\\lambda_n=|A|$ $a_{ii}$是$A$对角线上的元素。 由性质2可知，$A$是可逆矩阵的充要条件是它的n个特征值全不为零（因为可逆矩阵的充要条件是行列式不为0）。 其他性质： 若$p_i$是矩阵$A$对应于特征值$\\lambda_i$的特征向量，则$kp_i$（$k\\neq 0$）也是对应于$\\lambda_i$的特征向量。 若$\\lambda$是$A$的特征值，则$\\lambda^k$是$A^k$的特征值。$\\phi(\\lambda)$是$\\phi(A)$的特征值（其中，$\\phi(\\lambda)=a_0+a_1\\lambda+\\cdots+a_m\\lambda^m$，$\\phi(A)=a_0E+a_1A+\\cdots+a_mA^m$）。 定理2：设$\\lambda_1\\sim\\lambda_m$是方阵$A$的m个特征值，$p_1\\sim p_m$依次是与之对应的特征向量，如果$\\lambda_1\\sim\\lambda_m$各不相等，则$p_1\\sim p_m$线性无关。 3. 相似矩阵 设$A, B$都是n阶方阵，若有可逆矩阵$P$，使： P^{-1}AP=B 则称$B$是$A$的相似矩阵，运算$P^{-1}AP$称为对$A$进行相似变换。可逆矩阵$P$称为把$A$变成$B$的相似变换矩阵。 定理3：若n阶矩阵$A$与$B$相似，则$A$与$B$的特征多项式相同，从而$A$与$B$的特征值也相同。 推论：若n阶矩阵$A$与对角矩阵 \\Lambda=\\left(\\begin{matrix} \\lambda_1 & & & \\\\ &\\lambda_2 & & \\\\ & &\\ddots & \\\\ & & &\\lambda_n \\end{matrix}\\right) 相似，则$\\lambda_1\\sim\\lambda_n$是$A$的n个特征值。 把矩阵$A$对角化：指寻找相似变换矩阵$P$的过程，使$P^{-1}AP=\\Lambda$，$\\Lambda$是对角矩阵。 由$P^{-1}AP=\\Lambda$，得$AP=P\\Lambda$，即$A(p_1,p_2,\\cdots,p_n)=(p_1,p_2,\\cdots,p_n)\\left(\\begin{matrix} \\lambda_1 &amp; &amp; &amp; \\\\ &amp;\\lambda_2 &amp; &amp; \\\\ &amp; &amp;\\ddots &amp; \\\\ &amp; &amp; &amp;\\lambda_n \\end{matrix}\\right)$ 于是有$Ap_i=\\lambda_ip_i$ $\\lambda_i$是$A$的特征值，$p_i$是相似变换矩阵$P$的列向量。那么，相似变换矩阵$P$可以由$A$的n个特征向量构成（注：$P$不唯一），但是要满足$P$是可逆矩阵这一条件。 定理4：n阶矩阵$A$能对角化的充要条件是$A$有n个线性无关的特征向量。 因为$A$的n个特征向量可构成相似变换矩阵$P$，而$P$是可逆的，故$P$的每个列向量都线性无关（这是可逆矩阵的性质）。因此，$A$的n个特征向量也必须线性无关。 推论：如果n阶矩阵$A$的n个特征值互不相等，则$A$与对角矩阵相似。（由定理2可得）","link":"/2021/04/12/linear-algebra-ch5/"},{"title":"模式识别导论：第1章 基本介绍","text":"在此整理《Introduction to Pattern Recognition》这门课的笔记，这是第一篇文章，对模式识别的定义、流程、相关术语进行了简单介绍。参考资料是老师上课时使用的课件。 1. Definition 模式识别是关于机器如何能够感知环境，分辨事物所遵循的模式，并根据模式做出决策的研究。 2. Pattern Recognition Systems 这是模式识别的完整过程，右半部分是训练模型的过程，左半部分是利用模型，对采集到的数据进行分类的过程。 3. The Design Cycle 这是训练模型部分的决策过程。 Data collection 选择质量较好、有代表性的样本，并不是样本数量越多越好。 采集数据的时间和成本也是限制因素。 Feature extraction / selection 选择有区分度的特征，考虑训练成本和灵活度。 Model selection 考虑样本之间的依赖和先验信息，对缺失特征的处理，模型的计算复杂度等。 Training 利用训练集训练模型，得到模型参数。 有如下训练方式（本课程学习1、2、3、6）： Supervised Learning Unsupervised Learning Semi-Supervised Learning（半监督学习） Reinforcement Learning（强化学习） Learning Theory Deep Learning Evaluation 利用测试集，评估模型效果。 为防止过拟合，有两种再采样方法： Independent Run（独立运行，也被称为Bootstrap）：随机划分训练集和测试集，运行n次，计算平均正确率。 Cross-validation（交叉验证）：K-fold交叉验证，即训练集和测试集大小为 K - 1 : 1。运行K次，计算平均正确率。 选择平均正确率最高的模型作为最终模型。","link":"/2021/04/17/PR-ch1/"},{"title":"模式识别导论：第2章 Bayesian Decision Theory","text":"本章介绍贝叶斯决策理论，介绍贝叶斯公式、决策理论、误差分析、损失函数、最小化风险规则。 1. Bayes Rule 贝叶斯公式（Bayes Formula）： p(w|x)=\\frac{p(x|w)p(w)}{p(x)} $p(w|x)$是后验概率（posterior），指某证据下现象发生的概率。 $p(x|w)$是似然（likelihood），指某现象下该证据发生的概率。 $p(w)$是先验概率（prior），指依据过往经验得到现象发生的概率。 $p(x)$是证据（evidence），指依据过往经验得到证据发生的概率。 贝叶斯决策法则（Bayes Decision Rule）： 比较现象的后验概率，后验概率大的现象则为当前现象。贝叶斯公式等号右边的部分，分母是一样的，所以比较分子即可。 最大似然法则（Maximum Likelihood (ML) Rule）： 若$p(w)$是一样的，比较似然$p(x|w)$即可。 2. Bayes Error 多分类问题的错误率（Probability of error） p(error|x)=1-max(p(w_1|x),p(w_2|x),\\cdots,p(w_c|x)) 错误率是错误分类的概率。个人认为这里假设了不同分类的分布是相互独立的。根据Bayes Rule，贝叶斯分类器做分类时会将后验概率最大的分类作为分类结果，但分类结果有可能是错误的，即其他情况的概率就是分错的概率，也就是【1 - 该分类结果的概率】。 贝叶斯误差（Bayes Error Rate） 贝叶斯分类器选某一类作为分类结果时，有最低的错误率，则该错误率被称为贝叶斯误差。不同分类的分布并不是相互独立的。 错误率的测定： p(error)=\\sum^{M}_ {i=1}\\int_{R_i}{(\\sum_{j\\neq i}^{M}p(x|w_j)p(w_j))dx} M是分类总数；$R_i$是分类i的后验概率最大时，x的范围。$R_i$由decision boundary（决策边界）划分。 计算的是贝叶斯分类器将i类作为结果时，真实结果是其他类的概率，也就是错误率。 二分类的例子： p(error)=\\int_{R_1}p(x|w_2)p(w_2)dx+\\int_{R_2}p(x|w_1)p(w_1)dx 当决策边界选在$x_b$时，此时的错误率最小，即为贝叶斯误差： 当决策边界不在$x_b$时，此时的错误率不是最小，多了reducible-error（可减少误差）。可减少误差表示误差是可以通过优化方法减少的，比如在这里就是选择$x_b$作为决策边界： 3. Loss Function 条件风险（Conditional risk）： R(a_i|x)=\\sum^c_{j=1}\\lambda_{ij}P(w_j|x) 即做出决策$a_i$的风险，决策$a_i$就是将$w_i$作为分类结果。$\\lambda_{ij}$是真实分类为$w_j$时，做出决策$a_i$的损失。 若做出某决策$a_i$时，有最小风险$R(a_i|x)$。则该风险为Bayes Risk（贝叶斯风险）。 最小风险决策规则（Minimum Risk Decision Rule）： 选择有最小风险$R(a_i|x)$时做出的决策$a_i$。 根据条件风险公式计算并比较。 4. Discriminant Function 若做出正确决策$a_i$，则$\\lambda_{ii}=0$，所以上述条件风险公式可简化为$R(a_i|x)=1 - P(w_i|x)$，$R(a_i|x)$越小越好，所以要选出$w_i$，使$P(w_i|x)$最大。这是Minimum-Error-Rate的想法。 判别函数（discriminant functions）：$g_i(x),i=1,2,\\cdots,c$ 从Minimum-Error-Rate的角度出发，则令$g_i(x)=P(w_i|x)$。 当$g_i(x) &gt; g_j(x)$，则将x分类到$w_i$。 如果是二分类问题，常用以下两种形式的判别函数。当$g(x)&gt;0$，选择$w_1$，否则$w_2$： $g(x)=p(w_1|x)-p(w_2|x)$ $g(x)=ln\\frac{p(x|w_1)p(w_1)}{p(x|w_2)p(w_2)}$ 5. Normal Distribution 正态分布公式： 标准情形： p(x)=\\frac{1}{\\sqrt{2\\pi}\\delta}e^{-\\frac{1}{2}(\\frac{x-\\mu}{\\delta})^2} 拓展到多维度： p(x)=\\frac{1}{(2\\pi)^{d/2}|\\Sigma|^{1/2}}e^{-\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu)} $\\Sigma$是协方差矩阵。 多元正态分布的判别函数 由上一小节可知，判别函数可写作：$g_i(x)=ln(p(x|w_i))+ln(p(w_i))$ 将多维度的正态分布公式代入，可得：$g_i(x)=-\\frac{1}{2}(x-\\mu_i)^T\\Sigma^{-1}_i(x-\\mu_i)-\\frac{d}{2}ln(2\\pi)-\\frac{1}{2}ln|\\Sigma_i|+ln(p(w_i))$，假设$p(x|w_i)\\sim N(\\mu_i,\\Sigma_i)$ 假设每种分类的样本的协方差相同，即$\\Sigma_i=\\Sigma$，则将上式的$g_i(x)$化简得到： \\begin{aligned} g_i(x)&=\\Sigma^{-1}\\mu_i x - \\frac{1}{2}\\mu_i^T\\Sigma^{-1}\\mu_i+ln(p(w_i)) \\\\ &=w_ix+w_{i0} \\end{aligned} 发现是个线性判别函数。 更一般地，每种分类的样本的协方差不一定相同，则将$g_i(x)$化简得到： \\begin{aligned} g_i(x)&=x^T(-\\frac{1}{2}\\Sigma^{-1}_i)x +\\Sigma^{-1}_i\\mu_i x - \\frac{1}{2}\\mu_i^T\\Sigma^{-1}_i\\mu_i-\\frac{1}{2}ln|\\Sigma_i|+ln(p(w_i)) \\\\ &=x^TW_ix+w_ix+w_{i0} \\end{aligned} 对比上式，多了一些项，这是因为在协方差相同时才能将多出的这些项移除。 6. Maximum Likelihood 上一节假设了$p(x|w_i)$服从高斯分布，所以需要估计参数$\\mu_i$和$\\Sigma_i$。 ML的优势：简单，且样本数量增加时可以收敛。 假设每个样本属于集合$D$，且独立同分布，令$\\theta=(\\mu,\\Sigma)$，则$p(D|\\theta)=\\prod_{i=1}^np(x_i|\\theta)$ $p(D|\\theta)$是$\\theta$关于样本的似然。 ML参数估计（ML Parameter Estimation），即找到使上式最大的$\\theta$。 若上式达到最大值，根据贝叶斯决策理论，误差最小。 可以将上式化为对数似然（log likelihood），求导数为0时的$\\hat{\\theta}$： $\\hat{\\mu}=\\frac{1}{n}\\sum_{k=1}^nx_k$ $\\hat{\\sigma}^2=\\frac{1}{n}\\sum_{k=1}^n(x_k-\\hat{\\mu})^2$ 利用ML训练分类器： 第一步：利用ML，计算每个分类下参数的值。 第二步：利用多元正态分布的判别函数和计算得到的参数值，对每个分类计算判别函数，得到决策边界。 例子： 注意，图3判别函数的形式可能会引起误会。但其实就是个式子，不是矩阵。","link":"/2021/04/17/PR-ch2/"},{"title":"论文阅读笔记：不规则采样时间序列的方法归纳","text":"这周主要任务是调研不规则采样时间序列在交通、消费场景的应用，并阅读综述A Review of Deep Learning Methods for Irregularly Sampled Medical Time Series Data(arxiv, 2020)，整理处理ISMTS的方法。 1. 交通、消费场景的调研我略读了上周调研到的关于交通和消费场景的时间序列的两篇文章，发现交通场景的时间序列，往往和地理位置结合，归为时空序列这一类；消费场景的时间序列，大都也是周期性的采样。 因此，个人认为，这两个场景的数据集，和不规则采样时间序列关系不大。 2. 处理ISMTS的方法上周我介绍了ISMTS的4个特点和相关论文针对这些特点的解决方案，本周阅读的综述，进一步将这些解决方案进行分类，并列出了经典模型：Missing data-based，指的是以处理缺失数据为基础的模型；Raw data-based，指的是以处理原始数据为基础的模型，主要关注原始数据之间的依赖关系。 2.1 Missing data-based Two-step：两阶段方法，即对缺失数据进行填补后，再进行下游任务（分类、聚类、预测）。容易导致次优结果。 End-to-end：缺失数据的填补只是作为下游任务执行的辅助。RNN、GRU-D只注意到局部信息（如最近的观测值），未关注序列的整体结构。而LGnet能够同时关注全局信息和局部信息。 2.2 Raw data-based原始数据之间的关系主要分为intra-series relation和inter-series relation，即序列内关系和序列间关系。 序列内关系体现在irregular time intervals（不规则时间间隔），序列间关系体现在multi sampling rates（不同变量的采样率不同）。针对这两个特点，模型分为了两个类别： irregular time intervals：T-LSTM提出的time-aware机制，直接对不规则时间间隔进行建模。然而，对于多变量ISMTS，必须使每个维度的的时间间隔对齐，这又变成了解决缺失数据的问题了。DATA-GRU在此基础上，引入了注意力机制。MRE模型没有采用RNN对时序的动态性进行建模，而是更关注时间聚类不变性。 multi sampling rates：无需对缺失数据进行处理，MR-HDMM，是多采样率层次深马尔可夫模型，但忽略了不规则的时间间隔。 IPN是一个完全模块化的插值预测网络。 2.3 小结上周我提到了ISMTS的两方面核心问题，而这篇综述归纳了处理ISMTS的三点挑战，相当于进一步补充： “如何平衡缺失数据的插补与预测任务？”这一点相当于建立Missing data-based的模型时，应选择Two-step方法还是End-to-end方法，个人认为后者会更好。 “如何处理intra-series relation和inter-series relation？”个人认为模型要尽量能够平衡这两种关系。而且，文章还指出了对于不同定义下的多变量ISMTS，这两种关系的定义也不同。多变量ISMTS，可以视作多个单变量的ISMTS的结合，也可以看作一个整体。 “选择missing data-based还是raw data-based的模型？”两种方法各有优劣，前者是最常用的方法，但是有手工填补引入新的数据依赖，缺失过多会导致填补规模过大等问题。 后者中针对irregular time intervals的处理，为使多维度对齐，又变成了填补缺失数据的问题。针对multi sampling rates的处理，要考虑多变量ISMTS的具体定义：视为多个单变量的ISMTS的结合，参数数量可能会很多；视为一个整体，要作一些关于数据的假设。 3. 思考 下周我打算进一步了解LGnet和IPN，以及神经常微分方程。 经过调研，发现不规则采样时间序列的应用场景确实较少，至少到目前为止我没有找到其他场景的有关数据集。那未来我是否就不再关注其他场景，只专研医疗场景？","link":"/2021/04/14/survey-ISTS/"},{"title":"模式识别导论：第3章 Linear Classification Models","text":"本章讲了介绍了线性分类模型，包括线性判别函数、最小二乘分类、多分类等。 1. Parametric VS Non-Parametric可以采用参数方法或非参数方法给样本做分类。 参数方法：假设样本服从某种分布，利用样本估计参数，如上一章介绍的ML Parameter Estimation。如果假设正确，结果会很准确，否则结果很差。 非参数方法：假设判别函数为某种形式，不假设样本分布。如：神经网络、SVM等。局部最优，但易于使用。 2. Linear Classification Models2.1 Geometry of Linear Discriminants 对于平面内的二分类问题，判别函数$g(x)=w^Tx+w_0$，决策边界为$g(x)=0$。因为$g(x)$是线性的，所以是超平面（hyperplane）。这是判别函数的几何意义。 原点到决策平面的距离：$-\\frac{w_0}{||w||}$ 在决策平面上有两点$x_a,x_b$，则易得$w^T(x_a-x_b)=-w_0-(-w_0)=0$（在决策平面上的点的$g(x)$值为0）。所以$w^T$是垂直于决策平面的向量。 设原点在决策平面上的投影为$x_0$，则向量$x_0-0=x_0$和向量$w^T$平行。 故原点到$x_0$的距离，就是原点到决策平面的距离，为$\\frac{w^Tx_0}{||w||}$（用到了向量内积）。$||w||$是行列式的绝对值。 $x_0$是决策平面上的点，进一步化简得到$-\\frac{w_0}{||w||}$。 任一点x到决策平面的距离：$\\frac{g(x)}{||w||}$ 设任一点x，在决策平面上的投影为$x_0$，同上，可解得x到决策平面的距离为$\\frac{w^Tx+w_0}{||w||}=\\frac{g(x)}{||w||}$。 2.2 Convexity of Decision Regions 决策区域的凸性，即若两点同属于一个区域，则这两点的连线上的点都属于这个区域。 2.3 Least-Squares Classification $Xw=b$，则平方误差和公式$J_s(w)=||Xw-b||^2$。 若$X$是非奇异的，则可以直接$w=X^{-1}b$。但是通常没有这么巧合。 Pseudo-inverse method（伪逆方法）：对平方误差和公式求导，导数为0时，求得$w=(X^TX)^{-1}X^Tb$。$X^TX$一般都是奇异矩阵，可以求逆。但不能完全保证是奇异矩阵，这时就可以写成$w =\\lim_{\\epsilon\\to0}(X^TX+\\epsilon I)^{-1}X^Tb$。 Least-Mean-Squares (LMS)：除了伪逆方法，还可用LMS方法计算参数。即用到了Gradient Descent（梯度下降）。平方误差和公式对$w$求导，得到$\\nabla J_s(w)=2(Xw-b)X$，故更新公式$w(k+1)=w(k)+\\eta\\nabla J_s(w)$。因为梯度噪声，LMS方法难以得到最优值。 2.4 Generalized Linear Model 广义线性模型，也是一样的道理。一般用LMS方法求参数。 3. Multi-Class Classification 多分类问题，可以使用一些two-class classifiers： One-versus-the-rest：划分出一个分类后，在剩下的分类里继续划分。需要c个分类器。 One-versus-one：所有分类两两划分。需要c(c-1)/2个分类器。 这两种方法都会导致一些区域无法被分类： 第一种： 第二种：","link":"/2021/04/19/PR-ch3/"},{"title":"模式识别导论：第4章 Neural Networks","text":"本章介绍了神经网络的基本知识，包括感知器，后向传播算法，还介绍了特殊的神经网络——RBF，正则化项的引入，以及实际应用中设计多层神经网络需要考虑的问题。 1. Perceptron 感知机使用的函数是非线性激活函数： f(x)=\\begin{cases} +1,&x \\geq 0 \\\\ -1,&x < 0 \\end{cases} 假设两类线性可分，则超平面是$w^Tx=0$，若$w^Tx&gt;0$，则分为类1；若$w^Tx0$，表示分类正确，否则分类错误。 perceptron cost：$J(w)=-\\sum_{x\\in M} w^Txy$，其中，$M$是被分类错的样本。 用梯度下降更新权重，即$w(t+1)=w(t)+\\eta\\sum xy$： 也就是说，每分类错一个，就要更新权值。 例子： 2. Multi-Layer Neural Network（Multi-Layer Perceptron） 常使用Back-Propagation Algorithm（后向传播算法）更新权重。 3. Radial Basis Function（RBF） 径向基神经网络，通常由输入层、隐藏层、输出层组成。隐藏层的激活函数是Gaussian function（高斯函数）。经过高斯函数映射，隐藏层的输出线性可分，乘不同的权重，传给输出层。 高斯函数：$\\phi_j(x)=exp(-\\frac{1}{2v^2}\\sum_{i=1}^{n}(x_i-x_j)^2)$，$u$是中心，$v$是宽度。 Parameter Determination： 高斯函数中的$u, v$，还有权重$w$，都是需要决定的参数。 算法： 可用梯度下降法更新。 优点：比多层感知机（MLP）计算速度快，且隐藏层输出更具可解释性。缺点：单个神经元的计算速度比MLP慢。 4. Regularization 在损失函数的基础上，添加正则化项(Regularization term)，使决策边界更光滑。 目标函数：$R_{emp}+\\lambda \\psi(f)$，前者是训练损失，后者是正则化项。目标是最小化目标函数。 一般令$\\psi(f)=||w||_2^2$ 5. Practical Techniques 如何设计MLP，以下是几点tips： Scaling input（缩放输入） Target values（设计目标值） Number of hidden layers Number of hidden units Initializing weights（不要初始化为0） Stochastic and batch training（随机训练和批训练） Stopped training（及时停止训练）","link":"/2021/04/20/PR-ch4/"},{"title":"论文阅读笔记：Neural Ordinary Differential Equations","text":"这周了解了一个比较热门的模型：Neural Ordinary Differential Equations（神经ODE方程）。它是一种全新的神经网络结构，层与层之间不再是离散的，而是连续的。这一特性使之能更好地应用于不规则采样时间序列的处理。 1. 结构 许多神经网络模型，如残差网络、RNN等，都是通过一系列的转换来获得隐层状态。而残差网络的隐层状态计算可以表示为：h_{t+1}=h_t +f(h_t,\\theta_t) 如果添加无限增加隐含层数，无限缩小时间步，则上式可转换为常微分方程： \\frac{dh(t)}{dt}=f(h_t,t,\\theta) 若有初始状态$h(0)$，则T时刻的状态$h(T)$可通过求解上述常微分方程得到。现在已有很成熟的ODE求解器，可直接将之当成黑盒使用。 这就是神经ODE的基本思路：将层与层之间的离散关系，变为连续关系。 由此可推出神经ODE的前向传播过程： 通过一个连续的转换函数（神经网络）对t时刻的输入进行非线性变换，从而得到$f(h(t),t,\\theta)$。随后ODE求解器对$f(h(t),t,\\theta)$进行积分，得到$h(t_1)$： h(t_1)=h(t_0)+\\int^{t_1}_{t_0}{f(h(t),t,\\theta))dt}对比残差网络的计算，可以说残差网络是神经ODE的离散形式。 反向传播过程： 为了更新参数$\\theta$，使用了adjoint sensitivity method来计算梯度，无需保存大量的中间结果。 设损失函数为$L$，隐层状态为$z$，定义$a(t)=\\frac{\\partial L}{\\partial z(t)}$。 由链式法则，得： \\frac{da(t)}{dt}=-a(t)^T\\frac{\\partial f(z(t),t,\\theta)}{\\partial z(t)} 这也是一个常微分方程。利用第二个ODE求解器，可以从后往前求出每个时刻的$a(t)$。继而计算损失函数对$\\theta$的梯度： \\frac{dL}{d\\theta}=-\\int^{t_0}_{t_1}a(t)^T\\frac{\\partial f(z(t),t,\\theta)}{\\partial \\theta}dt 利用梯度，可更新$\\theta$。 需要注意的是，利用常微分方程求$a(t)$时，需要对应时刻的$z(t)$。而神经ODE为节约内存，前向传播没有保留中间结果，所以需要重新计算$z(t)$。这也可以用求$a(t)$的ODE求解器来求解。 算法如下： 2. 优势 内存优化 传统的反向传播需要利用前向传播计算得到的激活值计算梯度，这样会占用大量内存，且误差会累积，这是深度学习模型的瓶颈之一。本文给出的adjoint sensitivity method来计算梯度，使用的内存仅仅是常数级别的，不用保存大量的中间结果。 自适应计算 现在已有很成熟的ODE求解器，可以确保神经ODE在误差容忍度之内逼近常微分方程的真实解。调整误差容忍度，即在模型准确度与计算成本之间做权衡。 可缩放且可逆的规范化流 - 注：这点暂时没能看懂 连续时间模型 由于神经ODE的层级是连续的而不是离散的，它可以计算任一时刻的值，可以直接处理不规则采样时间序列，而无需像多数模型一样对不规则间隔进行离散化。后者会损失不规则间隔蕴含的信息。 3. 局限性本文认为神经ODE有一些局限性。 小批量学习可能更加复杂。因为需要求解多个ODE的误差。 需保证ODE有唯一解。根据Picard’s existence theorem，神经网络需具有有限的权值并且使用Lipshitz非线性（如Tanh和ReLU）。 ODE求解器误差容忍度的设置需要依靠经验。为了在模型准确度与计算成本之间做权衡，前向传播和后向传播过程都要设置误差容忍度。本文认为，对于sequence modeling，可以使用1.5e-8，对于分类问题和概率密度实验，可以使用1e-3和1e-5。 反向传播可能引入数值误差。因为反向传播时，由于重新从后往前计算$z(t)$，可能偏离原来前向传播计算的动态轨迹。可以引入一些检查点来解决。 4. 处理不规则采样时间序列在第二节中已经讲到了神经ODE在处理不规则采样时间序列的优越之处，本文建立了Latent ODE模型，用于为不规则采样时间序列建模。 Latent ODE模型，相当于一个变分自编码器。标准RNN作为encoder，神经ODE作为decoder： - 疑问：使用RNN作为encoder计算隐层状态时，本文似乎没有提及如何处理不规则间隔。 5. 小结 神经ODE模型将ODE应用于深度学习领域，将层与层之间的关系变得连续，使模型更具解释性。同时，减轻了内存负担，并由于ODE的特性，使我们可以调节误差容忍度来调节模型的精度和成本，而且也为不规则采样时间序列的处理提供了新的思路。 计划跑一下作者提供的源代码，看一些相关文章对该模型的评价。 6. 参考资料 https://cloud.tencent.com/developer/news/375423 http://www.maplelearning.cn/index.php/2020/03/24/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%8Aneural-ordinary-differential-equations%E3%80%8B/","link":"/2021/04/25/neural-ODE/"},{"title":"数值分析：第1章 误差","text":"本章介绍了和误差相关的基本概念，以及数值运算需要注意的一些问题。 1. 误差的来源 模型误差、观测误差、方法误差（截断误差）、舍入误差。 2. 误差、误差限和有效数字2.1 误差 近似值$x^{\\star}$和准确值$x$的差，也可称为绝对误差。 记作$e^{\\star}=x^{\\star}-x$。也可写作$e(x^{\\star})$。 误差为正值时，称为强近似；为负值，称为弱近似。 2.2 误差限 误差的绝对值不可能超过的某个正数。 记作$\\epsilon^{\\star}$。有$|e^{\\star}|=|x^{\\star}-x|\\leq \\epsilon^{\\star}$。也可写作$\\epsilon(x^{\\star})$。 对任何数值进行四舍五入后所得到的近似值，它的误差限是它末位的半个单位。（注意要四舍五入才符合） 2.3 有效数字 若近似值$x^{\\star}$的误差限为该值的某一位的半个单位，且从该位开始往左数到$x^{\\star}$的最高位非0数字共有n位，则称近似值$x^{\\star}$有n位有效数字。 通常认为准确值有无穷多位有效数字。 2.4 有效数字与误差限的关系 设$x^{\\star}$为准确值$x$的近似值，且将$x^{\\star}$表示为（$\\alpha_1\\neq 0$）： x^{* } = \\pm 0.\\alpha_1\\cdots\\alpha_m \\times 10^p 则误差限为$\\frac{1}{2}\\times10^{p-n}$，$n$为有效数字。 这也是有效数字的等价定义。 3. 相对误差和相对误差限3.1 相对误差 记作$e^{\\star}_r=\\frac{x^{\\star}-x}{x}$，也可记作$e_r(x^{\\star})$。 由于实际计算中，准确值一般不知道，所以相对误差也定义为（较常用）：$e^{\\star}_r=\\frac{x^{\\star}-x}{x^{\\star}}$。也就是误差 / 近似值。 3.2 相对误差限 由于$|e^{\\star}_r|=|\\frac{x^{\\star}-x}{x^{\\star}}|\\leq \\frac{\\epsilon^{\\star}}{|x^{\\star}|}$，则记相对误差限$\\epsilon^{\\star}_r=\\frac{\\epsilon^{\\star}}{|x^{\\star}|}$。也就是误差限 / 近似值的绝对值。 也可记作$\\epsilon_r(x^{\\star})$。 3.3 相对误差限与有效数字的关系 若$x^{\\star} = \\pm 0.\\alpha_1\\cdots\\alpha_m \\times 10^p$具有$n$位有效数字，则相对误差限为：\\epsilon^{\\star}_r=\\frac{\\epsilon^{\\star}}{|x^{\\star}|}\\leq\\frac{1}{2\\alpha_1}\\times10^{-(n-1)} 有效数字位越多，相对误差限越小。 若$x^{\\star} = \\pm 0.\\alpha_1\\cdots\\alpha_m \\times 10^p$的相对误差限满足关系式$\\epsilon^{\\star}_r=\\frac{\\epsilon^{\\star}}{|x^{\\star}|}\\leq\\frac{1}{2(\\alpha_1+1)}\\times10^{-(n-1)}$，则$x^{\\star}$有$n$位有效数字。 注意到已知有效数字求相对误差限，和已知相对误差限求有效数字，不是等价的。 4. 数值运算中的误差估计 计算$f(x)$的值时，若用近似值$x^\\star$代替$x$，则$f(x)$的值必定存在误差。函数$f(x)$在$x^\\star$附近按泰勒公式展开，得到$f(x^\\star)$的绝对误差：$e(f(x^\\star))\\approx f^{‘}(x^\\star)e(x^\\star)$。 $f(x^\\star)$的相对误差：$e_r(f(x^\\star)) \\approx \\frac{f^{‘}(x^\\star)e(x^\\star)}{f(x^\\star)}$。 $f(x^\\star)$的误差限：$\\epsilon(f(x^\\star)) \\approx |f^{‘}(x^\\star)|\\epsilon(x^\\star)$。 $f(x^\\star)$的相对误差限：$\\epsilon_r(f(x^\\star)) \\approx |\\frac{f^{‘}(x^\\star)}{f(x^\\star)}|\\epsilon(x^\\star)$。 两个数的和、差、积、商的误差估计式： 5. 数值运算中应注意的一些问题 避免两个相近的数相减。由$\\epsilon_r(x^\\star-y^\\star)$的公式可知，分母较小，则相对误差限就比较大。 要防止小数被大数“吃掉”而使有效数字位损失。要靠变化公式来防止有效数字位数损失。 要注意减少运算的次数。 避免做除数绝对值远远小于被除数绝对值的除法。 要选择数值稳定的计算公式。","link":"/2021/04/28/NA-ch1/"},{"title":"数据结构：树","text":"本章总结树的基础知识，以及具体题型。参考了老师上课的课件、在leetcode、牛客网刷的相关题目和《数据结构与算法分析（C++版）（第三版）》。 1. Definitions and Properties主要熟悉下树的相关定义和英文表达。 ancestor（祖先），descendant（后代）。 depth（深度）：某节点的depth，是根节点到该节点的路径长度。 height（高度）：树的高度是最大深度 + 1。 level（层）：根节点是level 0，每往下一层level + 1。 Full binary tree（满二叉树）：每个Internal node（中间节点） 都有两个children（子节点）。 理论：满二叉树叶子个数 = 中间节点个数 + 1。 可用数学归纳法证明： 初始情况：假设有0个中间节点，显然成立。 假设有 n - 1 个中间节点时，理论成立，现在要证明有 n 个中间节点时也成立：当有 n 个中间节点时，假设对某个中间节点 I ，去掉它的两个children，使其变成叶子节点。此时，中间节点的个数变为 n - 1 ，那么叶子节点个数就为 n 。再加回两个children，则中间节点的个数为 n ，叶子节点的个数为 n + 2 - 1 = n + 1（之所以要减 1，是因为节点 I 从叶子节点又变回了中间节点）。得证。 Complete Binary Trees（完全二叉树）：除了最低层外，每一层都是满的。最低层节点按从左到右的顺序排列，不一定是满的。 2. Binary Tree Implementations2.1 Link-based这应该是最常用的实现树的方式了。 满二叉树的空指针数目，是树的节点数目 + 1。（上图直观地显示了这一点） 因为对于满二叉树，每个节点都有两个children，则children总数为2n（假设叶子节点也有children，即为空指针）。而在树中，除了根节点没有父节点，其余节点都有父节点，则parent总数为 n - 1 。2n - (n - 1) = n + 1 ，就是空指针的数目。 2.2 Array-based由于数组的连续性，一般用来实现完全二叉树。 Parent (r) = (r - 1) / 2 if r &gt; 0 and r &lt; n Leftchild(r) = 2r + 1 if 2r+1 &lt; n Rightchild(r) = 2r + 2 if 2r +2 &lt; n 3. Traversal of Binary Tree Preorder traversal（先序遍历）：中-&gt;左-&gt;右（即节点-&gt;节点左子树-&gt;节点右子树的遍历顺序） Postorder traversal（后序遍历）：左-&gt;右-&gt;中 Inorder traversal（中序遍历）：左-&gt;中-&gt;右 3.1 树的递归 递归是树最常用的思想之一，将左右节点看作左右子树，类似题型有： 获得树的总节点数和高度。 给定一个有序数组，创建一棵高度最小的二叉搜索树：面试题 04.02. 最小高度树 二叉树的镜像：剑指 Offer 27. 二叉树的镜像 3.2 已知遍历建树 题目通常会给出一个或两个前/中/后序遍历的字符串，根据字符串建立一棵树。 3.2.1 有#符号表示空节点 根据前序遍历或后序遍历的字符串建树即可。前序遍历的第一个节点就是根节点，后序遍历的最后一个节点就是根节点！ 如二叉树遍历（清华大学复试上机题），这是给定了一个前序遍历的字符串来建树。核心代码如下： TreeNode* buildTree(int&amp; pos) { char c = s[pos++]; if(c == '#') return NULL; TreeNode* cur = new TreeNode(c); cur-&gt;left = buildTree(pos); cur-&gt;right = buildTree(pos); return cur; } 后序遍历是从后往前遍历，先建立根节点，再建立右子树，后建立左子树。 3.2.2 没有#符号表示空节点 需要前序遍历+中序遍历或后序遍历+中序遍历的字符串。 对前/后序遍历的字符串，前序遍历从前往后遍历，或后序遍历从后往前遍历。每找到一个节点，就在中序遍历的序列里找到这个节点的位置，将中序遍历的序列分成左子树和右子树两个部分！ 如二叉树遍历（华中科技大学复试上机题），给定了前序遍历+中序遍历的字符串。核心代码如下： // s1是前序遍历的字符串，s2是中序遍历的字符串。 TreeNode* buildTree(string s1, string s2) { if(s1.length() == 0) return NULL; TreeNode* cur = new TreeNode(s1[0]); int pos = s2.find(s1[0]); cur-&gt;left = buildTree(s1.substr(1,pos), s2.substr(0,pos)); cur-&gt;right = buildTree(s1.substr(pos + 1), s2.substr(pos + 1)); return cur; } 后序遍历+中序遍历可相应改成： int pos = s2.find(s1[len - 1]); cur-&gt;left = buildTree(s1.substr(0, pos), s2.substr(0, pos)); cur-&gt;right = buildTree(s1.substr(pos , len – pos - 1), s2.substr(pos + 1)); 4. Binary Search Trees 定义：假设有一节点的值为 K ，则它的左子树中所有节点的值都 &lt; K，右子树中所有节点的值都 &gt;= K 。 搜索节点的平均时间复杂度：$\\theta(logn)$；最坏情况是：$\\theta(n)$，退化成了线性树；最好情况是：$\\theta(1)$，根节点就是需要找的节点。 这也是向二叉树插入或移除一个节点的时间复杂度情况。 插入和搜索操作较为简单，重点写写移除操作。 4.1 Remove操作设要删除元素e。 若 e &lt; root-&gt;val，在左子树里继续寻找。 若 e &gt; root-&gt;val，在右子树里继续寻找。 若 e = root-&gt;val： 若左子树为空，则该子树的根节点就是原根节点的右孩子；若右子树为空，则该子树的根节点就是原根节点的左孩子。 如果左右子树都不为空，则找到右子树中最小的节点（也可以是左子树中最大的节点），作为新的根节点。 5. Heaps and Priority Queues大顶堆：每个节点的值都&gt;=其孩子的值。小顶堆：每个节点的值都&lt;=其孩子的值。 堆类 isLeaf (int pos)：2pos &gt;= n，即pos &gt;= n / 2，此刻孩子的索引就超界了，故这个pos就是叶子节点。 parent (int pos)：如果pos是奇数，那么父节点就是 (pos - 1) / 2，因为是左孩子；如果是偶数，父节点是 pos / 2 - 1，由于除法向下取整，和奇数节点的父节点公式可以统一。 插入元素 Ө (logn)【以下都是大顶堆的实现】 堆就相当于完全二叉树，是Array-based的实现方法。 将元素插入到最后一个位置，如果值大于parent值，则与parent的位置互换。向上遍历，直到值不大于parent的值。 建堆 Ө (n) 若已知n个元素，要建立堆，如果调用n次插入操作，则时间复杂度为 Ө (nlogn)。有更简单的方法。 可以将左右子树都转换为堆，最后将整棵子树转换为堆。 具体来说，就是自底向上，对每个中间节点（包括根）【范围：$0 \\sim \\frac{n}{2} - 1$】，进行siftdown操作。siftdown操作就是自顶向下将树转换为堆的操作： j在每次循环中，指向最大的节点。 siftdown操作的时间复杂度是Ө (logn)。 总的时间复杂度计算：$\\sum_{i=1}^{logn}\\frac{(i-1)n}{2^i}=\\theta(n)$。i = 1，表示最底层。 移除元素 Ө (logn) 设要移除的节点位置为pos，将要移除的元素和最后一个元素交换，如果pos位置对应的节点大于父节点，则两者交换，pos位置指向父节点。 对pos位置的节点进行siftdown操作。当然，移除堆顶元素，直接siftdown 0号位置即可。 与有序链表的比较 5.1 top k 问题 堆有一个重要应用，就是求解top k问题。即找到数组中前k个最小/大的数。 C++中使用priority_queue，默认为大顶堆。求解数组中前k个最小/大的数，可以先插入数组前k个数，再把剩下的数插入，如果插入的数小于/大于堆顶元素，则弹出堆顶元素，将该数插入堆，否则不插入。这样能够保证是k个最小/大的数在堆中。 所以，求k个最小的数，用大顶堆，堆顶是这k个最小的数中最大的，也就是第k个最小数；求k个最大的数，用小顶堆（priority_queue&lt; int, vector&lt; int&gt;, greater&lt; int&gt;&gt;），堆顶是这k个最大的数中最小的，也就是第k个最大数。 相关题目：剑指 Offer 40. 最小的k个数，378 有序矩阵中第K小的元素。 自定义堆的比较器： struct temp { int index; double dis; temp(int i, double j) { index = i; dis = j; } bool operator&lt;(const temp&amp; a) const { return dis &lt; a.dis; //大顶堆 } }; priority_queue&lt;temp&gt; q; 如果是小顶堆： struct temp { ... bool operator&lt;(const temp&amp; a) const { return dis &gt; a.dis; //小顶堆 } } priority_queue&lt;temp&gt; q; 6. Huffman Coding Trees 给字母进行编码时，如果使用固定长度的编码，会造成空间浪费，可以给使用频率高的字母更短的编码，这是Huffman coding的思想。可以建立Huffman Coding Tree。 huffman tree的cost huffman tree基于大顶堆实现，也可以说是优先队列： 每次弹出两个节点，这两个节点的值一定是队列中最小的（值表示优先级）。让它们的值相加，得到新值。 创建新节点，放入优先队列。直到队列中只剩下一个节点，这就是huffman tree的根。 编码：可以用深度优先搜索得到字符对应的编码。从根开始，访问左孩子，编码字符串加’0’；访问右孩子，编码字符串加’1’。当访问到叶节点，记录叶节点表示的字符和对应的编码字符串，可用哈希表记录。当用户输入需要编码的字符串，直接查阅哈希表即可。 解码：编码过程中用哈希表记录了“字符-编码字符串”的对应关系，同时也可另用一个哈希表记录“编码字符串-字符”的对应关系。当用户输入需要解码的编码字符串，直接查阅该哈希表即可，不断对编码字符串分割子串来查找。","link":"/2021/04/30/ds-tree/"},{"title":"数值分析：第2章 代数插值与数值微分","text":"本章主要介绍代数插值与数值微分，代数插值指构造代数多项式$p_n(x)$来近似代替$f(x)$，数值微分涉及函数求导。 1. 线性插值与二次插值1.1 线性插值 给定两点$(x_0,y_0),(x_1,y_1)$，构造线性插值函数$p_1(x)$，$x_0,x_1$是插值节点。 1.1.1 线性插值函数的Lagrange形式 由直线方程的两点式可以直接得到： p_1(x)=\\frac{x-x_1}{x_0-x_1}y_0+\\frac{x-x_0}{x_1-x_0}y_1 记$l_0(x)=\\frac{x-x_1}{x_0-x_1},l_1(x)=\\frac{x-x_0}{x_1-x_0}$，则$l_0(x),l_1(x)$称为一次插值基函数。 还有另外的方法得到Lagrange形式：求$l_0(x)$时，由于$l_0(x_0)=1,l_0(x_1)=0$，则$l_0(x)$有因子$x-x_1$。又因为线性插值函数最高项次数为1，所以$l_0(x)$只能有常数项，设为$A$，则$A(x_0-x_1)=1$，求得$A=\\frac{1}{x_0-x_1}$，即$l_0(x)=\\frac{x-x_1}{x_0-x_1}$。同理，求得$l_1(x)=\\frac{x-x_0}{x_1-x_0}$。 1.1.2 线性插值函数的Newton形式 若记$f(x_i,x_j)=\\frac{f(x_i)-f(x_j)}{x_i-x_j}$，则称其为函数$f(x)$在$x_i,x_j$处的一阶差商，其中$x_i,x_j$互异。 由直线方程的点斜式可以直接得到： p_1(x)=f(x_0)+f(x_0,x_1)(x-x_0) 还可以假设：$p_1(x)=A+B(x-x_0)$。由于$p_1(x_0)=f(x_0)$，故$A=f(x_0)$。由于$p_1(x_1)=f(x_0)+B(x_1-x_0)=f(x_1)$，故$B=f(x_0,x_1)$。 1.1.3 线性插值余项（误差） 设$p_1(x)$是过$(x_0,y_0)$和$(x_1,y_1)$两点的线性插值函数，$[a, b]$是包含$[x_0, x_1]$的任一区间，并设$f(x)\\in C^1[a, b]$(一阶导函数存在且连续)，$f^{‘’}(x)$在$[a, b]$上存在(二阶导函数存在)，则对任意给定的$x \\in [a, b]$，总存在一点$\\xi$，使得： R(x)=f(x)-p_1(x)=\\frac{f^{''}(\\xi)}{2!}(x-x_0)(x-x_1) 其中，$\\xi$依赖于$x$。 证明： 设$\\phi(t)=f(t)-p_1(t)-k(t-x_0)(t-x_1)$，易得$\\phi(x_0)=\\phi(x_1)=0$，目标是使$\\phi(x)=0$ 由洛尔定理（若$f(t)$在$[a, b]$上连续，在$(a, b)$上可导，且有$f(a)=f(b)=0$，则在$(a, b)$内至少有一点$c$，使$f^{‘}(c)=0$），由于$\\phi(t)$有三个零点$x_0,x_1,x$，构成两个子区间，则$\\phi^{‘}(t)$在每个子区间上至少有一个零点，可记为$\\xi_1,\\xi_2$，有$\\phi^{‘}(\\xi_1)=\\phi^{‘}(\\xi_2)=0$。同理，$\\phi^{‘’}(t)$在区间$[\\xi_1, \\xi_2]$上至少有一个零点，记为$\\xi$，即有$\\phi^{‘’}(\\xi)=0$。 对$\\phi(t)$求两阶导，$\\phi^{‘’}(t)=f^{‘’}(t)-p_1^{‘’}(t)-2!k$。由于$p_1(t)$是一次多项式，则$p_1^{‘’}(t)=0$，将$\\xi$代入，$f^{‘’}(\\xi)=2k$，得到$k=\\frac{f^{‘’}(\\xi)}{2}$。 对于线性插值余项，可以进一步证明$|R(x)|\\leq \\frac{(x_1-x_0)^2}{8}max_{x_0\\leq x\\leq x_1}|f^{‘’}(x)|$。 证： $|R(x)|=|\\frac{f^{‘’}(\\xi)}{2!}(x-x_0)(x-x_1)|\\leq max_{x_0\\leq x\\leq x_1}|\\frac{f^{‘’}(x)}{2!}(x-x_0)(x-x_1)|$ 由于$(x_1-x_0)^2 = (x_1 - x + x - x_0)^2 = 2(x_1-x)(x-x_0) + (x_1-x_0)^2 + (x - x_0)^2 \\geq 4(x_1-x)(x-x_0)$ 所以，$|R(x)|\\leq \\frac{(x_1-x_0)^2}{8}max_{x_0\\leq x\\leq x_1}|f^{‘’}(x)|$ 1.2 二次插值 给定三点$(x_0,y_0),(x_1,y_1),(x_2,y_2)$，构造二次插值函数$p_2(x)$，$x_0,x_1,x_2$是插值节点。 1.2.1 二次插值函数的Lagrange形式 类似线性插值函数的Lagrange形式的求法，得到： p_2(x)=\\frac{(x-x_1)(x-x_2)}{(x_0-x_1)(x_0-x_2)}y_0+\\frac{(x-x_0)(x-x_2)}{(x_1-x_0)(x_1-x_2)}y_1+\\frac{(x-x_0)(x-x_1)}{(x_2-x_0)(x_2-x_1)}y_2 1.2.2 二次插值函数的Newton形式 定义二阶差商：$f(x_i,x_j,x_k)=\\frac{f(x_i,x_j)-f(x_j,x_k)}{x_i-x_k}$。 类似线性插值函数的Newton形式的求法，得到： p_2(x)=f(x_0)+f(x_0,x_1)(x-x_0)+f(x_0,x_1,x_2)(x-x_0)(x-x_1) 1.2.3 二次插值余项 设$p_2(x)$是过$(x_0,y_0),(x_1,y_1),(x_2,y_2)$三点的二次插值函数，$[a, b]$是包含$x_0, x_1,x_2$的任一区间，并设$f(x)\\in C^2[a, b]$(二阶导函数存在且连续)，$f^{‘’’}(x)$在$[a, b]$上存在(二阶导函数存在)，则对任意给定的$x \\in [a, b]$，总存在一点$\\xi$，使得： R(x)=f(x)-p_2(x)=\\frac{f^{'''}(\\xi)}{3!}(x-x_0)(x-x_1)(x-x_2) 2. n次插值 给定n + 1个点。 2.1 n次插值函数的Lagrange形式 p_n(x)=\\sum_{i=0}^n\\frac{(x-x_0)\\cdots(x-x_{i-1})(x-x_{i+1})\\cdots(x-x_n)}{(x_i-x_0)\\cdots(x_i-x_{i-1})(x_i-x_{i+1})\\cdots(x_i-x_n)}y_i2.2 n次插值函数的Newton形式 定义n阶差商：$f(x_{0},\\cdots,x_{n})=\\frac{f(x_{0},x_{1},\\cdots,x_{n-1})-f(x_1,x_2,\\cdots,x_{n})}{x_0-x_n}$ p_n(x)=f(x_0)+f(x_0,x_1)(x-x_0)+f(x_0,x_1,x_2)(x-x_0)(x-x_1)+\\cdots+f(x_{0},\\cdots,x_{n})(x-x_0)\\cdots(x-x_{n-1}) 可以构建差商表来求，例子： 2.3 n次插值余项 R(x)=f(x)-p_n(x)=\\frac{f^{(n+1)}(\\xi)}{(n+1)!}(x-x_0)(x-x_1)\\cdots(x-x_n)3. 分段线性插值 高次插值可能会发生Runge（龙格）现象：越靠近端点，误差越大。 分段线性插值：将插值区间分为若干段，每段上使用线性插值。 构造$p(x)$： p(x)=\\begin{cases} \\frac{x-x_1}{x_0-x_1}y_0+\\frac{x-x_0}{x_1-x_0}y_1(x_0\\leq x \\leq x_1) \\\\ \\frac{x-x_2}{x_1-x_2}y_1+\\frac{x-x_1}{x_2-x_1}y_2(x_1< x \\leq x_2)\\\\ \\vdots \\\\\\\\ \\frac{x-x_n}{x_{n-1}-x_n}y_{n-1}+\\frac{x-x_{n-1}}{x_n-x_{n-1}}y_n(x_{n-1} < x \\leq x_n) \\\\ \\end{cases} 定理：分段线性插值余项$|R(x)| = |f(x)-p(x)| \\leq \\frac{h^2}{8}M$，其中$h=max_{0\\leq i\\leq n-1}|x_{i+1}-x_i|,M=max_{x_0\\leq x\\leq x_1}|f^{‘’}(x)|$ 4. Hermite插值 在某些实际问题中，为了保证插值函数更好地逼近被插值函数，不仅要求插值函数在插值节点的值与被插值函数在插值节点上的值相等，而且还要求插值函数在插值节点的导数值与被插值函数在插值节点的导数值相等。这需要用到Hermite插值。 4.1 三次Hermite插值 构造不超过三次的代数多项式$H(x)=y_0h_0(x)+y_1h_1(x)+m_0H_0(x)+m_1H_1(x)$ ，满足$H(x_0)=y_0,H(x_1)=y_1,H^{‘}(x_0)=m_0,H^{‘}(x_1)=m_1$，故： 构造$h_0(x)$，它必有因子$(x-x_1)^2$，因为$h_0(x_1)=h_0^{‘}(x_1)=0$。由于$h_0(x)$是不超过三次的代数多项式，从而$h_0(x)$可表示为$(a+b(x-x_0))(\\frac{x-x_1}{x_0-x_1})^2$。由$h_0(x_0)=1,h_0^{‘}(x_0)=0$，可求得$a, b$。$h_1(x)$同理可得。 构造$H_0(x)$，它必有因子$(x-x_0)(x-x_1)^2$，已经是三次的代数多项式，从而$H_0(x)$可表示为$c(x-x_0)(\\frac{x-x_1}{x_0-x_1})^2$。由$H_0^{‘}(x_0)=1$，可求得c。 因此， \\begin{aligned} H(x)&=y_0h_0(x)+y_1h_1(x)+m_0H_0(x)+m_1H_1(x)\\\\ &=(1+2\\frac{x-x_0}{x_1-x_0})(\\frac{x-x_1}{x_0-x_1})^2y_0+(1+2\\frac{x-x_1}{x_0-x_1})(\\frac{x-x_0}{x_1-x_0})^2y_1\\\\&+(x-x_0)(\\frac{x-x_1}{x_0-x_1})^2m_0+(x-x_1)(\\frac{x-x_0}{x_1-x_0})^2m_1 \\end{aligned} 定理：设$H(x)$是过$f(x)$的三次Hermite插值函数，$[a, b]$是包含$[x_0, x_1]$的任一区间，并设$f(x)\\in C^3[a, b]$（三阶导函数存在且连续），$f^{(4)}(x)$在$[a, b]$上存在(四阶导函数存在)，则对任意给定的$x \\in [a, b]$，总存在一点$\\xi$，使得： R(x)=f(x)-H(x)=\\frac{f^{(4)}(\\xi)}{4!}(x-x_0)^2(x-x_1)^2 其中，$\\xi$依赖于$x$。 证明也是利用了洛尔定理。 4.2 2n + 1次Hermite插值 构造不超过2n + 1次的代数多项式$H(x)$，满足$H(x_i)=y_i,H^{‘}(x_i)=m_i(i=0,1,\\cdots,n)$： \\begin{aligned} H(x) &= \\sum_{i=0}^n(y_ih_i(x)+m_iH_i(x)) \\\\ &= \\sum_{i=0}^n\\{y_i[1-2 (x-x_i)(\\sum_{j=1, j\\neq i}^n\\frac{1}{x_i-x_j})]l_i^2(x)+m_i(x-x_i)l_i^2(x) \\} \\end{aligned} 其中，$l_i(x)=\\frac{(x-x_0)\\cdots(x-x_{i-1})(x-x_{i+1})\\cdots(x-x_n)}{(x_i-x_0)\\cdots(x_i-x_{i-1})(x_i-x_{i+1})\\cdots(x_i-x_n)}$ 定理：设$H(x)$是过$f(x)$的2n + 1次Hermite插值函数，$[a, b]$是包含$x_0\\sim x_n$的任一区间，并设$f(x)\\in C^{2n + 1}[a, b]$（2n + 1阶导函数存在且连续），$f^{(2n + 2)}(x)$在$[a, b]$上存在(2n + 2阶导函数存在)，则对任意给定的$x \\in [a, b]$，总存在一点$\\xi$，使得： R(x)=f(x)-H(x)=\\frac{f^{(2n + 2)}(\\xi)}{(2n + 2)!}(x-x_0)^2(x-x_1)^2\\cdots(x-x_n)^2 其中，$\\xi$依赖于$x$。 5. 分段三次Hermite插值 第3节介绍的分段线性插值在节点处的导数是不连续的，可以构造分段三次Hermite插值解决这个问题。 构造$q(x)$，满足： 在任意分段上为三次代数多项式。 $q(x_i)=y_i(i=0,1,\\cdots,n)$。 $q(x)\\in C^1[a, b]$，其中$a=x_0,b=x_n$。 定理：$|R(x)| = |f(x)-q(x)| \\leq \\frac{h^4}{384}M$，其中$h=max_{0\\leq i\\leq n-1}|x_{i+1}-x_i|,M=max_{x_0\\leq x\\leq x_1}|f^{(4)}(x)|$。和分段线性插值余项进行类比。 可用三次Hermite插值余项和基本不等式的性质证明。 6. 三次样条插值 小结一下前面提到的插值方法：为了减少高次插值的误差，使用分段插值，但分段插值在节点处不够光滑，比如分段线性插值只保证在节点处连续，导数在节点处就不连续了；分段三次Hermite插值，也只有一阶导数连续，二阶导数就不连续了。 往往实际问题中有时很难给出导数条件，为了提高插值函数在节点处的光滑度，同时减少对节点处导数的要求，可以考虑样条插值。 构造三次样条插值函数$s(x)$，是一个分段代数多项式，在节点上连续，其一阶和二阶导数在节点上也连续： 在任意分段上为不超过三次的代数多项式。 $s(x_i)=y_i(i=0,1,\\cdots,n)$ $s(x)\\in C^2[a, b]$，其中$a=x_0,b=x_n$。 $s(x)$在每个分段上满足三次Hermite插值的条件，故可以按照三次Hermite插值，在区间$[x_i, x_{i+1}]$写出公式。不同的是，$m_i$是未知的。 三次样条插值算法： 第一种边界条件是指已知被插值函数$y=f(x)$在两个端点$x_0,x_n$处的一阶导数值。 第二种边界条件是指已知被插值函数$y=f(x)$在两个端点$x_0,x_n$处的二阶导数值为0。 三次样条插值例子： 定理：设$s(x)$是满足第一种边界条件的三次样条插值函数，$[a, b]$是包含$x_0\\sim x_n$的任一区间，并设$f(x)\\in C^{4}[a, b]$，则对任意给定的$x \\in [a, b]$，总存在一点$\\xi$，有误差估计式： max_{a\\leq x \\leq b}|f^{(k)}(x)-s^{(k)}(x)|\\leq c_k max_{a\\leq x \\leq b}|f^{(4)}(x)|h^{(4-k)},(k=0,1,2) 其中，$c_0=\\frac{5}{384},c_1=\\frac{1}{24},c_2=\\frac{1}{8},h=max_{0\\leq i\\leq n- 1}|x_{i+1}-x_i|$。 7. 数值微分 若函数$f(x)$不能直接求导，可以用插值函数$p(x)$近似$f(x)$，再求$p(x)$的导数。 7.1 使用n次插值函数求导数 已知误差估计式：$R(x)=f(x)-p_n(x)=\\frac{f^{(n+1)}(\\xi)}{(n+1)!}w(x)$，对误差估计式求导，得到：$R^{‘}(x_k)=f^{‘}(x_k)-p_n^{‘}(x_k)=\\frac{f^{(n+1)}(\\xi)}{(n+1)!}w^{‘}(x_k)$，其中$w(x)=(x-x_0)(x-x_1)\\cdots(x-x_n)$。 7.2 使用三次样条插值函数求导数","link":"/2021/05/04/NA-ch2/"},{"title":"数值分析：第3章 数据拟合","text":"代数插值函数需要经过给定的数据，必然保留原来数据的一切误差。使用数据拟合法，可以避免这种情况的发生。这也是本章要介绍的内容。 1. 单变量数据拟合及最小二乘法 单变量，相当于一个样本只有一个特征。 以偏差平方和最小为原则选择近似函数的方法称为最小二乘法。偏差：$\\sum_{i=1}^{n}\\delta_i^2=\\sum_{i=1}^{n}(f(x_i)-F(x_i))^2$。 单变量数据拟合函数可以选取$F(x)=a+bx$。偏差平方和对$a,b$求导，可以整理得到求$a,b$的表达式，也是正规方程组。 2. 多变量数据拟合 多变量，相当于一个样本有多个特征。 多变量数据拟合的最小二乘解： 3. 非线性数据线性化 数据之间的关系可能是非线性的，可以通过变换变成线性的。 常用非线性拟合函数线性化的方法： 4. 正交多项式拟合 多变量数据拟合法中，正规方程组有唯一解，但当多项式次数较高时，正规方程组就会变成病态方程组，即系数矩阵和常数项发生微小变化时，就会引起解的很大变化。 为了克服这个缺点，将多项式拟合函数取为$y^\\star=\\sum_{k=0}^ma_kp_k(x)$，$p_k(x)$是正交多项式簇。","link":"/2021/05/06/NA-ch3/"},{"title":"数值分析：第4章 数值积分","text":"本章介绍定积分的一些近似方法，因为有时被积函数的积分不能用求原函数的方法获得。 1. 梯形求积公式、Simpson求积公式、Newton-Cotes求积公式根据代数插值法，对于任一被积函数$f(x)$，都可以构造一个插值多项式$p(x)$近似代替它，对$p(x)$求定积分。 1.1 梯形求积公式 利用线性插值公式，求积分得$\\int^b_{a}f(x)dx\\approx \\int^b_{a}p_1(x)dx = \\frac{b-a}{2}[f(a) + f(b)]$。 1.2 Simpson求积公式 利用二次插值公式，把区间[a, b]二等分，将a，b，(a+b)/2作为插值节点，求积分得$\\int^b_{a}f(x)dx\\approx \\int^b_{a}p_2(x)dx = \\frac{b-a}{6}[f(a) + 4f(\\frac{a+b}{2}) + f(b)]$。 1.3 Newton-Cotes求积公式 利用n次插值公式，把区间[a, b] n等分，求积分得$\\int^b_{a}f(x)dx\\approx \\int^b_{a}p_n(x)dx = \\sum_{i=0}^n(b-a)c_i^{(n)}f(x_i)$。 $c_i^{(n)}$是Newton-Cotes系数，在计算时可以查Newton-Cotes系数表： 梯形求积公式是n = 1的情况，Simpson求积公式是n = 2的情况。 1.4 例题 2. 求积公式的代数精确度 定义：对一般求积近似公式，如果当$f(x)$为任意一个次数不高于n次的代数多项式时，积分近似公式$\\int^b_{a}f(x)dx\\approx \\sum_{k=0}^mA_kf(x_k)$精确成立，而当$f(x)$为n + 1次时不精确成立，则称该积分近似公式有n次代数精确度。 定理1：梯形求积公式具有一次代数精确度。 根据插值公式的误差证明，并假设$f(x)=x^2$，发现不满足要求。 也就是说，被积函数为不超过一次代数多项式时，用梯形求积公式求积分是没有误差的。 定理2：Newton-Cotes求积公式至少具有n次代数精确度，n为偶数时至少为n + 1次。 定理3：Simpson求积公式的代数精确度为3。 由定理2可知，并假设$f(x)=x^4$，发现不满足要求。 3. 梯形求积公式和Simpson求积公式的误差估计 定理4：若$f(x)\\in C^2[a, b]$，则梯形求积公式有误差估计： \\int_a^bf(x)dx-\\frac{b-a}{2}[f(a) + f(b)]=-\\frac{(b-a)^3}{12}f^{''}(\\eta) 其中，$a\\leq \\eta \\leq b$。 利用插值的误差余项和积分中值定理的第一定理证明。积分中值定理的第一定理：如果函数$f(x),g(x)$在闭区间$[a, b]$上连续，且$g(x)$在$[a, b]$上不变号，则在积分区间$[a, b]$上至少存在一个点$\\epsilon$，使$\\int_a^{b}f(x)g(x)dx=f(\\epsilon)\\int_a^{b}g(x)dx$成立。 定理5：若$f(x)\\in C^4[a, b]$，则Simpson求积公式有误差估计： \\int_a^bf(x)dx-\\frac{b-a}{6}[f(a) + 4f(\\frac{a+b}{2}) + f(b)]=-\\frac{(b-a)^5}{2880}f^{(4)}(\\eta) 其中，$a\\leq \\eta \\leq b$。 如果直接利用插值的误差余项和积分中值定理的第一定理证明，$(x-a)(x-\\frac{a+b}{2})(x-b)$是在区间内是变号的，不满足积分中值定理的第一定理的条件。 要先构建一个三次插值多项式$p_3(x)$，而不是二次插值多项式。因为Simpson求积公式的代数精确度是3，所以$\\int_a^bp_3(x)dx=\\frac{b-a}{6}[f(a) + 4f(\\frac{a+b}{2}) + f(b)]$。 $\\int_a^bf(x)dx-\\frac{b-a}{6}[f(a) + 4f(\\frac{a+b}{2}) + f(b)]$得到的值是三次多项式插值的误差余项$\\frac{1}{4!}\\int_a^b(x-a)(x-\\frac{a+b}{2})^2(x-b)dx$，可以用积分中值定理的第一定理了。 4. 复化求积公式梯形求积公式和Simpson求积公式误差较大，Newton-Cotes求积公式的收敛性对某些被积函数$f(x)$得不到保证，不能通过盲目加大$n$来达到提高精度的目的。为了提高积分近似公式的精度，可以使用复化公式。 4.1 复化梯形求积公式及其误差估计 把积分区间$[a, b]$ n等分，对每个子区间使用梯形求积公式。得： \\int_a^bf(x)dx=\\sum_{k=0}^{n-1}\\frac{x_{k+1}-x_k}{2}[f(x_k) + f(x_{k+1})] = \\frac{h}{2}[f(a) + 2\\sum_{k=1}^{n-1}f(a+kh) + f(b)] 注：$h=x_{k+1}-x_k$。除头尾外，其他点的函数值都被加了两次。 这就是复化梯形求积公式。 定理6：若$f(x)\\in C^2[a, b]$，则对复化梯形求积公式$T_n$有误差估计： \\int_a^bf(x)dx-T_n=-\\frac{(b-a)}{12}h^2f^{''}(\\eta) 其中，$h=\\frac{b-a}{n},a \\leq \\eta \\leq b$。 利用定理4 梯形求积公式的误差估计，和连续函数的性质可以证明。连续函数的性质：$\\frac{1}{n}\\sum_{k=0}^{n-1}f^{‘’}(\\eta_k)=f^{‘’}(\\eta)$。 例题（通过误差估计式判断n取多大可以满足要求）： 定理7：积分近似值$T_n$和$T_{2n}$有如下关系：$T_{2n}=\\frac{1}{2}[T_n + h\\sum_{k=1}^nf(a+(2k-1)\\frac{b-a}{2n})]$ 把积分区间$[a, b]$ 2n等分，即在n等分的基础上，对每个子区间再平分一次。 这一定理被用于后文要介绍的自动选取步长梯形法。 4.2 复化Simpson求积公式及其误差估计 把积分区间$[a, b]$ n等分，对每个子区间$[x_{2k}, x_{2k+2}]$使用Simpson求积公式。由于n是偶数，不妨设$n=2m$，得： \\int_a^bf(x)dx=\\sum_{k=1}^{m}\\frac{x_{2k}-x_{2k-2}}{6}[f(x_{2k-2}) + 4f(x_{2k-1}) + f(x_{2k}))] = \\frac{h}{3}[f(a) + 4\\sum_{k=1}^{m}f(x_{2k-1}) + 2\\sum_{k=1}^{m-1}f(x_{2k}) + f(b)] 注：$x_{2k}-x_{2k-2}=2h$。除头尾外，奇数点对应的函数值被加了四次，偶数点对应的函数值被加了两次。 定理8：若$f(x)\\in C^4[a, b]$，则对复化Simpson求积公式$S_n$有误差估计： \\int_a^bf(x)dx-S_n=-\\frac{(b-a)}{2880}(2h)^4f^{(4)}(\\eta) 其中，$a\\leq \\eta \\leq b$。 和复化梯形求积公式的误差估计的证明类似。注意，$m=n/2=(b-a)/2h$。 5. 自动选取步长梯形法 在使用复化梯形求积公式之前必须选定$n$，需要先验知识。可以使用自动选取步长梯形法选取n，使n满足精度要求。 反复使用定理7的公式$T_{2n}=\\frac{1}{2}[T_n + h\\sum_{k=1}^nf(a+(2k-1)\\frac{b-a}{2n})]$，得到$1/3(T_{2n}-T_n)\\approx \\int_a^bf(x)dx-T_{2n}$。 由此得到自动选取步长梯形法的计算步骤： 先用梯形求积公式（不是复化）计算出积分的第一次近似值$T_1$。 把区间2等分，用定理7的公式求出积分的第二次近似值$T_2$。判别$|T_2-T_1|&lt;3\\epsilon$是否成立（$\\epsilon$是题目的精度要求），若成立，则计算停止，否则继续计算。 把区间4等分，用定理7的公式求出积分的第三次近似值$T_4$。继续判断。 直到$T_{2n}-T_n&lt;3\\epsilon$为止，$T_{2n}$即为满足精度要求的近似值。 6. 数值方法中的加速收敛技巧——Richardson外推算法 假设有一个量$F^\\star$，现用一个以步长h为变量的函数$F_0(h)$近似代替它，则误差估计为$F^\\star-F_0(h)=a_1h^{p_1}+a_2h^{p_2}+\\cdots$。其中$0&lt; p_1 &lt; p_2 &lt; \\cdots$。可以看到，当$h$适当小时，第一项$h^{p_1}$是最主要的，称为误差的阶。 可以构造一个新函数$F_1(h)$，使$F^\\star$和这个函数的误差的阶的幂比$F^\\star$和$F_0(h)$的误差的阶的幂更高一些。 依次类推，可以构造出一系列的函数，$F^\\star$和这些函数的误差的绝对值一个比一个小。这就是Richardson外推算法。 一般形式：$F_m(h)=\\frac{F_{m-1}(qh)-q^{p_m}F_{m-1}(h)}{1-q^{p_m}}$。 7. Romberg求积法 可以证明复化梯形求积公式的另一个误差估计式：$\\int_a^bf(x)dx-T_n=a_2h^2+a_4h^4+a_6h^6+\\cdots$，其中$h=\\frac{b-a}{n}$。复化梯形求积公式和积分准确值的误差的阶为$h^2$。 用Richardson外推算法加速时，是沿着这些阶加速的。 Romberg求积法的计算过程： 一行一行按顺序求。 用复化梯形求积公式求$T_0(h)$。 利用定理7：$T_{2n}=\\frac{1}{2}[T_n + h\\sum_{k=1}^nf(a+(2k-1)\\frac{b-a}{2n})]$，求$T_0(\\frac{h}{2^k}),k=1,2,3,\\cdots;h=\\frac{b-a}{n}$。 利用公式$T_m(\\frac{h}{2^k})=\\frac{4^mT_{m-1}(\\frac{h}{2^{k+1}})-T_{m-1}(\\frac{h}{2^{k}})}{4^m-1},m=1,2,3,\\cdots;k=0,1,2,\\cdots$。 例题： 8. Gauss求积公式","link":"/2021/05/09/NA-ch4/"},{"title":"数值分析：第6章 解线性代数方程组的迭代法","text":"上一章解线性代数方程组的直接法，时间复杂度都是$O(n^3)$，当$n$较大时，计算量比较大。而迭代法的基本思想是构造一个向量序列$X^{(n)}$，使其能够收敛到某个极限向量$X^\\star$，即为要求的方程组的准确解。本章主要介绍常用的迭代法和迭代收敛的条件。 1. 几种常见的迭代格式1.1 简单迭代法（Jacobi迭代）（由于数学公式较多，直接贴书上的图） 迭代格式：$X^{(k+1)}=BX^{(k)}+g$。$B=D^{-1}(D-A)$为Jacobi迭代的迭代矩阵。 1.2 Seidel迭代法 将Jacobi迭代的迭代矩阵$B$分解为$L+U$，$L$是下三角矩阵，$U$是上三角矩阵。 迭代格式：$X^{(k+1)}=LX^{(k+1)}+UX^{(k)}+g$。注意到计算第$i$个分量$x_i^{(k+1)}$时，前面的$i-1$个分量用的是本次迭代新算出的$x_1^{(k+1)},\\cdots,x_{i-1}^{(k+1)}$，有可能提高收敛速度。 标准的迭代格式：可将上式右边第一项移至左边，得$X^{(k+1)}=(I-L)^{-1}UX^{(k)}+(I-L)^{-1}g$，$(I-L)^{-1}U$为Seidel迭代法的迭代矩阵。 1.3 松弛法（SOR迭代） 松弛法可看作是Seidel迭代法的加速，Seidel迭代法是松弛法的特例。由Seidel迭代法得到$\\Delta X=X^{(k+1)}-X^{(k)}=LX^{(k+1)}+UX^{(k)}+g-X^{(k)}$。于是$X^{(k+1)}=X^{(k)}+\\Delta X$。 迭代格式：为修正项加上一个参数$w$，就得到松弛法的迭代格式。即$X^{(k+1)}=X^{(k)}+w\\Delta X=(1-w)X^{(k)}+w(LX^{(k+1)}+UX^{(k)}+g)$。$w$为松弛因子，大于1为超松弛，小于1为低松弛，等于1为Seidel迭代法。 标准的迭代格式：$X^{(k+1)}=(1-wL)^{-1}[(1 - w)I+wU]X^{(k)}+w(I-wL)^{-1}g$。$(1-wL)^{-1}[(1 - w)I+wU]$是迭代矩阵。 2. 迭代法收敛性理论 定理1：对任何初始向量$X^{0}$和常数项$f$，由迭代格式$X^{(k+1)}=MX^{(k)}+f$，产生的向量序列$\\{X^{(k)}\\}$收敛的充要条件是迭代矩阵$M$的谱半径$\\rho(M)&lt;1$。 这里的$M$是迭代矩阵，故给定的迭代格式要先化为标准格式。 谱半径是矩阵最大特征值的绝对值。 定理2：若迭代矩阵$M$的范数$||M||=q&lt;1$，则迭代格式$X^{(k+1)}=MX^{(k)}+f$对任何初始向量$X^{0}$一定收敛，且$||X^{(k)}-X^\\star||\\leq \\frac{q}{1-q}||X^{(k)}-X^{(k-1)}||\\leq \\frac{q^k}{1-q}||X^{(1)}-X^{(0)}||$。 由于谱半径$\\rho(M)$往往难以计算，但由于谱半径不大于矩阵范数，所以可以用$||M||$来估计。但定理2只是充分条件。 2.1 三种迭代法迭代矩阵的谱半径与系数矩阵的关系2.1.1 Jacobi迭代 $B=D^{-1}(D-A)$为Jacobi迭代的迭代矩阵，由$|\\lambda I-B|=0$，得 \\left| \\begin{matrix} \\lambda a_{11} &a_{12} &\\cdots &a_{1n} \\\\ a_{21} &\\lambda a_{22} &\\cdots &a_{2n} \\\\ \\vdots & \\vdots & &\\vdots \\\\ a_{n1} &a_{n2} &\\cdots &\\lambda a_{nn} \\end{matrix} \\right| = 0 定理3：Jacobi迭代收敛的充要条件是上述行列式的所有根$\\lambda_i$的绝对值（复数理解为模）小于1。因为谱半径是最大特征值的模，需要小于1。 2.1.2 Seidel迭代 $(I-L)^{-1}U$为Seidel迭代法的迭代矩阵，由$|\\lambda I-B|=0$，得 \\left| \\begin{matrix} \\lambda a_{11} &a_{12} &\\cdots &a_{1n} \\\\ \\lambda a_{21} &\\lambda a_{22} &\\cdots &a_{2n} \\\\ \\vdots & \\vdots & &\\vdots \\\\ \\lambda a_{n1} &\\lambda a_{n2} &\\cdots &\\lambda a_{nn} \\end{matrix} \\right| = 0 定理4：Seidel迭代收敛的充要条件是上述行列式的所有根$\\lambda_i$的绝对值（复数理解为模）小于1。 2.1.3 松弛法（SOR迭代） $(1-wL)^{-1}[(1 - w)I+wU]$是松弛法的迭代矩阵，由$|\\lambda I-B|=0$，得 \\left| \\begin{matrix} (\\lambda+w-1) a_{11} &w a_{12} &\\cdots &wa_{1n} \\\\ w\\lambda a_{21} &(\\lambda+w-1) a_{22} &\\cdots &wa_{2n} \\\\ \\vdots & \\vdots & &\\vdots \\\\ w\\lambda a_{n1} &w\\lambda a_{n2} &\\cdots &(\\lambda+w-1) a_{nn} \\end{matrix} \\right| = 0 注：下三角和上三角是有细微区别的。 定理5：松弛法收敛的充要条件是上述行列式的所有根$\\lambda_i$的绝对值（复数理解为模）小于1。 2.1.4 特别的判定方法 若$A$是强对角占优，或弱对角占优不可约，则：Jacobi迭代、Seidel迭代一定收敛；若松弛因子$w$满足$0&lt; w \\leq 1$，则松弛迭代一定收敛。 不可约的定义：如果矩阵$A$不能通过行、列变换变成$\\left[\\begin{matrix} A_{11} &amp; A_{12}\\\\ 0 &amp; A_{22}\\end{matrix}\\right ]$，其中$A_{11},A_{12}$为方阵，则称$A$不可约。不可约的充要条件是$A$矩阵对应的邻接图是强连通图。 强对角和弱对角占优：如果矩阵$A$满足$|a_{ii}|\\geq \\sum_{i \\neq j}|a_{ij}|$，且至少有一个$i$值使之严格成立，则称矩阵$A$具有对角优势（又称为弱对角占优）。如果所有$i$值都使之严格成立，则$A$具有强对角占优。若$A$强对角占优，或弱对角占优不可约，则$det(A)\\neq 0$。 松弛法收敛的必要条件是$0 &lt; w &lt; 2$。 若矩阵$A$对称且对角线元素均为正实数，则当$0 &lt; w &lt; 2$时，松弛法收敛的充要条件是$A$正定。 设$A$为分块三对角阵，且$a_{ii}\\neq 0$。Jacobi迭代的迭代矩阵$B$的特征值为实值，且$0 &lt; \\rho(B) &lt; 1$。则当$0 &lt; w &lt; 2$时，SOR法迭代收敛，最优松弛因子（使收敛最快的因子）$W_{opt}=\\frac{2}{1+\\sqrt(1-\\rho^2(B))}$。","link":"/2021/05/09/NA-ch6/"},{"title":"数值分析：第5章 解线性代数方程组的直接法","text":"直接法是经过有限步计算后求得方程组准确解的方法，本章研究的对象是n阶线性代数方程组。 1. 高斯消去法1.1 顺序高斯消去法 通过初等变换，将方程组$Ax=b$，转换为一个等价的三角形方程组： 这个过程被称为消元。逐个求出$x_n,x_{n-1},\\cdots,x_1$，这个过程被称为回代。 顺序高斯消去法求解算法：先消元后回代。 1.2 列主元高斯消去法 在顺序高斯消去法的消元过程中，若出现$a_{kk}^{(k)}=0$，则消元无法进行。接近0，也会使舍入误差扩散。因此提出了主元高斯消去法。 列主元高斯消去法求解算法： 每次消元前在当前列中的当前行之下选择一个绝对值最大的数作为主元。通过行变换将主元所在行和当前行交换。 1.3 全主元高斯消去法 与列主元高斯消去法类似，只不过选取主元的范围是整个系数矩阵。既有行变换，也有列变换。可以将舍入误差控制在最小的范围，但是花费时间多。 2. LU分解法 由顺序高斯消去法可以看到，只要方程$AX=b$的系数矩阵$A$的所有顺序主子式不为零，则$A$一定可以分解成$A=LU$。其中， L=\\left[ \\begin{matrix} 1 & & &\\\\ l_{21} &1 && \\\\ \\vdots & \\vdots &\\ddots & \\\\ l_{n1} &l_{n2} &\\cdots &1 \\end{matrix} \\right],U= \\left[ \\begin{matrix} u_{11} &u_{12} &\\cdots &u_{1n} \\\\ &u_{22} &\\cdots &u_{2n} \\\\ & & \\ddots&\\vdots \\\\ & & &u_{nn} \\end{matrix} \\right] 定理1：如果系数矩阵$A$的所有顺序主子式不为零，则$A$有惟一的$LU$分解。 2.1 直接LU分解法线性方程组$AX=b$。 求解$LU=A$，得到$L,U$。 先求$U$的第 i 行，再求$L$的第 i 列。可以说，$U$的第 i 行是由$L$的第 i 行辅助求得的，$L$的第 i 列是由$U$的第 i 列辅助求得的。 $U$的第一行就是$A$的第一行。 求解$LY=b$，得到$Y$。 求解$Y=UX$，得到$X$。 2.2 列主元LU分解法 直接进行LU分解时，$u_{kk}$可能为0，或绝对值接近0，引起舍入误差的积累。 3. 对称正定矩阵的平方根法和分解法 定理2：设$A$是对称正定矩阵，则$A$有如下分解：$A=LDL^T$。其中，$L$是单位下三角阵，$D$为对角阵，且这种分解是唯一的。 用$L$有唯一的$LU$分解可证明。 定理3：n阶对称正定矩阵$A$一定有Cholesky分解$A=L_1L_1^T$，当限定$L_1$的对角线为正时，矩阵的Cholesky分解唯一。 定理提到的两种分解，都可以用来解方程组。分为平方根法（$A=L_1L_1^T$）和$LDL^T$分解法。 平方根法： 求解$LL^T=A$，得到$L$。 求解$LY=b$，得到$Y$。 求解$L^TX=Y$，得到$X$。 $LDL^T$分解法（避免平方根法的开方运算） 求解$LDL^T=A$，得到$L,D$。 求解$LY=b$，得到$Y$。 求解$DZ=Y$，得到$Z$。 求解$L^TX=Z$，得到$X$。 4. 向量与矩阵范数4.1 向量范数 是n维欧几里得空间中长度概念的推广，向量$X$的范数记为$||X||$。 满足三个条件： 非负性 齐次性：$||\\alpha X||\\leq |\\alpha|\\cdot||X||$ 三角不等式：$||X+Y||\\leq ||X||+||Y||$，$X,Y$均为向量。 常用范数： 1范数：$\\sum_{i=1}^n|x_i|$ 2范数：$(\\sum_{i=1}^n|x_i|^2)^{1/2}$ ∞范数：$max_{1\\leq i \\leq n}|x_i|$ 更一般的p范数：$(\\sum_{i=1}^n|x_i|^p)^{1/p}$ 范数等价：若存在与$X$无关的两个正常数$C_1,C_2$，使$C_1||X||_\\alpha\\leq ||X|| _\\beta \\leq C_2||X||_\\alpha$，则这两个范数等价。 有限维空间上任何两个范数都等价。 4.2 矩阵范数 矩阵$A$的范数记为$||A||$。 满足四个条件： 非负性 齐次性 三角不等式 乘法不等式：$||AB||\\leq ||A|| \\cdot ||B||$。$A,B$都是n阶方阵。 矩阵范数与向量范数相容：$||AX||\\leq ||A|| \\cdot ||X||$ 诱导矩阵范数：$||A||=max_{||X||= 1}||AX||$ 常用范数： 1范数（列范数）：$||A||_ 1 = max_{1\\leq j\\leq n}\\sum_{i=1}^n|a_{ij}|$ 2范数：$||A||_ 2=(\\lambda_{max}(A^HA))^{1/2}$ ∞范数（行范数）：$||A||_ \\infty =max_{1\\leq i\\leq n}\\sum_{j=1}^n|a_{ij}|$ F范数：2范数的推广，$||A||_F=(\\sum _{i,j=1}^n|a _{ij}|^2)^{1/2}$ 4.3 谱半径 $A$的谱半径记作$\\rho(A)=max_{1\\leq i \\leq n}|\\lambda_i|$，$\\lambda$是$A$的特征值。 定理9：$A$的谱半径不超过$A$的任何一个矩阵范数，即$\\rho(A)\\leq ||A||$。 定理10：设$A$是任意n阶方阵，由$A$的各次幂所组成的矩阵序列$I,A,\\cdots,A^k$，收敛于0的充要条件是$\\rho(A) &lt; 1$。 4.4 方程右端误差对解的影响4.5 系数矩阵误差对解的影响","link":"/2021/05/09/NA-ch5/"},{"title":"数值分析：第7章 非线性方程和非线性方程组的数值解","text":"如果能在满足一定的精度要求下，求出非线性方程的近似根，则可以认为求根的计算问题已经解决。本章介绍求非线性方程实根近似值的方法，这些方法要先确定求根区间，或给出某根的近似值。 1. 对分法 适用于求有根区间内的单实根或奇重实根，理论依据是连续函数的介质定理：设$f(x)$在$[a, b]$上连续，$f(a)\\cdot f(b)&lt;0$，则在$(a, b)$内至少存在一点$\\xi$，使$f(\\xi)=0$。 对分法算法： 收敛性分析：$|x_k-\\xi|\\leq\\frac{1}{2}|b_k-a_k|=\\frac{1}{2^2}|b_{k-1}-a_{k-1}|=\\cdots=\\frac{1}{2^{k+1}}|b-a|$（因为每次缩小一半区间），故$k\\to\\infty$，$x$收敛于$\\xi$。 此公式可以用于估计对分次数$k$（$k$从0开始）。 对分法的收敛速度较慢，不能求出偶重根，常用于试探实根的分布区间，或求根的初始近似值。 对分法例题： 2. 迭代法2.1 迭代法 构造$y=f(x)$的等价方程$x=\\psi(x)$，取定根的一个近似值$x_0$，构造序列$x_{k+1}=\\psi(x_k)$，如果序列收敛于$x^\\star$，则$x^\\star$就是方程$x=\\psi(x)$的根。 迭代法算法： 2.2 迭代法的几何意义 将$x=\\psi(x)$写成$y=x,y=\\psi(x)$，则求解方程$x=\\psi(x)$，等价于求直线$y=x$和$y=\\psi(x)$的交点的横坐标$x^\\star$。 2.3 迭代法收敛条件 局部收敛的定义：如果在根$x^\\star$的某个邻域$|x-x^\\star|\\leq \\delta$，对任意的属于该邻域的$x_0$（初始近似值），构造格式$x_{k+1}=\\psi(x_k)$收敛，则称格式在$x^\\star$附近局部收敛。 定理1：设$x^\\star=\\psi(x^\\star)$，$\\psi^{‘}(x^\\star)$在$x^\\star$的某个邻域内连续，并且$|\\psi^{‘}(x)|\\leq q$，$q&lt;1$是常量，则： 格式$x_{k+1}=\\psi(x_k)$在$x^\\star$的该邻域附近局部收敛。 证：由拉格朗日中值定理，存在$\\xi$属于该邻域，使$x_k-x^\\star=\\psi(x_{k-1})-\\psi(x^\\star)=\\psi^{‘}(\\xi)(x_{k-1}-x^\\star)$，则$|x_k-x^\\star|\\leq q|x_{k-1}-x^\\star|\\leq\\cdots\\leq q^{k}|x_0-x^\\star|$，得证。 $|x_k-x^\\star|\\leq\\frac{1}{1-q}|x_{k+1}-x_k|$，$|x_k-x^\\star|\\leq\\frac{q^k}{1-q}|x_1-x_0|$。 证： \\begin{aligned} |x_k-x^\\star|&=|x_k-x_{k+1}+x_{k+1}-x^\\star|\\leq|x_k-x_{k+1}|+|x_{k+1}-x^\\star|\\\\ &=|x_k-x_{k+1}|+ |\\psi(x_{k})-\\psi(x^\\star)|=|x_k-x_{k+1}|+ |\\psi^{'}(\\xi)||x_{k}-x^\\star|\\\\ &\\leq|x_k-x_{k+1}|+q|x_{k}-x^\\star| \\end{aligned} 故$|x_k-x^\\star|\\leq\\frac{1}{1-q}|x_{k+1}-x_k|\\leq\\frac{q}{1-q}|x_{k}-x_{k-1}|\\leq\\cdots\\leq\\frac{q^k}{1-q}|x_1-x_0|$ 可以利用$|\\psi^{‘}(x)|&lt; 1$，粗略估计格式在根的附近的收敛情况。 定理2：给定方程$x=\\psi(x)$，若$\\psi(x)$满足： 对任意的$x\\in[a, b]$，有$\\psi(x)\\in C[a, b]$ 对任意的$x,y\\in[a, b]$，有$|\\psi(x)-\\psi(y)|\\leq q|x-y|$，$0\\leq q &lt; 1$为常数。 则对任意的$x_0\\in[a, b]$，格式$x_{k+1}=\\psi(x_k)$生成的序列${x_k}$收敛于$x=\\psi(x)$的根$x^\\star$。 证明和定理1类似。适用范围扩大成“任意”。这里的$\\psi(x)$为区间的压缩映射。 2.4 迭代法的加速 $|\\psi^{‘}(x)|$接近1时，收敛可能很慢。所以可以用松弛法或埃特金法加速。 2.4.1 松弛法 松弛法算法： $w_k$的表达式，是对$x=(1-w)x+w\\psi(x)=\\phi(x)$求导，令$\\phi^{‘}(x)=0$，解得$w$。 2.4.2 埃特金法 松弛法有时要计算导数，使用时可能很不方便，故引入埃特金方法。 设初始近似根为$x_0$，则$x_1=\\psi(x_0),x_2=\\psi(x_1)$。因为，$x^\\star=x_2+x^\\star-x_2=x_2+\\psi^{‘}(\\xi)(x^\\star-x_1)\\approx x_2+\\frac{x_2-x_1}{x_1-x_0}(x^\\star-x_1)$（用了差商）。解得$x^\\star$。 埃特金算法： 几何解释： $(x_0,x_1),(x_1,x_2)$这两点是在$\\psi(x)$曲线上的，这两点构成的直线和$y=x$的交点公式，就是埃特金公式。 3. 牛顿法3.1 牛顿公式 将非线性问题线性化，把$f(x)$在$x_0$处作泰勒展开，取泰勒展开公式的线性部分作为$f(x)=0$的近似方程：$p(x)=f(x_0)+f^{‘}(x_0)(x-x_0)=0$。 牛顿法公式：$x_{k+1}=x_k-\\frac{f(x_k)}{f^{‘}(x_k)}$。其实是一般迭代法用松弛法加速。 几何解释：是$y=f(x)$的切线方程和x轴相交的横坐标公式。设初始近似解为$x_0$，则$(x_0,f(x_0)$处的切线与x轴交点的横坐标就是$x_1$，以此逐步逼近$x^\\star$。 3.2 牛顿法的收敛速度 收敛阶的定义：设序列$\\{x_k\\}$收敛于$x^\\star$，令$\\epsilon=x^\\star-x_k$，设$k\\to\\infty$时，有$\\frac{|\\epsilon_{k+1}|}{|\\epsilon_{k}|^p}\\to c(c&gt;0)$。则称序列$\\{x_k\\}$是$p$阶收敛 p = 1：线性收敛 p = 2：二阶收敛（几何收敛） 1 &lt; p &lt; 2：超线性收敛 定理3：设$x^\\star=\\psi(x^\\star)$，在$x^\\star$的某个邻域内$\\psi^{(p)}(x)$连续，$p&gt;1$是常量，并且满足$\\psi^{(l)}(x^\\star)=0(l=1,2,\\cdots,p-1),\\psi^{(p)}(x^\\star)\\neq0$，则$x_{k+1}=\\psi(x_k)$生成的序列收敛于$x^\\star$，且为$p$阶收敛。 证： 由$\\psi^{‘}(x^\\star)=0&lt;1$，则$x_{k+1}=\\psi(x_k)$生成的序列收敛于$x^\\star$。 在$x^\\star$处作泰勒展开，并代入$x_k$：$x_{k+1}=\\psi(x_k)=\\psi(x^\\star)+\\psi^{‘}(x^\\star)(x_k-x^\\star)+\\cdots+\\psi^{(p)}(\\xi_k)[(x_k-x^\\star) ]^p/p!$。 由题可化为：$x_{k+1}-\\psi(x^\\star)=\\psi^{(p)}(x^\\star)[(x_k-x^\\star) ]^p/p!$，即$\\frac{x_{k+1}-x^\\star}{[(x_k-x^\\star) ]^p}=\\psi^{(p)}(\\xi_k)/p!$。 由于$\\xi_k$在$x_k,x^\\star$之间，且$x_k\\to x^\\star$，所以$\\xi_k\\to x^\\star$，$\\psi^{(p)}(\\xi_k)\\neq 0$。 满足$p$阶收敛的充分条件。 牛顿法的收敛：牛顿法在单根附近至少是二阶收敛的。而在重根附近，是线性收敛。 在单根附近：由牛顿法公式，$\\psi(x)=x-\\frac{f(x)}{f^{‘}(x)}$，有$\\psi^{‘}(x)=\\frac{f(x)f^{‘’}(x)}{[f^{‘}(x) ]^2}$，单根的情况，$f^{‘}(x^\\star)\\neq 0,f(x^\\star)= 0$，故$\\psi^{‘}(x)=0$。至少是二次收敛。 在重根附近：由于重根$f^{‘}(x^\\star)= 0$，要变换一下形式。 设$x^\\star$是$f(x)$的m重根，则$f(x)=(x-x^\\star)^mq(x)$，且$q(x^\\star)\\neq 0$。 则$|\\psi^{‘}(x)|=1-\\frac{1}{m}&lt;1$，序列有局部收敛性。 而$x_{k+1}-x^\\star=x_k-\\frac{f(x_k)}{f^{‘}(x_k)}-x^\\star=(x_k-x^\\star)\\frac{(m-1)q(x_k)+(x_k-x^\\star)q^{‘}(x_k)}{mq(x_k)+(x_k-x^\\star)q^{‘}(x_k)}$ 故$\\frac{|x_{k+1}-x^\\star|}{|(x_k-x^\\star)|}\\to\\frac{|m-1|}{|m|}$，满足线性收敛的条件。 牛顿法的优点是收敛很快，缺点是对重根收敛较慢，对初值要求较严。 习题： 导出下式的牛顿迭代法格式：（1）$\\frac{1}{c}$，不使用除法（2）$\\sqrt{c}$，不使用开方。 导出下式的迭代格式，并证明$lim_{k\\to\\infty}\\sqrt{2+\\sqrt{2+\\sqrt{\\cdots+\\sqrt{2}}}}=2$ 4. 割线法 $x_{k+1}=x_k-\\frac{f(x_k)}{f(x_k) - f(x_{k-1})}(x_k - x_{k-1})$，这也被称为双点割线法。 其实就是牛顿法的变形。用$\\frac{f(x_k) - f(x_{k-1})}{x_k - x_{k-1}}$替代了$f^{‘}(x)$。 单点割线法：$x_{k+1}=x_k-\\frac{f(x_k)}{f(x_k) - f(x_{0})}(x_k - x_{0})$。","link":"/2021/05/13/NA-ch7/"},{"title":"模式识别导论：第5章 Support Vector Machines","text":"本章介绍支持向量机（SVM）的相关知识，介绍了基本概念，以及SVM的三种种类：硬间隔、软间隔、非线性（基于核函数）。最后总结了SVM的优缺点。 1. Classification Margin support vectors（支持向量）：离超平面最近的数据点。 margin（边距）：定义为支持向量的距离。 优化目标是尽可能最大化margin，这也是SVM的目标。由于只和support vectors有关，减轻过拟合并最小化了复杂程度。 2. Support Vector Machines for Classification2.1 Linear Discrimination2.1.1 硬间隔SVM 任一数据点 x 到超平面的距离：$r=\\frac{w^Tx+b}{||w||}$（r的符号由分子的符号有关） 推导： 设有向量$x_0$，方向是超平面的方向。利用向量减法，得到$x-x_0=\\frac{rw}{||w||}$，即$x_0=x-\\frac{rw}{||w||}$。将$w^Tx_0+b=0$代入，得到$w^Tx-\\frac{rw^Tw}{||w||}+b=0$。 由于$\\sqrt{w^Tw}=||w||$，则上式可化为：$r=\\frac{w^Tx+b}{||w||}$。 假设支持向量到超平面的距离为$d (d &gt;0)$，则： \\begin{aligned} y_i &= +1:\\frac{w^Tx_i+b}{||w||}\\geq d \\\\ y_i &= -1:\\frac{w^Tx_i+b}{||w||}\\leq -d \\end{aligned} 可以假设$||w||d=1$，不影响后续优化问题的解决： \\begin{aligned} y_i &= +1:w^Tx_i+b\\geq 1 \\\\ y_i &= -1:w^Tx_i+b\\leq -1 \\end{aligned} 优化目标是最大化margin，而支持向量到平面的距离为$w^Tx_i+b=1$和$w^Tx_i+b=-1$，则margin = $\\frac{2}{||w||}$。 转换为Quadratic optimization problem（二次规划）问题：最小化 $\\frac{1}{2}||w||^2$，约束条件是$y_i(w^Tx_i+b)\\geq1$。这被称为带约束的原始问题。 拉格朗日函数：$L(w,b,\\alpha)=\\frac{1}{2}||w||^2+\\sum\\alpha_i(1-y_i(w^Tx_i+b))$。即无约束的原始问题，优化目标：$min_{w,b}max_{\\alpha}L(w,b,\\alpha)$ 该问题的对偶问题（dual problem）：$max_{\\alpha}min_{w,b}L(w,b,\\alpha)$。因为有弱对偶关系$min_{w,b}max_{\\alpha}L(w,b,\\alpha)\\geq max_{\\alpha}min_{w,b}L(w,b,\\alpha)$，而原始问题是凸二次优化问题，存在强对偶关系，该式可取等号。 对$w,b$求偏导，令$\\frac{\\partial L}{\\partial w}=\\frac{\\partial L}{\\partial b}=0$： \\begin{aligned} &w=\\sum \\alpha_ix_iy_i\\\\ &\\sum\\alpha_iy_i=0 \\end{aligned} 注意，这里涉及到了标量对向量的求导：$\\frac{\\partial (\\frac{1}{2}||w||^2)}{\\partial w}$，求导的结果也是个向量。因为$\\frac{1}{2}||w||^2=\\frac{1}{2}\\sum w_i^2$，对第k个分量求导得$\\frac{\\partial (\\frac{1}{2}||w||^2)}{\\partial w_k}=w_k$，所以，$\\frac{\\partial (\\frac{1}{2}||w||^2)}{\\partial w}=w$。 $x_i$是向量，$y_i,b$都是标量。 将上述式子代入$L(w,b,\\alpha)$，得到： \\begin{aligned} L(w,b,\\alpha)&=\\frac{1}{2}w^Tw+\\sum\\alpha_i-\\sum\\alpha_iy_iw^Tx_i-0\\\\ &=\\frac{1}{2}(\\sum_{i=1} \\alpha_ix_iy_i)^T(\\sum_{i=1} \\alpha_ix_iy_i)+\\sum_{i=1}\\alpha_i-\\sum_{i=1}\\alpha_iy_i(\\sum_{i=1} \\alpha_ix_iy_i)^Tx_i\\\\ &=\\frac{1}{2}(\\sum_{i=1} \\sum_{j=1}\\alpha_i\\alpha_jy_iy_jx_i^Tx_j)+\\sum_{i=1}\\alpha_i-\\sum_{i=1} \\sum_{j=1}\\alpha_i\\alpha_jy_iy_jx_i^Tx_j \\\\ &=\\sum_{i=1}\\alpha_i-\\frac{1}{2}(\\sum_{i=1} \\sum_{j=1}\\alpha_i\\alpha_jy_iy_jx_i^Tx_j) \\end{aligned} 注意，第二步到第三步将两个求和公式合并时，引入了$j$，可以类比简单的乘法$(1+2) \\times (1+2)=1 \\times (1+2) + 2 \\times (1+2)=\\sum_{i=1}^2i\\sum_{j=1}^2j$，就能理解为什么引入$j$了。 故对偶问题转换为：$max_{\\alpha}(\\sum_{i=1}\\alpha_i-\\frac{1}{2}\\sum_{i=1} \\sum_{j=1}\\alpha_i\\alpha_jy_iy_jx_i^Tx_j)$，且$\\alpha_i\\geq 0,\\sum\\alpha_iy_i=0$。可以用通用的二次规划解法来求解$\\alpha$，但是问题的规模正比于训练样本数，开销较大。可以选择一些高效的算法，比如SMO算法来求解$\\alpha$，这里不展开讲。 得到$\\alpha$后，即得$f(x)=w^Tx + b =\\sum \\alpha_iy_ix_i^Tx+b$。 注意到带约束的原始问题有不等式约束，需满足KKT条件： \\begin{cases} \\alpha_i\\geq 0\\\\ y_if(x_i)-1\\geq 0\\\\ \\alpha_i(y_if(x_i)-1)=0 \\end{cases} 分别称为乘子条件、约束条件、互补条件。 易得非支持向量对于$f(x)$无影响，因为$y_if(x_i)-1\\neq 0$，为满足$\\alpha_i(y_if(x_i)-1)=0$，则$\\alpha_i=0$。因此，最终模型仅与支持向量有关，即$f(x)=\\sum_{i\\in S} \\alpha_iy_ix_i^Tx+b$。 所以，$w,b$的计算可简化为： \\begin{aligned} w&=\\sum_{i\\in S} \\alpha_ix_iy_i \\\\ b&=1/y_s- \\sum_{i\\in S}\\alpha_iy_ix_i^Tx_s \\end{aligned} S是支持向量的集合，b是根据f(x)的公式得到的。$x_s$是某一支持向量。 虽然可选用任一支持向量得到b，但是常采用更鲁棒的做法：使用所有支持向量求解的平均值： \\begin{aligned} b&=1/|S|\\sum_{s\\in S}(1/y_s- \\sum_{i\\in S}\\alpha_iy_ix_i^Tx_s)\\\\ &=1/|S|\\sum_{s\\in S}(y_s- \\sum_{i\\in S}\\alpha_iy_ix_i^Tx_s) \\end{aligned} （因为$1/y_s$要么是1，要么是-1，可简化） 2.1.2 软间隔SVM 有时候数据并不是线性可分的，解决这一问题的方法之一是引入软间隔的概念，即允许有些数据点不满足$y_i(w^Tx_i+b)\\geq1$的约束条件。 设slack variable（松弛变量）为$\\epsilon_i$，则带约束的原始问题为：最小化 $\\frac{1}{2}||w||^2+C\\sum\\epsilon_i$，约束条件是$y_i(w^Tx_i+b)\\geq1-\\epsilon_i$，且$\\epsilon_i\\geq0$。参数$C&gt;0$被称作惩罚参数。$C$越大对误分类惩罚越大，取正无穷就是硬间隔SVM，越小对误分类惩罚越小。$C$过大容易过拟合，过小容易欠拟合。 可以用和求解硬间隔SVM的相同方法求解参数。 2.2 Nonlinear Discrimination 在原始样本空间内可能找不到能划分两类样本的超平面，可以将样本映射到更高维的特征空间，在这个空间内进行线性分类。 设$\\phi(x)$是样本映射后的特征向量，则超平面的表达式：$f(x)=\\sum_{i\\in S} \\alpha_iy_i\\phi(x_i)^T\\phi(x)+b$。 由于特征空间的维数可能很高，直接计算内积很困难，因此引入核函数：$k(x_i,x_j)=\\phi(x_i)^T\\phi(x_j)$，即$x_i,x_j$在特征空间的内积等于在原始样本空间内通过核函数计算的结果，这也被称作核技巧（kernel trick）。 模型改写为：$f(x)=\\sum_{i\\in S} \\alpha_iy_ik(x,x_i)+b$。 Mercer’s theorem：任意一个对称函数所对应的核矩阵半正定，则其能作为核函数。 常用的核函数： 核函数可以通过组合得到： 3. Summary 优点 Good generalization in theory and in practice Work well with few training instances Find globally best model Efficient algorithms Amenable to the kernel trick 缺点 二次规划问题求解时的运算规模较大，不适用于超大数据集（SMO算法可以缓解这个问题）。 只适用于二分类问题。（SVM的推广SVR也适用于回归问题；可以通过多个SVM的组合来解决多分类问题） 4. 参考资料 周志华《机器学习》 https://zhuanlan.zhihu.com/p/49331510","link":"/2021/05/14/PR-ch5/"},{"title":"模式识别导论：第6章 Decision Trees","text":"本章介绍了决策树的基本概念，选择分割节点方式的三种指标，防止过拟合的方法等。 1. What is a Decision Tree 树状模型，每个分支都包含着一种决策。 Hunt’s Algorithm： 对于一个树节点，有以下几种情况： 当包含的数据的类别相同，停止拓展，该节点为叶节点。 当不包含任何数据，该节点为叶节点。 当数据的类别不一致，根据某些规则继续拓展该节点。 2. How to Construct a Decision Tree主要有三种方式确定下一步选择哪个属性分割当前节点，倾向于选择使节点分割后更加homogeneous（同质的）。 2.1 Gini Index 对某个节点t，$Gini(t)=1-\\sum_j^m[P(j|t) ]^2$。$j$是该节点包含的类别之一。 最大值：$1-\\frac{1}{m}$，所有类别的概率相同。 最小值：0。 值越小，表示越homogeneous。 对某个节点p进行分割后，得到子节点$t_1,t_2,\\cdots,t_m$，则这种分割方式的$Gini_{split}=\\sum_{i=1}^m\\frac{n_i}{n}Gini(t_i)$。$n$是p的数据总数，$n_i$是$t_i$子节点的数据总数。 2.2 Infomation Gain 先介绍Infomation Entropy（信息熵）的概念，对某个节点t，$Entropy(t)=-\\sum_j^mP(j|t)logP(j|t)$。$j$是该节点包含的类别之一。 最大值：$log(m)$，所有类别的概率相同。 最小值：0。 值越小，表示越homogeneous。 对某个节点p进行分割后，得到子节点$t_1,t_2,\\cdots,t_m$，则这种分割方式的$Gain_{split}=Entropy(p)-\\sum_{i=1}^m\\frac{n_i}{n}Entropy(t_i)$。选择信息收益最大的分割方式，意味着这种分割方式使节点的熵最小，最homogeneous。 2.3 Error Rate 对某个节点t，$Error(t)=1-max_iP(i|t)$。 最大值：$1-\\frac{1}{m}$，所有类别的概率相同。 最小值：0。 值越小，表示越homogeneous。 3. Avoid Overfitting预剪枝和后剪枝策略： 3.1 Pre-Pruning 在树过于庞大之前，停止训练。 在节点中的数据类别相同时，停止拓展该节点。 当节点中的数据数目小于某个阈值，停止拓展该节点。 当不能改善节点的纯度，停止拓展该节点。 3.2 Post-pruning 先尽可能地拓展树，再自底向上修剪，如果修剪时发现可以改善泛化误差，则将当前节点设置为叶节点。叶节点的类别由该节点中的大部分数据所属的类别决定。 4. Summary 优点 容易理解和建立。 对未知记录进行分类时，速度快。 对离散属性的处理效率高。 缺点 引起误差传播，泛化性能不佳。","link":"/2021/05/18/PR-ch6/"},{"title":"模式识别导论：第7章 Ensemble","text":"本章介绍了集成学习方法，主要介绍了Bagging类方法和Boosting类方法的典型代表。 1. Background 集成学习（Ensemble learning），是一种构建并结合多个学习器来完成任务的方法。分为并行化方法和序列化方法。前者的代表是Bagging和Random Forest，后者的代表是Boosting。 2. Bagging和Random Forest为得到泛化性能较强的集成学习器，则希望个体学习器尽可能相互独立。可以采取随机采样训练样本，在这些不同的子集上训练个体学习器的方式。但采样时可能只用到一小部分学习数据，可考虑使用相互有交叠的采样子集。 2.1 Bagging 即Bootstrap aggregating的缩写。采用有放回的采样方式获得多个采样子集，即若原数据集有m个样本，则有放回的采样T个有m个样本的子集。对预测输出进行结合时，分类任务一般使用简单投票法，预测任务一般使用简单平均法。 由于每个个体学习器只用到了训练集的约63.2%的样本，剩下的被称为out-of-bag estimate。可以将out-of-bag estimate作为训练时的验证集。 Bagging主要关注降低方差，在不剪枝的决策树、神经网络等易受样本扰动的学习器上效用更加明显。 Bagging减少了过拟合，预测精度上升，一般使用同一种类型的分类器。 如果Bagging使用决策树作为基学习器，缺点是若有一个决策指标很强大，那么所有个体决策树的形式基本相同。由此引出了Random Forest（随机森林）的方法。 2.2 Random Forest 在Bagging训练的基础上，引入了随机属性选择。选择属性划分决策树的节点时，不再从全部d个属性中选择，而是随机选择一个有k个属性的子集，在这个子集中选一个最优属性进行划分。 2.3 降低方差的原因 若各个体学习器是同分布的，但并不相互独立（identically independent，简写为i.d.），则方差平均值为：$\\rho\\delta^2+\\frac{(1-\\rho)}{B}\\delta^2$。$B$是个体学习器的数目，$\\rho$是变量的相关度。 显然，随着Bagging方法的$B$增大，方差会减小，而Random Forest依靠随机选取属性，进一步减小了$\\rho$，继而进一步减小方差。 3. AdaBoost Boosting方法能将弱学习器提升到强学习器。最著名的算法是AdaBoost。即从初始训练集中训练出一个基学习器，根据其分类表现，调整每个样本的权重，使被分错的样本在后续训练中得到更多的关注。然后基于调整后的样本训练下一个基学习器。直到训练的基学习器达到指定数目T时，将这T个基学习器进行加权结合。 具体算法如下: 优点： 除了基学习器的数目，没有其他超参数，无需调参。 算法简单，容易实现。 我们无需找到一个很强的基学习器，只要基学习器分类正确率大于50%即可。 缺点： 模型性能取决于数据集和基学习器的选择。 易受到噪声干扰，异常样本在迭代中可能会获得较高的权重。 4. 参考资料 周志华《机器学习》 https://blog.csdn.net/guyuealian/article/details/70995333","link":"/2021/05/22/PR-ch7/"},{"title":"模式识别导论：第8章 Clustering","text":"本章介绍了聚类的相关知识，包括Partitional Clustering和Hierarchical clustering这两种类别以及代表方法。 1. What is Cluster Analysis 聚类分析是一种无监督学习方式，需探索数据的内在结构，尽量最小化类内距离和最大化类间距离。 2. Partitional Clustering 划分聚类，每个样本只属于一个聚类。 2.1 k-means聚类算法 常用k-means聚类算法。可以用elbow finding（肘部法则） 确定合适的聚类数目K。 缺点： 容易受到异常点的影响。可以通过监视某些可能的异常点，在几次迭代后移除之。或是采用随机采样的方法，这样选择到异常点的几率大大降低。 对初始聚类中心的选择敏感。可以多选择几种聚类中心，选一个聚类效果好的。 不适用于非椭球体的数据分布。 2.2 Fuzzy C-means聚类算法 允许一个样本属于一个或多个聚类，通过计算该样本对所有聚类的隶属度，确定该样本的所属的聚类。 隶属度：$u_{k,i}=\\frac{1}{\\sum_{j=1}^C(d_{k,i}/d_{k,j})^{\\frac{2}{m-1}}}$，表示第k个点对第i个聚类的隶属度，$d_{k,i}$是第k个点到第i个聚类的距离，$m$是fuzzy weighting factor，一般取2。 聚类中心：$v_i=\\frac{\\sum_{k=1}^n(u_{k,i})^mx_k}{\\sum_{k=1}^n(u_{k,i})^m}$。 算法： 随机初始化隶属度矩阵。 计算初始聚类中心。 更新隶属度矩阵。 如果隶属度矩阵变化小于某个阈值，则算法结束，否则跳转回第2步。 例子： 优点：总是收敛的。 缺点：对初始聚类中心和噪声敏感。 3. Hierarchical clustering 层次聚类，将聚类组织成一棵树形图（dendrogram）。不需要预先确定聚类数目，可以通过在不同层级上的划分，得到对应的聚类数目。没有明确的目标函数。 3.1 Agglomerative clustering 初始时将每个数据点看作一个独立的聚类，不断聚合距离最近的两个聚类，直到只剩下一个聚类。 衡量聚类相似度的指标： MIN or Single Link：两个聚类的相似度取决于两个聚类距离最近的点。 可以处理非椭球形状，但对噪声敏感。 MAX or Complete Link：两个聚类的相似度取决于两个聚类距离最远的点。 对噪声不敏感，但是容易误判一个大聚类（聚类内的点距离很远）的情况。 Group Average：两个聚类的相似度取决于两个聚类所有点的距离的平均值。 3.2 Divisive clustering 将所有数据点看作一个聚类，不断划分，直至划分成每个数据点都为一个聚类。","link":"/2021/05/26/PR-ch8/"},{"title":"模式识别导论：第9章 Principal Component Analysis","text":"本章介绍了PCA算法及其相关应用。 1. Motivation 现实生活中，数据集样本特征过多，难以看出联系。需要选择一个保留了重要特征的，特征维度较低的子集。 目标：希望将数据点投影到更低维，且投影后的点尽可能分散，即最大化方差。且最小化投影点和原始数据点的距离。 投影后的特征向量可用一组线性无关的基表示，可被证明应采用正交（orthogonal）基。第一个基指向目标特征空间方差最大的方向，第二个基与第一个基正交，指向子空间方差最大的方向，以此类推。 2. PCA Algorithms 计算样本的协方差矩阵：$\\Sigma=\\frac{1}{m}X^TX$，$m$是样本数量，$X$是$m\\times n$矩阵。 计算该协方差矩阵的特征值和特征向量，将特征向量按列组成矩阵$U$。要将原始数据集降至k维，则选择前k列子集$U_k$，$U_k$即为降维矩阵。 降维计算：$Z=XU_k$，从n维降至k维。 3. Application 人脸识别 图像压缩 噪声过滤 4. Shortcomings 并不知道类别标签。 不能用于非线性的结构。","link":"/2021/05/30/PR-ch9/"},{"title":"数据结构：排序","text":"本章总结排序的基础知识，以及具体题型。主要针对internal sorting，参考了老师上课的课件、在leetcode、牛客网刷的相关题目和《数据结构与算法分析（C++版）（第三版）》。 1. 基础定义 Internal Sorting（内部排序）是对存储在内存中的数据进行排序。External Sorting（外部排序）是对存储在外存中的数据进行排序。 Stable（稳定的）：相等的关键字经排序后，关键字的次序没有发生改变。Unstable（不稳定的）：相等的关键字经排序后，关键字的次序发生改变。 以下排序算法的介绍，均以升序为正序。 2. Insertion Sort 插入排序，是指在待排序的元素中，假设前面 n - 1 (n &gt;= 2) 个数已经是排好顺序的，现将第n个数插到前面已经排好的序列中的合适位置，使这n个数排好顺序。 往前寻找到合适位置，并将该位置以后的数据往后移动一个位置，操作过于繁琐。可以直接在往前寻找的过程中，和前一个元素比较大小，若为逆序，则交换两个元素，否则停止寻找，以达到相同的效果。 代码： 时间复杂度：最好情况是整个序列是正序的，为$\\theta(n)$；最坏情况是整个序列是逆序的，为$\\theta(n^2)$。 稳定性：是稳定的排序算法。 3. Bubble Sort 冒泡排序：它重复地遍历过要排序的序列，依次比较两个相邻的元素，如果顺序错误就把它们交换过来。直到没有相邻元素需要交换，也就是说该序列已经排序完成。 在每一次遍历后，待排序元素中的最小元素都会被交换至序列的前面，形成“冒泡”的效果。 代码： 时间复杂度：最好和最坏的情况都是$\\theta(n^2)$，引入flag变量后，最好情况下可能达到$\\theta(n)$，即一次遍历就将序列排好序。 稳定性：是稳定的排序算法。 4. Selection Sort 选择排序：每次遍历都从待排序的数据元素中选出最小的一个元素，存放在序列的起始位置（第一次遍历）或已排序的序列的末尾，直到全部待排序的数据元素的个数为零。 代码： 时间复杂度：最好和最坏的情况都是$\\theta(n^2)$。 稳定性：是不稳定的排序算法。如：2，2，1这一序列，第一次遍历时，1和第一个2交换了位置，导致第一个2排在了第二个2后面，且排序结束后该顺序不会复原，相同元素的顺序经排序后发生了改变。 5. Shell Sort 希尔排序：为了能发挥插入排序在最好情况下的时间复杂度的优势，希望排序时是对基本有序的序列排序。把元素按下标的一定增量分组，对每组使用直接插入排序算法排序。随着增量逐渐减少，每组包含的关键词越来越多，当增量减至 1 时，整个序列恰被分成一组，算法便终止。 代码： C++1234567891011121314151617181920212223242526272829/* 希尔排序 */void shell_sort(vector&lt;int&gt;&amp; v){ int size = v.size(); int cur = size / 2; // 增量 // 在某一增量下，对元素进行分组 while (cur &gt;= 1) { // 起始位置i &lt; cur，若&gt;=cur，则选取了重复的序列 for (int i = 0; i &lt; cur; i++) { insertion_sort(v, i, cur); } cur /= 2; }}void insertion_sort(vector&lt;int&gt;&amp; v, int begin, int cur){ int size = v.size(); // 前面介绍的插入排序，是这里cur = 1的情况。 for (int i = begin + cur; i &lt; size; i += cur) { for (int j = i; j &gt; begin &amp;&amp; v[j] &lt; v[j - cur]; j -= cur) { swap(v[j], v[j - cur]); } }} 时间复杂度：$O(n^{1.3-2})$ 稳定性：是不稳定的排序算法。因为在分组排序的过程中，相同的元素可能在不同的分组序列中移动，最后的稳定性就可能被打乱。 6. Quick Sort 快速排序： C++1cout &lt;&lt; endl; 7. Merge Sort8. Heap Sort9. BinSort and Radix Sort10. Empirical Comparison of Sorting Algorithms11. Lower bounds for Sorting","link":"/2021/06/17/ds-sort/"}],"tags":[{"name":"不规则采样","slug":"不规则采样","link":"/tags/%E4%B8%8D%E8%A7%84%E5%88%99%E9%87%87%E6%A0%B7/"},{"name":"时间序列","slug":"时间序列","link":"/tags/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97/"},{"name":"srp周报","slug":"srp周报","link":"/tags/srp%E5%91%A8%E6%8A%A5/"},{"name":"线性代数","slug":"线性代数","link":"/tags/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"},{"name":"数据结构","slug":"数据结构","link":"/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"模式识别","slug":"模式识别","link":"/tags/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB/"},{"name":"数值分析","slug":"数值分析","link":"/tags/%E6%95%B0%E5%80%BC%E5%88%86%E6%9E%90/"}],"categories":[{"name":"论文阅读笔记","slug":"论文阅读笔记","link":"/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"name":"数学","slug":"数学","link":"/categories/%E6%95%B0%E5%AD%A6/"},{"name":"计科","slug":"计科","link":"/categories/%E8%AE%A1%E7%A7%91/"}]}